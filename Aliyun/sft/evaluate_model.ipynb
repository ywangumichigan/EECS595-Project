{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Fine-tuned Model on MATH Test Dataset\n",
        "\n",
        "This notebook evaluates the fine-tuned Qwen3 model downloaded from Aliyun on the MATH test dataset.\n",
        "\n",
        "## Setup\n",
        "- Model location: `model/` directory\n",
        "- Test dataset: `test_math.json`\n",
        "- Evaluation metric: Exact Match (EM) using SymPy normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# If you encounter version compatibility errors between transformers and peft,\n",
        "# you may need to install compatible versions. Try this cell first, and if it fails,\n",
        "# run the troubleshooting cell below.\n",
        "\n",
        "%pip install -q transformers accelerate peft sympy torch bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting: Version Compatibility\n",
        "\n",
        "If you get an error like `ModuleNotFoundError: No module named 'transformers.modeling_layers'`, \n",
        "run the cell below to fix the version compatibility issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If you see version errors, uncomment one of the options above and run this cell.\n"
          ]
        }
      ],
      "source": [
        "# TROUBLESHOOTING CELL - Run this if you get version compatibility errors\n",
        "# This installs compatible versions of transformers and peft\n",
        "\n",
        "# Option 1: Try upgrading to latest compatible versions\n",
        "# %pip install --upgrade transformers peft accelerate\n",
        "\n",
        "# Option 2: Install specific compatible versions (recommended)\n",
        "# Uncomment the line below if Option 1 doesn't work:\n",
        "# %pip install \"transformers>=4.37.0,<4.52.0\" \"peft>=0.7.0\" accelerate sympy torch bitsandbytes\n",
        "\n",
        "# Option 3: Latest versions (may work, try if others fail)\n",
        "# %pip install \"transformers>=4.40.0\" \"peft>=0.10.0\" accelerate sympy torch bitsandbytes\n",
        "\n",
        "print(\"If you see version errors, uncomment one of the options above and run this cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: peft not available or version incompatible: No module named 'transformers.modeling_layers'\n",
            "You can only use full models (not LoRA adapters) without peft.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import sympy\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Try to import PeftModel (only needed if using LoRA adapter)\n",
        "# If import fails, we'll handle it when loading the adapter\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    PEFT_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"Warning: peft not available or version incompatible: {e}\")\n",
        "    print(\"You can only use full models (not LoRA adapters) without peft.\")\n",
        "    PEFT_AVAILABLE = False\n",
        "    PeftModel = None\n",
        "\n",
        "# Configuration\n",
        "MODEL_DIR = \"model\"  # Path to the model DIRECTORY (folder containing model files, not the weights file itself)\n",
        "# MODEL_DIR should contain: config.json, tokenizer files, and either:\n",
        "#   - model.safetensors (full model), OR\n",
        "#   - adapter/ folder (LoRA adapter)\n",
        "TEST_DATA_PATH = \"test_math.json\"  # Path to test dataset\n",
        "USE_8BIT = True  # Use 8-bit quantization to save memory (requires bitsandbytes)\n",
        "MAX_NEW_TOKENS = 512\n",
        "NUM_SAMPLES = None  # None means evaluate on all test samples, or set a number for quick testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Evaluation Functions\n",
        "\n",
        "These functions are used to extract answers from model output and compare them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_boxed(latex_string):\n",
        "    \"\"\"Extract content from \\\\boxed{} in LaTeX string.\"\"\"\n",
        "    if not latex_string:\n",
        "        return None\n",
        "\n",
        "    match = re.search(r'\\\\boxed\\s*\\{', latex_string, re.IGNORECASE)\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    start_index = match.end()\n",
        "    brace_count = 1\n",
        "    content = []\n",
        "\n",
        "    for i in range(start_index, len(latex_string)):\n",
        "        char = latex_string[i]\n",
        "        if char == '{':\n",
        "            brace_count += 1\n",
        "            content.append(char)\n",
        "        elif char == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                return \"\".join(content)\n",
        "            else:\n",
        "                content.append(char)\n",
        "        else:\n",
        "            content.append(char)\n",
        "    return None\n",
        "\n",
        "def normalize_sympy(s):\n",
        "    \"\"\"Normalize mathematical expression using sympy.\"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        return sympy.sympify(s)\n",
        "    except (sympy.SympifyError, TypeError):\n",
        "        return None\n",
        "\n",
        "print(\"Evaluation functions loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "The notebook will automatically detect if the model is a full model or a LoRA adapter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we have a full model or LoRA adapter\n",
        "model_path = Path(MODEL_DIR)\n",
        "has_full_model = (model_path / \"model.safetensors\").exists() or (model_path / \"pytorch_model.bin\").exists()\n",
        "has_adapter = (model_path / \"adapter\" / \"adapter_config.json\").exists()\n",
        "\n",
        "print(f\"Model directory: {model_path.absolute()}\")\n",
        "print(f\"Has full model: {has_full_model}\")\n",
        "print(f\"Has adapter: {has_adapter}\")\n",
        "\n",
        "if not has_full_model and not has_adapter:\n",
        "    raise ValueError(f\"No model found in {MODEL_DIR}. Please check the path.\")\n",
        "\n",
        "# Check if adapter requires peft\n",
        "if has_adapter and not PEFT_AVAILABLE:\n",
        "    raise ValueError(\n",
        "        \"LoRA adapter detected but peft is not available or incompatible.\\n\"\n",
        "        \"Please fix the version compatibility issue:\\n\"\n",
        "        \"Option 1: Upgrade packages: pip install --upgrade transformers peft\\n\"\n",
        "        \"Option 2: Install compatible versions: pip install 'transformers>=4.37.0,<4.52.0' 'peft>=0.7.0'\\n\"\n",
        "        \"Option 3: Use a full model instead of a LoRA adapter\"\n",
        "    )\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model based on type\n",
        "if has_adapter and PEFT_AVAILABLE:\n",
        "    # Load base model first (need to check config for base model path)\n",
        "    adapter_config_path = model_path / \"adapter\" / \"adapter_config.json\"\n",
        "    with open(adapter_config_path, 'r') as f:\n",
        "        adapter_config = json.load(f)\n",
        "    \n",
        "    # Note: If base_model_name_or_path is a local path that doesn't exist,\n",
        "    # you may need to manually specify the base model name\n",
        "    base_model_path = adapter_config.get(\"base_model_name_or_path\", \"\")\n",
        "    \n",
        "    print(f\"Loading base model from: {base_model_path}\")\n",
        "    \n",
        "    # Try to load base model - if path doesn't exist, we'll try loading from MODEL_DIR\n",
        "    if os.path.exists(base_model_path):\n",
        "        base_model_path_to_use = base_model_path\n",
        "    else:\n",
        "        # Assume the full model is in MODEL_DIR (if it exists)\n",
        "        if has_full_model:\n",
        "            base_model_path_to_use = str(model_path)\n",
        "            print(f\"Base model path not found, using model directory: {base_model_path_to_use}\")\n",
        "        else:\n",
        "            # If no base model found, we need the HuggingFace model name\n",
        "            # This should be provided by the user or detected from config\n",
        "            raise ValueError(f\"Cannot find base model. Please check the adapter config.\")\n",
        "    \n",
        "    # Import BitsAndBytesConfig for quantization (if available)\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quantization_config = None\n",
        "        if USE_8BIT:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=6.0,\n",
        "            )\n",
        "    except ImportError:\n",
        "        print(\"Warning: bitsandbytes not available. Setting USE_8BIT=False\")\n",
        "        USE_8BIT = False\n",
        "        quantization_config = None\n",
        "    \n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path_to_use if os.path.exists(base_model_path_to_use) else MODEL_DIR,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Load adapter\n",
        "    adapter_path = model_path / \"adapter\"\n",
        "    print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
        "    model = PeftModel.from_pretrained(base_model, str(adapter_path))\n",
        "    print(\"LoRA adapter loaded successfully!\")\n",
        "    \n",
        "else:\n",
        "    # Load full model\n",
        "    # Import BitsAndBytesConfig for quantization (if available)\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quantization_config = None\n",
        "        if USE_8BIT:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=6.0,\n",
        "            )\n",
        "    except ImportError:\n",
        "        print(\"Warning: bitsandbytes not available. Setting USE_8BIT=False\")\n",
        "        USE_8BIT = False\n",
        "        quantization_config = None\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_DIR,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Full model loaded successfully!\")\n",
        "\n",
        "model.eval()\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Dataset\n",
        "\n",
        "Load the test dataset in JSON format (instruction/output pairs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test dataset\n",
        "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test examples\")\n",
        "\n",
        "# Limit number of samples if specified\n",
        "if NUM_SAMPLES is not None:\n",
        "    test_data = test_data[:NUM_SAMPLES]\n",
        "    print(f\"Limited to {NUM_SAMPLES} samples for evaluation\")\n",
        "\n",
        "# Show an example\n",
        "print(\"\\nExample test item:\")\n",
        "print(json.dumps(test_data[0], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function\n",
        "\n",
        "This function evaluates the model on the test dataset and computes exact match accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_data, max_new_tokens=512, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate model on test dataset.\n",
        "    Returns: exact_match_score, detailed_results\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    # Extract system prompt and user question from instruction\n",
        "    # Format: \"You are a math assistant...\\\\n\\\\n<problem>\"\n",
        "    SYSTEM_PROMPT = \"You are a math assistant. Solve the problem step by step, explain your reasoning, and box the final answer using \\\\boxed{}.\"\n",
        "    \n",
        "    for idx, item in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
        "        instruction = item[\"instruction\"]\n",
        "        gold_output = item[\"output\"]\n",
        "        \n",
        "        # Parse instruction to get system prompt and problem\n",
        "        # The instruction format is: \"You are a math assistant...\\n\\n<problem>\"\n",
        "        # Try both actual newlines and escaped newlines\n",
        "        if \"\\n\\n\" in instruction:\n",
        "            parts = instruction.split(\"\\n\\n\", 1)\n",
        "            system_msg = parts[0]\n",
        "            problem = parts[1]\n",
        "        elif \"\\\\n\\\\n\" in instruction:\n",
        "            parts = instruction.split(\"\\\\n\\\\n\", 1)\n",
        "            system_msg = parts[0]\n",
        "            problem = parts[1]\n",
        "        else:\n",
        "            # If no separator, use the whole instruction as problem\n",
        "            system_msg = SYSTEM_PROMPT\n",
        "            problem = instruction\n",
        "        \n",
        "        # Extract gold answer\n",
        "        gold_ans_str = extract_boxed(gold_output)\n",
        "        gold_ans_sym = normalize_sympy(gold_ans_str)\n",
        "        \n",
        "        # Construct prompt in Qwen3 format\n",
        "        # Format: <|im_start|>system\\n<system_prompt><|im_end|>\\n<|im_start|>user\\n<problem><|im_end|>\\n<|im_start|>assistant\\n\n",
        "        prompt = f\"<|im_start|>system\\n{system_msg}<|im_end|>\\n<|im_start|>user\\n{problem}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        \n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode only the newly generated tokens\n",
        "        generated_tokens = outputs[0][input_length:]\n",
        "        pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "        \n",
        "        # Extract predicted answer\n",
        "        pred_ans_str = extract_boxed(pred_text)\n",
        "        pred_ans_sym = normalize_sympy(pred_ans_str)\n",
        "        \n",
        "        # Check if correct\n",
        "        is_correct = False\n",
        "        if pred_ans_sym is not None and gold_ans_sym is not None:\n",
        "            is_correct = (pred_ans_sym == gold_ans_sym)\n",
        "        elif (pred_ans_str == \"\" or pred_ans_str is None) and (gold_ans_str == \"\" or gold_ans_str is None):\n",
        "            is_correct = True\n",
        "        \n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        results.append({\n",
        "            \"problem\": problem[:100] + \"...\" if len(problem) > 100 else problem,\n",
        "            \"predicted\": pred_ans_str,\n",
        "            \"gold\": gold_ans_str,\n",
        "            \"predicted_full\": pred_text[:200] + \"...\" if len(pred_text) > 200 else pred_text,\n",
        "            \"correct\": is_correct\n",
        "        })\n",
        "        \n",
        "        if verbose and idx < 5:  # Show first 5 examples\n",
        "            print(f\"\\n--- Example {idx + 1} ---\")\n",
        "            print(f\"Problem: {problem[:150]}...\")\n",
        "            print(f\"Predicted answer: {pred_ans_str}\")\n",
        "            print(f\"Gold answer: {gold_ans_str}\")\n",
        "            print(f\"Correct: {is_correct}\")\n",
        "            print(f\"Generated text (first 200 chars): {pred_text[:200]}...\")\n",
        "    \n",
        "    exact_match = correct / total if total > 0 else 0.0\n",
        "    return exact_match, results\n",
        "\n",
        "print(\"Evaluation function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation\n",
        "\n",
        "Evaluate the model on the test dataset and compute exact match accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting evaluation...\")\n",
        "print(f\"Total test examples: {len(test_data)}\")\n",
        "print(f\"Max new tokens: {MAX_NEW_TOKENS}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "exact_match_score, detailed_results = evaluate_model(\n",
        "    model, \n",
        "    tokenizer, \n",
        "    test_data,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Exact Match Score: {exact_match_score:.4f} ({exact_match_score*100:.2f}%)\")\n",
        "print(f\"Correct: {sum(r['correct'] for r in detailed_results)}\")\n",
        "print(f\"Total: {len(detailed_results)}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Save the evaluation results to a file for later analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON file\n",
        "results_file = \"evaluation_results.json\"\n",
        "results_summary = {\n",
        "    \"exact_match_score\": exact_match_score,\n",
        "    \"total_examples\": len(detailed_results),\n",
        "    \"correct\": sum(r['correct'] for r in detailed_results),\n",
        "    \"detailed_results\": detailed_results\n",
        "}\n",
        "\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Results saved to {results_file}\")\n",
        "\n",
        "# Show some statistics\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"  Accuracy: {exact_match_score*100:.2f}%\")\n",
        "correct_count = sum(r['correct'] for r in detailed_results)\n",
        "incorrect_count = len(detailed_results) - correct_count\n",
        "print(f\"  Correct: {correct_count}\")\n",
        "print(f\"  Incorrect: {incorrect_count}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CSE595",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
