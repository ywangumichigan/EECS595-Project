{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywangumichigan/EECS595-Project/blob/main/Aliyun/sft/evaluate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gePfdmzzSNsu"
      },
      "source": [
        "# Evaluate Fine-tuned Model on MATH Test Dataset\n",
        "\n",
        "This notebook evaluates the fine-tuned Qwen3 model downloaded from Aliyun on the MATH test dataset.\n",
        "\n",
        "## Setup\n",
        "- Model location: `model/` directory\n",
        "- Test dataset: `test_math.json`\n",
        "- Evaluation metric: Exact Match (EM) using SymPy normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WWCnu1-OSNs1",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "c06ee9f3-a890-4507-a9f3-01216c4888db"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1580654638.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# run the troubleshooting cell below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install -q transformers accelerate peft sympy torch bitsandbytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_installation_commands.py\u001b[0m in \u001b[0;36m_pip_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Colab is set up such that pip does the right thing, and pip install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# will properly trigger the pip install warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mmetadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         )\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assemble_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m_assemble_message\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_adapters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_adapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/__init__.py\u001b[0m in \u001b[0;36mmessage_from_string\u001b[0;34m(s, *args, **kws)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmessage_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/parser.py\u001b[0m in \u001b[0;36mparsestr\u001b[0;34m(self, text, headersonly)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadersonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheadersonly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# If you encounter version compatibility errors between transformers and peft,\n",
        "# you may need to install compatible versions. Try this cell first, and if it fails,\n",
        "# run the troubleshooting cell below.\n",
        "\n",
        "%pip install -q transformers accelerate peft sympy torch bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcAXZkf0SchS",
        "outputId": "9579806a-9499-4a02-fb88-d0cddf2942aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "p9vHeP_iSnqF",
        "outputId": "68a85eef-e0d0-44c6-9fd1-5f380a17dfe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQk7SJuFSNs4"
      },
      "source": [
        "## Troubleshooting: Version Compatibility\n",
        "\n",
        "If you get an error like `ModuleNotFoundError: No module named 'transformers.modeling_layers'`,\n",
        "run the cell below to fix the version compatibility issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfDlXiRBSNs4",
        "outputId": "14167983-7f54-497f-dc4b-4eacb8683d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "If you see version errors, uncomment one of the options above and run this cell.\n"
          ]
        }
      ],
      "source": [
        "# TROUBLESHOOTING CELL - Run this if you get version compatibility errors\n",
        "# This installs compatible versions of transformers and peft\n",
        "\n",
        "# Option 1: Try upgrading to latest compatible versions\n",
        "%pip install --upgrade transformers peft accelerate\n",
        "\n",
        "# Option 2: Install specific compatible versions (recommended)\n",
        "# Uncomment the line below if Option 1 doesn't work:\n",
        "# %pip install \"transformers>=4.37.0,<4.52.0\" \"peft>=0.7.0\" accelerate sympy torch bitsandbytes\n",
        "\n",
        "# Option 3: Latest versions (may work, try if others fail)\n",
        "# %pip install \"transformers>=4.40.0\" \"peft>=0.10.0\" accelerate sympy torch bitsandbytes\n",
        "\n",
        "print(\"If you see version errors, uncomment one of the options above and run this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OPGeGS1WSNs5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import sympy\n",
        "import torch\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Try to import PeftModel (only needed if using LoRA adapter)\n",
        "# If import fails, we'll handle it when loading the adapter\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    PEFT_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"Warning: peft not available or version incompatible: {e}\")\n",
        "    print(\"You can only use full models (not LoRA adapters) without peft.\")\n",
        "    PEFT_AVAILABLE = False\n",
        "    PeftModel = None\n",
        "\n",
        "# Configuration\n",
        "MODEL_DIR = \"model\"  # Path to the model DIRECTORY (folder containing model files, not the weights file itself)\n",
        "# MODEL_DIR should contain: config.json, tokenizer files, and either:\n",
        "#   - model.safetensors (full model), OR\n",
        "#   - adapter/ folder (LoRA adapter)\n",
        "TEST_DATA_PATH = \"/content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/test_math.json\"  # Path to test dataset (should have level and type fields)\n",
        "USE_8BIT = True  # Use 8-bit quantization to save memory (requires bitsandbytes)\n",
        "MAX_NEW_TOKENS = 128 # Reduced for faster generation\n",
        "NUM_SAMPLES = 10  # None means evaluate on all test samples, or set a number for quick testing\n",
        "DO_SAMPLE = False # Set to False for faster (greedy) generation\n",
        "TEMPERATURE = 0.7 # Lower temperature for less randomness when do_sample is True\n",
        "\n",
        "# Filtering options (set to None or [] to disable filtering)\n",
        "FILTER_LEVELS = [\"Level 2\"]  # e.g., [\"Level 1\", \"Level 2\"] or None for all levels\n",
        "FILTER_TYPES = None  # e.g., [\"algebra\", \"geometry\"] or None for all types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIRKxUNfSNs6"
      },
      "source": [
        "## Load Evaluation Functions\n",
        "\n",
        "These functions are used to extract answers from model output and compare them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d58e56c5",
        "outputId": "327a5548-4f32-494d-9975-058aa16bcabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation functions updated with LaTeX fraction preprocessing and implicit multiplication handling (corrected)!\n"
          ]
        }
      ],
      "source": [
        "import sympy\n",
        "import re\n",
        "\n",
        "def _parse_latex_braces(s, start_index):\n",
        "    \"\"\"Parses content within balanced braces, handling nesting.\"\"\"\n",
        "    if not s or start_index >= len(s) or s[start_index] != '{':\n",
        "        return None, -1\n",
        "\n",
        "    brace_count = 1\n",
        "    content_chars = []\n",
        "    current_index = start_index + 1\n",
        "\n",
        "    while current_index < len(s):\n",
        "        char = s[current_index]\n",
        "        if char == '{':\n",
        "            brace_count += 1\n",
        "            content_chars.append(char)\n",
        "        elif char == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                return \"\".join(content_chars), current_index\n",
        "            else:\n",
        "                content_chars.append(char)\n",
        "        else:\n",
        "            content_chars.append(char)\n",
        "        current_index += 1\n",
        "\n",
        "    return None, -1 # Unbalanced braces\n",
        "\n",
        "def _insert_multiplication_stars(s):\n",
        "    \"\"\"Inserts '*' for implicit multiplication in mathematical strings for SymPy.\"\"\"\n",
        "    if not s:\n",
        "        return s\n",
        "\n",
        "    # Regex patterns for implicit multiplication\n",
        "    # 1. Number followed by a variable (e.g., \"2x\" -> \"2*x\")\n",
        "    s = re.sub(r'(\\d)([a-zA-Z])', r'\\1*\\2', s)\n",
        "    # 2. Variable followed by a variable (e.g., \"xy\" -> \"x*y\")\n",
        "    #    This is a simplification for typical single-letter variables and may over-correct for function names (e.g., 'sin(x)').\n",
        "    #    For MATH dataset problems, variables are usually single letters, making this generally safe.\n",
        "    s = re.sub(r'([a-zA-Z])([a-zA-Z])', r'\\1*\\2', s)\n",
        "    # 3. Closing parenthesis followed by an opening parenthesis or a variable\n",
        "    #    (e.g., \"(a+b)(c+d)\" -> \"(a+b)*(c+d)\", \"(a+b)x\" -> \"(a+b)*x\")\n",
        "    s = re.sub(r'(\\))([a-zA-Z(])', r'\\1*\\2', s)\n",
        "    # 4. Number followed by an opening parenthesis (e.g., \"2(x+y)\" -> \"2*(x+y)\")\n",
        "    s = re.sub(r'(\\d)(\\()', r'\\1*\\2', s)\n",
        "    return s\n",
        "\n",
        "def extract_boxed(latex_string):\n",
        "    \"\"\"Extract content from the LAST \\boxed{} in LaTeX string, handling nested braces.\"\"\"\n",
        "    if not latex_string:\n",
        "        return None\n",
        "\n",
        "    last_boxed_content = None\n",
        "\n",
        "    # Find all occurrences of \\boxed{ using re.finditer\n",
        "    matches = list(re.finditer(r'\\\\boxed\\s*\\{', latex_string, re.IGNORECASE))\n",
        "\n",
        "    for match in matches:\n",
        "        # Start parsing content from after the opening brace of \\boxed{}\n",
        "        start_index = match.end()\n",
        "        brace_content, end_index = _parse_latex_braces(latex_string, start_index - 1) # Pass the index of '{'\n",
        "        if brace_content is not None:\n",
        "            last_boxed_content = brace_content\n",
        "    return last_boxed_content\n",
        "\n",
        "def normalize_sympy(s):\n",
        "    \"\"\"Normalize mathematical expression using sympy, preprocessing LaTeX fractions and implicit multiplication.\"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "\n",
        "    processed_s = s\n",
        "\n",
        "    # Step 1: Unescape double backslashes for \\frac, \\dfrac if they exist\n",
        "    processed_s = processed_s.replace('\\\\\\\\frac', '\\\\frac')\n",
        "    processed_s = processed_s.replace('\\\\\\\\dfrac', '\\\\dfrac')\n",
        "\n",
        "    # Step 2: Iteratively convert LaTeX fractions to SymPy compatible format\n",
        "    while True:\n",
        "        # Search for \\frac or \\dfrac pattern\n",
        "        match = re.search(r'\\\\(?:d?frac)\\s*\\{', processed_s)\n",
        "        if not match:\n",
        "            break\n",
        "\n",
        "        latex_frac_keyword_start = match.start()\n",
        "        num_brace_start = match.end() - 1 # Index of '{' for numerator\n",
        "\n",
        "        numerator, num_brace_end = _parse_latex_braces(processed_s, num_brace_start)\n",
        "        if numerator is None:\n",
        "            break # Malformed\n",
        "\n",
        "        denom_brace_start = num_brace_end + 1\n",
        "        if denom_brace_start >= len(processed_s) or processed_s[denom_brace_start] != '{':\n",
        "            break # Malformed\n",
        "\n",
        "        denominator, denom_brace_end = _parse_latex_braces(processed_s, denom_brace_start)\n",
        "        if denominator is None:\n",
        "            break # Malformed\n",
        "\n",
        "        # DO NOT call _insert_multiplication_stars here for numerator/denominator\n",
        "        # This will be handled in Step 3 on the fully processed string.\n",
        "\n",
        "        sympy_frac_str = f\"({numerator})/({denominator})\"\n",
        "\n",
        "        original_latex_frac = processed_s[latex_frac_keyword_start : denom_brace_end + 1]\n",
        "        processed_s = processed_s.replace(original_latex_frac, sympy_frac_str, 1)\n",
        "\n",
        "    # Step 3: Insert multiplication stars for implicit multiplication in the final string\n",
        "    processed_s = _insert_multiplication_stars(processed_s)\n",
        "\n",
        "    try:\n",
        "        return sympy.sympify(processed_s)\n",
        "    except (sympy.SympifyError, TypeError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "print(\"Evaluation functions updated with LaTeX fraction preprocessing and implicit multiplication handling (corrected)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d9d0d8b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous fix to `normalize_sympy` has been applied, and now I need to re-run the test cases to verify that the nested fraction test and all other tests pass as expected. This will confirm the correctness of the implicit multiplication handling and LaTeX fraction preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4bcccf5",
        "outputId": "c45f49b1-4592-49d0-ea2c-a31b87058f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Verifying normalize_sympy and sympy.equals() ---\n",
            "Original: '\\frac{1}{2}' -> Normalized: 1/2\n",
            "Original: '\\dfrac{1}{2}' -> Normalized: 1/2\n",
            "Original: '\\\\frac{1}{2}' -> Normalized: 1/2\n",
            "Original: '0.5' -> Normalized: 0.500000000000000\n",
            "Original: '0.5000' -> Normalized: 0.500000000000000\n",
            "\n",
            "Comparing all to base expression: 1/2\n",
            "  '\\frac{1}{2}' (1/2) equals base expression? True\n",
            "  '\\dfrac{1}{2}' (1/2) equals base expression? True\n",
            "  '\\\\frac{1}{2}' (1/2) equals base expression? True\n",
            "  '0.5' (0.500000000000000) equals base expression? True\n",
            "  '0.5000' (0.500000000000000) equals base expression? True\n",
            "\n",
            "Test: \"\\frac{1}{2}\" equals \"0.5\"? True\n",
            "\n",
            "Test: \"\\frac{1}{\\frac{2}{3}}\" equals \"3/2\"? True\n",
            "\n",
            "Test: \"\\frac{x+y}{2x-y}\" equals \"(x+y)/(2x-y)\" (symbolic)? True\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Verifying normalize_sympy and sympy.equals() ---\")\n",
        "\n",
        "test_strings = [\n",
        "    r\"\\frac{1}{2}\",\n",
        "    r\"\\dfrac{1}{2}\",\n",
        "    r\"\\\\frac{1}{2}\", # Example with double backslash if that's how it appears from some source\n",
        "    \"0.5\",\n",
        "    \"0.5000\"\n",
        "]\n",
        "\n",
        "sympy_expressions = []\n",
        "for s in test_strings:\n",
        "    normalized_expr = normalize_sympy(s)\n",
        "    sympy_expressions.append(normalized_expr)\n",
        "    print(f\"Original: '{s}' -> Normalized: {normalized_expr}\")\n",
        "\n",
        "# Compare all expressions to the first one (which should be 1/2)\n",
        "# Assuming the first expression is valid for comparison\n",
        "if sympy_expressions[0] is not None:\n",
        "    base_expr = sympy_expressions[0]\n",
        "    print(f\"\\nComparing all to base expression: {base_expr}\")\n",
        "    for i, expr in enumerate(sympy_expressions):\n",
        "        if expr is not None:\n",
        "            is_equal = base_expr.equals(expr)\n",
        "            print(f\"  '{test_strings[i]}' ({expr}) equals base expression? {is_equal}\")\n",
        "        else:\n",
        "            print(f\"  '{test_strings[i]}' ({expr}) could not be normalized, cannot compare.\")\n",
        "else:\n",
        "    print(\"Base expression could not be normalized, cannot perform comparisons.\")\n",
        "\n",
        "# Specific test: \\frac{1}{2} should equal 0.5\n",
        "expr_frac = normalize_sympy(r\"\\frac{1}{2}\")\n",
        "expr_decimal = normalize_sympy(\"0.5\")\n",
        "print(f\"\\nTest: \\\"\\\\frac{{1}}{{2}}\\\" equals \\\"0.5\\\"? {expr_frac.equals(expr_decimal) if expr_frac and expr_decimal else 'Cannot compare'}\")\n",
        "\n",
        "# Test with nested fractions (optional, but good for robustness)\n",
        "expr_nested_frac = normalize_sympy(r\"\\frac{1}{\\frac{2}{3}}\")\n",
        "expr_simplified_nested = normalize_sympy(\"3/2\")\n",
        "print(f\"\\nTest: \\\"\\\\frac{{1}}{{\\\\frac{{2}}{{3}}}}\\\" equals \\\"3/2\\\"? {expr_nested_frac.equals(expr_simplified_nested) if expr_nested_frac and expr_simplified_nested else 'Cannot compare'}\")\n",
        "\n",
        "# Test with complex expression\n",
        "expr_complex_latex = normalize_sympy(r\"\\frac{x+y}{2x-y}\")\n",
        "expr_complex_sympy = normalize_sympy(\"(x+y)/(2x-y)\")\n",
        "print(f\"\\nTest: \\\"\\\\frac{{x+y}}{{2x-y}}\\\" equals \\\"(x+y)/(2x-y)\\\" (symbolic)? {expr_complex_latex.equals(expr_complex_sympy) if expr_complex_latex and expr_complex_sympy else 'Cannot compare'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwmdHZFtZlHr",
        "outputId": "d24d0436-b768-49d8-afd4-bd2217f0ddec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# extract_boxed(\"\\\\boxed{\\\\frac{\\\\boldsymbol{x}}{1}}\")\n",
        "normalize_sympy(\"(2+x)\").equals(normalize_sympy(\"2+x\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHwhLC1QSNs7"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "The notebook will automatically detect if the model is a full model or a LoRA adapter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HScW6QaPSNs8",
        "outputId": "ecdb49ec-8a1f-4fc6-daf0-453cec9ea57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model directory: /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft/model\n",
            "Has full model: True\n",
            "Has adapter: True\n",
            "Tokenizer loaded: Qwen2TokenizerFast\n",
            "Vocab size: 151643\n"
          ]
        }
      ],
      "source": [
        "# Check if we have a full model or LoRA adapter\n",
        "model_path = Path(MODEL_DIR)\n",
        "has_full_model = (model_path / \"model.safetensors\").exists() or (model_path / \"pytorch_model.bin\").exists()\n",
        "has_adapter = (model_path / \"adapter\" / \"adapter_config.json\").exists()\n",
        "\n",
        "print(f\"Model directory: {model_path.absolute()}\")\n",
        "print(f\"Has full model: {has_full_model}\")\n",
        "print(f\"Has adapter: {has_adapter}\")\n",
        "\n",
        "if not has_full_model and not has_adapter:\n",
        "    raise ValueError(f\"No model found in {MODEL_DIR}. Please check the path.\")\n",
        "\n",
        "# Check if adapter requires peft\n",
        "if has_adapter and not PEFT_AVAILABLE:\n",
        "    raise ValueError(\n",
        "        \"LoRA adapter detected but peft is not available or incompatible.\\n\"\n",
        "        \"Please fix the version compatibility issue:\\n\"\n",
        "        \"Option 1: Upgrade packages: pip install --upgrade transformers peft\\n\"\n",
        "        \"Option 2: Install compatible versions: pip install 'transformers>=4.37.0,<4.52.0' 'peft>=0.7.0'\\n\"\n",
        "        \"Option 3: Use a full model instead of a LoRA adapter\"\n",
        "    )\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaCNebOnsfIK"
      },
      "source": [
        "## Preprocess Test Dataset (Extract Ground Truth Answers)\n",
        "\n",
        "Preprocess the test dataset to extract and normalize ground truth boxed answers in advance.\n",
        "This speeds up evaluation since we don't need to extract answers for each example during evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL4Em7VKse2Q",
        "outputId": "7794beb6-b449-45d8-abae-35b962d6f781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing test dataset...\n",
            "This may take a few minutes for large datasets.\n",
            "Processing 5000 examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing:   4%|▍         | 223/5000 [00:00<00:02, 2216.50it/s]<string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?\n",
            "Preprocessing: 100%|██████████| 5000/5000 [00:02<00:00, 2237.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Preprocessing complete! Saved to test_math_preprocessed.json\n",
            "  Processed 5000 examples\n",
            "  Valid boxed answers extracted: 4114/5000\n",
            "\n",
            "Preprocessing ready!\n"
          ]
        }
      ],
      "source": [
        "# Preprocess test dataset: extract and normalize ground truth answers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "PREPROCESSED_TEST_PATH = \"test_math_preprocessed.json\"  # Output path for preprocessed data\n",
        "\n",
        "# Check if preprocessed file already exists\n",
        "if os.path.exists(PREPROCESSED_TEST_PATH):\n",
        "    print(f\"Preprocessed file {PREPROCESSED_TEST_PATH} already exists.\")\n",
        "    print(\"Loading preprocessed data...\")\n",
        "    with open(PREPROCESSED_TEST_PATH, 'r', encoding='utf-8') as f:\n",
        "        test_data = json.load(f)\n",
        "    print(f\"Loaded {len(test_data)} preprocessed examples\")\n",
        "\n",
        "    # Show an example to verify format\n",
        "    if len(test_data) > 0:\n",
        "        example = test_data[0]\n",
        "        print(\"\\nExample preprocessed item:\")\n",
        "        print(f\"  Instruction: {example['instruction'][:100]}...\")\n",
        "        print(f\"  Gold boxed answer (raw): {example.get('gold_answer_str', 'N/A')}\")\n",
        "        print(f\"  Gold answer (normalized): {example.get('gold_answer_sympy', 'N/A')}\")\n",
        "else:\n",
        "    print(\"Preprocessing test dataset...\")\n",
        "    print(\"This may take a few minutes for large datasets.\")\n",
        "\n",
        "    # Temporarily increase the integer string conversion limit for SymPy serialization\n",
        "    # This helps with extremely large numbers that might appear in MATH dataset solutions (e.g., factorials)\n",
        "    original_int_limit = sys.get_int_max_str_digits()\n",
        "    sys.set_int_max_str_digits(0) # Set to 0 to disable the limit\n",
        "\n",
        "    try:\n",
        "        # Load original test data\n",
        "        with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "            original_test_data = json.load(f)\n",
        "\n",
        "        print(f\"Processing {len(original_test_data)} examples...\")\n",
        "\n",
        "        # Preprocess each example\n",
        "        preprocessed_data = []\n",
        "        for item in tqdm(original_test_data, desc=\"Preprocessing\"):\n",
        "            instruction = item[\"instruction\"]\n",
        "            gold_output = item[\"output\"]\n",
        "\n",
        "            # Extract boxed answer from gold output\n",
        "            gold_ans_str = extract_boxed(gold_output)\n",
        "\n",
        "            # Normalize using sympy\n",
        "            gold_ans_sym = normalize_sympy(gold_ans_str)\n",
        "\n",
        "            # Store normalized answer as string for comparison\n",
        "            # Wrap in try-except to handle potentially huge numbers that can't be stringified\n",
        "            gold_ans_sym_str = None\n",
        "            if gold_ans_sym is not None:\n",
        "                try:\n",
        "                    gold_ans_sym_str = str(gold_ans_sym)\n",
        "                except ValueError:\n",
        "                    # Fallback if the number is too large to stringify, store type or a placeholder\n",
        "                    gold_ans_sym_str = f\"SymPy object (too large to stringify: {type(gold_ans_sym)})\"\n",
        "\n",
        "                        # Create preprocessed item\n",
        "            preprocessed_item = {\n",
        "                \"instruction\": instruction,\n",
        "                \"output\": gold_output,  # Keep original output for reference\n",
        "                \"gold_answer_str\": gold_ans_str,  # Extracted boxed answer (raw string)\n",
        "                \"gold_answer_sympy\": gold_ans_sym_str,  # Normalized answer (as string for JSON)\n",
        "                \"gold_answer_sympy_obj\": None,  # Will be None, we'll reconstruct from string\n",
        "                # Preserve level and type if present in original data\n",
        "                \"level\": item.get(\"level\", None),\n",
        "                \"type\": item.get(\"type\", None)\n",
        "            }\n",
        "\n",
        "            preprocessed_data.append(preprocessed_item)\n",
        "\n",
        "        # Save preprocessed data\n",
        "        with open(PREPROCESSED_TEST_PATH, 'w', encoding='utf-8') as f:\n",
        "            json.dump(preprocessed_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"✓ Preprocessing complete! Saved to {PREPROCESSED_TEST_PATH}\")\n",
        "        print(f\"  Processed {len(preprocessed_data)} examples\")\n",
        "\n",
        "        # Statistics\n",
        "        valid_answers = sum(1 for item in preprocessed_data if item[\"gold_answer_sympy\"] is not None)\n",
        "        print(f\"  Valid boxed answers extracted: {valid_answers}/{len(preprocessed_data)}\")\n",
        "\n",
        "        # Load the preprocessed data\n",
        "        test_data = preprocessed_data\n",
        "\n",
        "    finally:\n",
        "        # Restore original integer string conversion limit\n",
        "        sys.set_int_max_str_digits(original_int_limit)\n",
        "\n",
        "print(\"\\nPreprocessing ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bb3f747",
        "outputId": "66f8b725-0b49-40e2-ee6a-f53e3db88adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data from: test_math_preprocessed.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking inconsistencies:   5%|▌         | 266/5000 [00:00<00:03, 1326.90it/s]<string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?\n",
            "Checking inconsistencies: 100%|██████████| 5000/5000 [00:03<00:00, 1391.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 1 items with potential inconsistencies.\n",
            "{\n",
            "  \"index\": 2854,\n",
            "  \"reason\": \"Normalized gold_answer_str does not equal stored gold_answer_sympy\",\n",
            "  \"gold_str\": \"\\\\frac{1}{2004!}\",\n",
            "  \"stored_sympy_str\": null,\n",
            "  \"re_normalized_sympy_obj\": \"SymPy object (too large to stringify: <class 'sympy.core.numbers.Rational'>)\",\n",
            "  \"stored_sympy_obj_type\": \"<class 'NoneType'>\",\n",
            "  \"re_normalized_sympy_obj_type\": \"<class 'sympy.core.numbers.Rational'>\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import sympy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure PREPROCESSED_TEST_PATH is defined from previous cells\n",
        "\n",
        "print(f\"Loading preprocessed data from: {PREPROCESSED_TEST_PATH}\")\n",
        "with open(PREPROCESSED_TEST_PATH, 'r', encoding='utf-8') as f:\n",
        "    preprocessed_data = json.load(f)\n",
        "\n",
        "incompatible_items = []\n",
        "\n",
        "for idx, item in enumerate(tqdm(preprocessed_data, desc=\"Checking inconsistencies\")):\n",
        "    gold_ans_str_from_json = item.get(\"gold_answer_str\")\n",
        "    gold_ans_sympy_str_from_json = item.get(\"gold_answer_sympy\")\n",
        "\n",
        "    # Case 1: gold_answer_str is present, but gold_answer_sympy is None\n",
        "    if gold_ans_str_from_json is not None and gold_ans_sympy_str_from_json is None:\n",
        "        # Re-normalize to confirm it should be None or if there was an error\n",
        "        re_normalized_sympy_obj = normalize_sympy(gold_ans_str_from_json)\n",
        "        if re_normalized_sympy_obj is not None:\n",
        "            try:\n",
        "                re_normalized_sympy_obj_to_store = str(re_normalized_sympy_obj)\n",
        "            except ValueError:\n",
        "                re_normalized_sympy_obj_to_store = f\"SymPy object (too large to stringify: {type(re_normalized_sympy_obj)})\"\n",
        "            incompatible_items.append({\n",
        "                \"index\": idx,\n",
        "                \"reason\": \"gold_answer_sympy was None, but gold_answer_str could be normalized\",\n",
        "                \"gold_str\": gold_ans_str_from_json,\n",
        "                \"stored_sympy_str\": gold_ans_sympy_str_from_json,\n",
        "                \"re_normalized_sympy_obj\": re_normalized_sympy_obj_to_store\n",
        "            })\n",
        "        continue\n",
        "\n",
        "    # Case 2: gold_answer_str is None, but gold_answer_sympy is present\n",
        "    if gold_ans_str_from_json is None and gold_ans_sympy_str_from_json is not None:\n",
        "        try:\n",
        "            re_sympified_from_stored_str = sympy.sympify(gold_ans_sympy_str_from_json)\n",
        "            if re_sympified_from_stored_str is not None:\n",
        "                try:\n",
        "                    sympified_from_stored_str_to_store = str(re_sympified_from_stored_str)\n",
        "                except ValueError:\n",
        "                    sympified_from_stored_str_to_store = f\"SymPy object (too large to stringify: {type(re_sympified_from_stored_str)})\"\n",
        "\n",
        "                incompatible_items.append({\n",
        "                    \"index\": idx,\n",
        "                    \"reason\": \"gold_answer_sympy was not None, but gold_answer_str was None\",\n",
        "                    \"gold_str\": gold_ans_str_from_json,\n",
        "                    \"stored_sympy_str\": gold_ans_sympy_str_from_json,\n",
        "                    \"sympified_from_stored\": sympified_from_stored_str_to_store\n",
        "                })\n",
        "        except (sympy.SympifyError, TypeError, SyntaxError):\n",
        "            incompatible_items.append({\n",
        "                \"index\": idx,\n",
        "                \"reason\": \"gold_answer_str was None, and stored gold_answer_sympy was malformed\",\n",
        "                \"gold_str\": gold_ans_str_from_json,\n",
        "                \"stored_sympy_str\": gold_ans_sympy_str_from_json\n",
        "            })\n",
        "        continue\n",
        "\n",
        "    # Case 3: Both are present, check for actual equivalence\n",
        "    if gold_ans_str_from_json is not None and gold_ans_sympy_str_from_json is not None:\n",
        "        re_normalized_sympy_obj = normalize_sympy(gold_ans_str_from_json)\n",
        "        try:\n",
        "            stored_sympy_obj = sympy.sympify(gold_ans_sympy_str_from_json)\n",
        "        except (sympy.SympifyError, TypeError, SyntaxError):\n",
        "            stored_sympy_obj = None # Malformed stored sympy string\n",
        "\n",
        "        # Robust comparison logic\n",
        "        is_equal = False\n",
        "        if re_normalized_sympy_obj is None or stored_sympy_obj is None:\n",
        "            # If either could not be normalized, and they are not both None, then they are not equal\n",
        "            # If both are None, they are implicitly compatible for this check\n",
        "            is_equal = (re_normalized_sympy_obj is None and stored_sympy_obj is None)\n",
        "        elif isinstance(re_normalized_sympy_obj, sympy.Expr) and isinstance(stored_sympy_obj, sympy.Expr):\n",
        "            # Both are SymPy expressions, use .equals()\n",
        "            is_equal = re_normalized_sympy_obj.equals(stored_sympy_obj)\n",
        "        else:\n",
        "            # One or both are not SymPy expressions (e.g., Python int, float, tuple, list)\n",
        "            # In this case, we can attempt a direct Python equality check.\n",
        "            is_equal = (re_normalized_sympy_obj == stored_sympy_obj)\n",
        "\n",
        "        if not is_equal:\n",
        "            # Safely convert to string for storing in the report\n",
        "            try:\n",
        "                re_normalized_sympy_obj_to_store = str(re_normalized_sympy_obj) if re_normalized_sympy_obj is not None else None\n",
        "            except ValueError:\n",
        "                re_normalized_sympy_obj_to_store = f\"SymPy object (too large to stringify: {type(re_normalized_sympy_obj)})\" if re_normalized_sympy_obj is not None else None\n",
        "\n",
        "            try:\n",
        "                stored_sympy_obj_to_store = str(stored_sympy_obj) if stored_sympy_obj is not None else None\n",
        "            except ValueError:\n",
        "                stored_sympy_obj_to_store = f\"SymPy object (too large to stringify: {type(stored_sympy_obj)})\" if stored_sympy_obj is not None else None\n",
        "\n",
        "            incompatible_items.append({\n",
        "                \"index\": idx,\n",
        "                \"reason\": \"Normalized gold_answer_str does not equal stored gold_answer_sympy\",\n",
        "                \"gold_str\": gold_ans_str_from_json,\n",
        "                \"stored_sympy_str\": stored_sympy_obj_to_store,\n",
        "                \"re_normalized_sympy_obj\": re_normalized_sympy_obj_to_store,\n",
        "                \"stored_sympy_obj_type\": str(type(stored_sympy_obj)),\n",
        "                \"re_normalized_sympy_obj_type\": str(type(re_normalized_sympy_obj))\n",
        "            })\n",
        "\n",
        "\n",
        "print(f\"\\nFound {len(incompatible_items)} items with potential inconsistencies.\")\n",
        "if incompatible_items:\n",
        "    for item in incompatible_items:\n",
        "        print(json.dumps(item, indent=2, ensure_ascii=False))\n",
        "else:\n",
        "    print(\"No obvious inconsistencies found between 'gold_answer_str' and 'gold_answer_sympy'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8JAINWGSNs8",
        "outputId": "5e2c9cfd-52e9-4137-826c-cdec3788c379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model from: /tmp/input_model/\n",
            "Base model path not found, using model directory: model\n",
            "Loading LoRA adapter from: model/adapter\n",
            "LoRA adapter loaded successfully!\n",
            "Model device: cuda:0\n",
            "Model dtype: torch.float16\n"
          ]
        }
      ],
      "source": [
        "# Load model based on type\n",
        "if has_adapter and PEFT_AVAILABLE:\n",
        "    # Load base model first (need to check config for base model path)\n",
        "    adapter_config_path = model_path / \"adapter\" / \"adapter_config.json\"\n",
        "    with open(adapter_config_path, 'r') as f:\n",
        "        adapter_config = json.load(f)\n",
        "\n",
        "    # Note: If base_model_name_or_path is a local path that doesn't exist,\n",
        "    # you may need to manually specify the base model name\n",
        "    base_model_path = adapter_config.get(\"base_model_name_or_path\", \"\")\n",
        "\n",
        "    print(f\"Loading base model from: {base_model_path}\")\n",
        "\n",
        "    # Try to load base model - if path doesn't exist, we'll try loading from MODEL_DIR\n",
        "    if os.path.exists(base_model_path):\n",
        "        base_model_path_to_use = base_model_path\n",
        "    else:\n",
        "        # Assume the full model is in MODEL_DIR (if it exists)\n",
        "        if has_full_model:\n",
        "            base_model_path_to_use = str(model_path)\n",
        "            print(f\"Base model path not found, using model directory: {base_model_path_to_use}\")\n",
        "        else:\n",
        "            # If no base model found, we need the HuggingFace model name\n",
        "            # This should be provided by the user or detected from config\n",
        "            raise ValueError(f\"Cannot find base model. Please check the adapter config.\")\n",
        "\n",
        "    # Import BitsAndBytesConfig for quantization (if available)\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quantization_config = None\n",
        "        if USE_8BIT:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=6.0,\n",
        "            )\n",
        "    except ImportError:\n",
        "        print(\"Warning: bitsandbytes not available. Setting USE_8BIT=False\")\n",
        "        USE_8BIT = False\n",
        "        quantization_config = None\n",
        "\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path_to_use if os.path.exists(base_model_path_to_use) else MODEL_DIR,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Load adapter\n",
        "    adapter_path = model_path / \"adapter\"\n",
        "    print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
        "    model = PeftModel.from_pretrained(base_model, str(adapter_path))\n",
        "    print(\"LoRA adapter loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    # Load full model\n",
        "    # Import BitsAndBytesConfig for quantization (if available)\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quantization_config = None\n",
        "        if USE_8BIT:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=6.0,\n",
        "            )\n",
        "    except ImportError:\n",
        "        print(\"Warning: bitsandbytes not available. Setting USE_8BIT=False\")\n",
        "        USE_8BIT = False\n",
        "        quantization_config = None\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_DIR,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Full model loaded successfully!\")\n",
        "\n",
        "model.eval()\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvH16QNRSNs9"
      },
      "source": [
        "## Load Test Dataset\n",
        "\n",
        "Load the test dataset in JSON format (instruction/output pairs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qPjBv0sSNs-",
        "outputId": "3e4730fb-1c19-4b87-b362-4d4fc5a1c20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5000 preprocessed test examples\n",
            "After filtering by levels ['Level 2']: 894 examples\n",
            "Randomly selected 10 samples for evaluation\n",
            "\n",
            "Statistics for selected test data:\n",
            "  Total examples: 10\n",
            "  Levels: {'Level 2': 10}\n",
            "  Types: {'Algebra': 4, 'Geometry': 1, 'Intermediate Algebra': 4, 'Prealgebra': 1}\n",
            "\n",
            "Example preprocessed test item:\n",
            "{\n",
            "  \"instruction\": \"You are a math assistant. Solve the problem step by step, explain your reasoning, and box the final answer using \\\\boxed{}.\\n\\nThe sum of seven consecutive integers is 49. What is the smallest of the seven integers?\",\n",
            "  \"output\": \"Let the smallest of these consecutive integers be $a-3$, and thus the largest will be $a+3$. The sum of all seven  integers is equal to the average of the first and last term, multiplied by the number of terms, which is $7a = 49$. Thus, $a=7$. The smallest of the seven integers is $a-3=7-3=\\\\boxed{4}$.\",\n",
            "  \"gold_answer_str\": \"4\",\n",
            "  \"gold_answer_sympy\": \"4\",\n",
            "  \"gold_answer_sympy_obj\": null,\n",
            "  \"level\": \"Level 2\",\n",
            "  \"type\": \"Algebra\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load preprocessed test dataset\n",
        "# The preprocessed data includes 'gold_answer_str' and 'gold_answer_sympy'\n",
        "with open(PREPROCESSED_TEST_PATH, 'r', encoding='utf-8') as f:\n",
        "    preprocessed_test_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(preprocessed_test_data)} preprocessed test examples\")\n",
        "\n",
        "# Apply filtering by level and type if specified\n",
        "filtered_data = preprocessed_test_data\n",
        "\n",
        "if FILTER_LEVELS is not None and len(FILTER_LEVELS) > 0:\n",
        "    filtered_data = [item for item in filtered_data if item.get(\"level\") in FILTER_LEVELS]\n",
        "    print(f\"After filtering by levels {FILTER_LEVELS}: {len(filtered_data)} examples\")\n",
        "\n",
        "if FILTER_TYPES is not None and len(FILTER_TYPES) > 0:\n",
        "    filtered_data = [item for item in filtered_data if item.get(\"type\") in FILTER_TYPES]\n",
        "    print(f\"After filtering by types {FILTER_TYPES}: {len(filtered_data)} examples\")\n",
        "\n",
        "if len(filtered_data) == 0:\n",
        "    raise ValueError(\"No examples remaining after filtering! Please check your FILTER_LEVELS and FILTER_TYPES settings.\")\n",
        "\n",
        "# Limit number of samples if specified and draw randomly\n",
        "if NUM_SAMPLES is not None:\n",
        "    if NUM_SAMPLES <= len(filtered_data):\n",
        "        # random.seed(42) # For reproducibility\n",
        "        test_data = random.sample(filtered_data, NUM_SAMPLES)\n",
        "        print(f\"Randomly selected {NUM_SAMPLES} samples for evaluation\")\n",
        "    else:\n",
        "        print(f\"NUM_SAMPLES ({NUM_SAMPLES}) is greater than filtered examples ({len(filtered_data)}). Using all filtered examples.\")\n",
        "        test_data = filtered_data\n",
        "else:\n",
        "    test_data = filtered_data\n",
        "\n",
        "# Show statistics\n",
        "if test_data:\n",
        "    levels = {}\n",
        "    types = {}\n",
        "    for item in test_data:\n",
        "        level = item.get(\"level\", \"Unknown\")\n",
        "        type_field = item.get(\"type\", \"Unknown\")\n",
        "        levels[level] = levels.get(level, 0) + 1\n",
        "        types[type_field] = types.get(type_field, 0) + 1\n",
        "\n",
        "    print(f\"\\nStatistics for selected test data:\")\n",
        "    print(f\"  Total examples: {len(test_data)}\")\n",
        "    print(f\"  Levels: {dict(sorted(levels.items()))}\")\n",
        "    print(f\"  Types: {dict(sorted(types.items()))}\")\n",
        "\n",
        "# Show an example\n",
        "print(\"\\nExample preprocessed test item:\")\n",
        "print(json.dumps(test_data[0], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-7thHEaSNs-"
      },
      "source": [
        "## Evaluation Function\n",
        "\n",
        "This function evaluates the model on the test dataset and computes exact match accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VLEnkEHESNs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1279d282-5c1d-43a9-dd85-83138a8b7691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation function defined!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, tokenizer, test_data, max_new_tokens=512, do_sample=True, temperature=0.7, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate model on test dataset.\n",
        "    Returns: exact_match_score, detailed_results\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    # Extract system prompt and user question from instruction\n",
        "    # Format: \"You are a math assistant...\\n\\n<problem>\"\n",
        "    SYSTEM_PROMPT = \"You are a math assistant. Solve the problem step by step, explain your reasoning, and box the final answer using \\boxed{}.\"\n",
        "\n",
        "    for idx, item in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
        "        instruction = item[\"instruction\"]\n",
        "        problem = instruction.split(\"\\n\\n\", 1)[1] if \"\\n\\n\" in instruction else instruction\n",
        "        system_msg = instruction.split(\"\\n\\n\", 1)[0] if \"\\n\\n\" in instruction else SYSTEM_PROMPT\n",
        "\n",
        "        # Use pre-extracted gold answers\n",
        "        gold_ans_str = item[\"gold_answer_str\"]\n",
        "        gold_ans_sympy_str = item[\"gold_answer_sympy\"]\n",
        "\n",
        "        gold_ans_sym = None\n",
        "        if gold_ans_sympy_str is not None and not gold_ans_sympy_str.startswith(\"SymPy object (too large to stringify:\"):\n",
        "            try:\n",
        "                gold_ans_sym = sympy.sympify(gold_ans_sympy_str)\n",
        "            except (sympy.SympifyError, TypeError, SyntaxError):\n",
        "                gold_ans_sym = None\n",
        "\n",
        "        # Construct prompt in Qwen3 format\n",
        "        # Format: <|im_start|>system\\n<system_prompt><|im_end|>\\n<|im_start|>user\\n<problem><|im_end|>\\n<|im_start|>assistant\\n\n",
        "        # Handle potential escaped newlines in system_msg for Qwen\n",
        "        system_msg_for_prompt = system_msg.replace('\\\\n', '\\n')\n",
        "        problem_for_prompt = problem.replace('\\\\n', '\\n')\n",
        "\n",
        "        prompt = f\"<|im_start|>system\\n{system_msg_for_prompt}<|im_end|>\\n<|im_start|>user\\n{problem_for_prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=do_sample,\n",
        "                temperature=temperature,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode only the newly generated tokens\n",
        "        generated_tokens = outputs[0][input_length:]\n",
        "        pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Extract predicted answer\n",
        "        pred_ans_str = extract_boxed(pred_text)\n",
        "        pred_ans_sym = normalize_sympy(pred_ans_str)\n",
        "\n",
        "        # Check if correct\n",
        "        is_correct = False\n",
        "        if pred_ans_sym is not None and gold_ans_sym is not None:\n",
        "            # Robust comparison for symbolic expressions\n",
        "            if isinstance(pred_ans_sym, sympy.Expr) and isinstance(gold_ans_sym, sympy.Expr):\n",
        "                is_correct = gold_ans_sym.equals(pred_ans_sym)\n",
        "            else:\n",
        "                # Fallback for non-sympy expressions like tuples or numbers\n",
        "                is_correct = (gold_ans_sym == pred_ans_sym)\n",
        "        elif (pred_ans_str == \"\" or pred_ans_str is None) and (gold_ans_str == \"\" or gold_ans_str is None):\n",
        "            # Both are empty/None, consider them equal\n",
        "            is_correct = True\n",
        "\n",
        "        results.append({\n",
        "            \"problem\": problem[:100] + \"...\" if len(problem) > 100 else problem,\n",
        "            \"predicted\": pred_ans_str,\n",
        "            \"gold\": gold_ans_str,\n",
        "            \"predicted_full\": pred_text[:200] + \"...\" if len(pred_text) > 200 else pred_text,\n",
        "            \"correct\": is_correct\n",
        "        })\n",
        "\n",
        "        if verbose and idx < 5:  # Show first 5 examples\n",
        "            print(f\"\\n--- Example {idx + 1} ---\")\n",
        "            print(f\"Problem: {problem[:150]}...\")\n",
        "            print(f\"Predicted answer: {pred_ans_str}\")\n",
        "            print(f\"Gold answer: {gold_ans_str}\")\n",
        "            print(f\"Correct: {is_correct}\")\n",
        "            print(f\"Generated text (first 200 chars): {pred_text[:200]}...\")\n",
        "\n",
        "    exact_match = correct / total if total > 0 else 0.0\n",
        "    return exact_match, results\n",
        "\n",
        "print(\"Evaluation function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTYgA8dBSNs_"
      },
      "source": [
        "## Run Evaluation\n",
        "\n",
        "Evaluate the model on the test dataset and compute exact match accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "eJeeZh8vSNtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61af697-944e-4f9f-88ed-30b9669294ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation...\n",
            "Total test examples: 10\n",
            "Max new tokens: 128\n",
            "Do sampling: False\n",
            "Temperature: 0.7\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  10%|█         | 1/10 [00:25<03:48, 25.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1 ---\n",
            "Problem: The sum of seven consecutive integers is 49. What is the smallest of the seven integers?...\n",
            "Predicted answer: 4\n",
            "Gold answer: 4\n",
            "Correct: True\n",
            "Generated text (first 200 chars): Let the seven consecutive integers be $n-3$, $n-2$, $n-1$, $n$, $n+1$, $n+2$, and $n+3$.  The sum of these seven integers is $7n$, and we are given that this sum is 49.  Therefore, $7n=49$, so $n=\\box...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  20%|██        | 2/10 [00:50<03:22, 25.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 2 ---\n",
            "Problem: Compute $\\sin 330^\\circ$....\n",
            "Predicted answer: -\\frac{1}{2}\n",
            "Gold answer: -\\frac{1}{2}\n",
            "Correct: True\n",
            "Generated text (first 200 chars): We have that\n",
            "\\[\\sin 330^\\circ = \\sin (360^\\circ - 30^\\circ) = -\\sin 30^\\circ = \\boxed{-\\frac{1}{2}}.\\]Note that $\\sin 330^\\circ$ is the same as $\\sin (-30^\\circ),$ so we can also use the fact that $\\s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  30%|███       | 3/10 [01:15<02:56, 25.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 3 ---\n",
            "Problem: If $\\&x$ is defined as $\\&x = x + 5$ and $\\#x$ is defined as $\\#x = x^2$ , what is the value of $\\#(\\&4)$?...\n",
            "Predicted answer: 81\n",
            "Gold answer: 81\n",
            "Correct: True\n",
            "Generated text (first 200 chars): We have that $\\&4 = 4+5 = 9$ and $\\#9 = 9^2 = 81$.  Therefore, $\\#(\\&4) = \\#9 = \\boxed{81}$.  (We could have also done this by first finding $\\#4 = 4^2 = 16$ and then finding $\\&16 = 16+5 = 21$ and th...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  40%|████      | 4/10 [01:41<02:31, 25.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 4 ---\n",
            "Problem: Find $w$ if $|3-2i+w|=0$....\n",
            "Predicted answer: -3+2i\n",
            "Gold answer: -3+2i\n",
            "Correct: True\n",
            "Generated text (first 200 chars): If $|3-2i+w|=0$, then $3-2i+w=0$.  Therefore, $w=-3+2i$.  So, $w=\\boxed{-3+2i}$.  (Note that $w$ is a complex number, not a real number.)  (The absolute value of a complex number is the distance from ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  50%|█████     | 5/10 [02:06<02:06, 25.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 5 ---\n",
            "Problem: What is the largest prime factor of 78?...\n",
            "Predicted answer: 13\n",
            "Gold answer: 13\n",
            "Correct: True\n",
            "Generated text (first 200 chars): We can factor 78 as $2\\cdot39$.  We can factor 39 as $3\\cdot13$.  Therefore, the largest prime factor of 78 is $\\boxed{13}$.  (We could also have factored 78 as $2\\cdot3\\cdot13$ and gotten the same an...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 10/10 [04:12<00:00, 25.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "Exact Match Score: 0.0000 (0.00%)\n",
            "Correct: 7\n",
            "Total: 10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting evaluation...\")\n",
        "print(f\"Total test examples: {len(test_data)}\")\n",
        "print(f\"Max new tokens: {MAX_NEW_TOKENS}\")\n",
        "print(f\"Do sampling: {DO_SAMPLE}\")\n",
        "print(f\"Temperature: {TEMPERATURE}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "exact_match_score, detailed_results = evaluate_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_data,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    do_sample=DO_SAMPLE,\n",
        "    temperature=TEMPERATURE,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Exact Match Score: {exact_match_score:.4f} ({exact_match_score*100:.2f}%)\")\n",
        "print(f\"Correct: {sum(r['correct'] for r in detailed_results)}\")\n",
        "print(f\"Total: {len(detailed_results)}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2mk3L1XSNtA"
      },
      "source": [
        "## Save Results\n",
        "\n",
        "Save the evaluation results to a file for later analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIsw0Ms3SNtA"
      },
      "outputs": [],
      "source": [
        "# Save results to JSON file\n",
        "results_file = \"evaluation_results.json\"\n",
        "results_summary = {\n",
        "    \"exact_match_score\": exact_match_score,\n",
        "    \"total_examples\": len(detailed_results),\n",
        "    \"correct\": sum(r['correct'] for r in detailed_results),\n",
        "    \"detailed_results\": detailed_results\n",
        "}\n",
        "\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Results saved to {results_file}\")\n",
        "\n",
        "# Show some statistics\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"  Accuracy: {exact_match_score*100:.2f}%\")\n",
        "correct_count = sum(r['correct'] for r in detailed_results)\n",
        "incorrect_count = len(detailed_results) - correct_count\n",
        "print(f\"  Correct: {correct_count}\")\n",
        "print(f\"  Incorrect: {incorrect_count}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}