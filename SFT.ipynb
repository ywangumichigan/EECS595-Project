{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PQTWEwxKZX5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8ee721e2-1f0a-4280-e29c-ee39fded4bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (1.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# Uncomment if running in Colab:\n",
        "!pip install transformers datasets peft accelerate bitsandbytes sympy tqdm\n",
        "!pip install -U bitsandbytes\n",
        "# For local environments, install via: pip install transformers datasets peft accelerate bitsandbytes sympy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yqpstHEaZnED"
      },
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    TrainerCallback,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import sympy\n",
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "# Optional: For Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    drive = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1o0RUgPemAd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eeb64bc-0889-45bf-f15a-91a345657d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab_Notebooks/CSE595_Proj\n",
            "Current working directory: /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj\n",
            "Configuration loaded. Output directory: /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/math-sft-models\n",
            "Using model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "# =============\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Can change to other models\n",
        "USE_8BIT = True  # Use 8-bit quantization to save memory\n",
        "\n",
        "# Data paths - using preprocessed JSONL files\n",
        "TRAIN_DATA_PATH = \"MATH_train_full.jsonl\"\n",
        "TEST_DATA_PATH = \"MATH_test_full.jsonl\"\n",
        "\n",
        "# Output paths\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_OUTPUT_DIR = \"/content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/math-sft-models\"\n",
        "    %cd /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    BASE_OUTPUT_DIR = \"./math-sft-models\"\n",
        "\n",
        "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
        "CHECKPOINT_DIR = os.path.join(BASE_OUTPUT_DIR, \"checkpoints\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_OUTPUT_DIR, \"final-model\")\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_EPOCHS = 1\n",
        "WARMUP_STEPS = 50\n",
        "MAX_LENGTH = 1024\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Evaluation\n",
        "VAL_SIZE = 50  # Size of validation set\n",
        "TEST_SIZE = 500  # Number of test samples to evaluate\n",
        "\n",
        "print(f\"Configuration loaded. Output directory: {BASE_OUTPUT_DIR}\")\n",
        "print(f\"Using model: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "skMLvIs9mGSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10896139-a079-4c0b-ec9b-2a95bd46eaf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Loaded 7500 training examples\n",
            "Loading test data...\n",
            "Loaded 5000 test examples\n",
            "Training examples: 7500\n",
            "Validation examples: 50\n",
            "Test examples: 500\n"
          ]
        }
      ],
      "source": [
        "# Load preprocessed data from JSONL files\n",
        "# =======================================\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    \"\"\"Load data from JSONL file.\"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "train_data = load_jsonl(TRAIN_DATA_PATH)\n",
        "print(f\"Loaded {len(train_data)} training examples\")\n",
        "\n",
        "print(\"Loading test data...\")\n",
        "test_data = load_jsonl(TEST_DATA_PATH)\n",
        "print(f\"Loaded {len(test_data)} test examples\")\n",
        "\n",
        "# Convert to HuggingFace dataset format\n",
        "train_ds = load_dataset('json', data_files=TRAIN_DATA_PATH, split='train')\n",
        "test_ds = load_dataset('json', data_files=TEST_DATA_PATH, split='train')\n",
        "\n",
        "# Create validation set from test set\n",
        "val_ds = test_ds.select(range(VAL_SIZE))\n",
        "test_ds = test_ds.select(range(VAL_SIZE, VAL_SIZE + TEST_SIZE))\n",
        "\n",
        "print(f\"Training examples: {len(train_ds)}\")\n",
        "print(f\"Validation examples: {len(val_ds)}\")\n",
        "print(f\"Test examples: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z4KR5PieZvp1",
        "outputId": "ce86f10a-8da7-4980-aeed-633dc7ed5631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System prompt: You are a math assistant. Solve the problem step by step, explain your reasoning, and box the final answer using \\boxed{}.\n"
          ]
        }
      ],
      "source": [
        "# System prompt for math assistant\n",
        "SYSTEM_PROMPT = \"You are a math assistant. Solve the problem step by step, explain your reasoning, and box the final answer using \\\\boxed{}.\"\n",
        "\n",
        "print(f\"System prompt: {SYSTEM_PROMPT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pIRtxLvZ9dZ",
        "outputId": "50d90861-4e7d-430a-ece7-0ee15b583462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Applying LoRA configuration...\n",
            "Trainable parameters: 4,505,600 (0.41%)\n",
            "Total parameters: 1,104,553,984\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer and model\n",
        "# ===============================\n",
        "\n",
        "print(f\"Loading tokenizer and model: {MODEL_NAME}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Configure quantization if using 8-bit\n",
        "quantization_config = None\n",
        "if USE_8BIT:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        llm_int8_threshold=6.0,\n",
        "    )\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "print(\"Applying LoRA configuration...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "0c5657658e6a485d988bbe896308a42d",
            "e4eba552c48045279a5660676d434a8a",
            "4946e61020d142b39aa62126b78ec909",
            "693504317cb6468aa8df27fc524f0f08",
            "bbd776eb364e4cf58a1c3fc98707f1c2",
            "ce9cb0207899405984ac9312018e26ea",
            "2963794a6a234bcea030daa7b4733bf8",
            "b58cee6bb6c94394887f41ad431e017b",
            "8a7cb3548d4e4cb08ed4c85fe77088b0",
            "9df2c77edb1b4086838a8ed2ac4fa5ae",
            "57b106f822664dc586a06a23246e5fcc"
          ]
        },
        "id": "-g8eIok5asL1",
        "outputId": "17c79cac-e1d7-45fc-b44b-f0e400be4f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing validation set:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c5657658e6a485d988bbe896308a42d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized training examples: 7500\n",
            "Tokenized validation examples: 50\n"
          ]
        }
      ],
      "source": [
        "# Tokenization function\n",
        "# =====================\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the messages format for TinyLlama chat template.\"\"\"\n",
        "    texts = []\n",
        "    for msg_list in examples[\"messages\"]:\n",
        "        # Format: system, user, assistant\n",
        "        system_msg = msg_list[0][\"content\"]\n",
        "        user_msg = msg_list[1][\"content\"]\n",
        "        assistant_msg = msg_list[2][\"content\"]\n",
        "\n",
        "        # TinyLlama chat format\n",
        "        text = f\"<|system|>\\n{system_msg}<|end|>\\n<|user|>\\n{user_msg}<|end|>\\n<|assistant|>\\n{assistant_msg}<|end|>\"\n",
        "        texts.append(text)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    # Labels are same as input_ids for causal LM\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_train = train_ds.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_ds.column_names,\n",
        "    desc=\"Tokenizing training set\"\n",
        ")\n",
        "tokenized_val = val_ds.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_ds.column_names,\n",
        "    desc=\"Tokenizing validation set\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenized training examples: {len(tokenized_train)}\")\n",
        "print(f\"Tokenized validation examples: {len(tokenized_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5KWaa1Fcs7I",
        "outputId": "48296827-2bc7-4afd-ec96-97283b2fd6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing evaluation functions...\n",
            "  Input: Some text \\boxed{1/2+3} more text...\n",
            "  Extracted: 1/2+3, Expected: 1/2+3\n",
            "  Input: \\boxed{42}...\n",
            "  Extracted: 42, Expected: 42\n",
            "  Input: No boxed answer here...\n",
            "  Extracted: None, Expected: None\n",
            "Evaluation functions ready!\n"
          ]
        }
      ],
      "source": [
        "# Evaluation utilities\n",
        "# ====================\n",
        "\n",
        "def extract_boxed(latex_string):\n",
        "    \"\"\"Extract content from \\\\boxed{} in LaTeX string.\"\"\"\n",
        "    if not latex_string:\n",
        "        return None\n",
        "\n",
        "    match = re.search(r'\\\\boxed\\s*\\{', latex_string, re.IGNORECASE)\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    start_index = match.end()\n",
        "    brace_count = 1\n",
        "    content = []\n",
        "\n",
        "    for i in range(start_index, len(latex_string)):\n",
        "        char = latex_string[i]\n",
        "        if char == '{':\n",
        "            brace_count += 1\n",
        "            content.append(char)\n",
        "        elif char == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                return \"\".join(content)\n",
        "            else:\n",
        "                content.append(char)\n",
        "        else:\n",
        "            content.append(char)\n",
        "    return None\n",
        "\n",
        "def normalize_sympy(s):\n",
        "    \"\"\"Normalize mathematical expression using sympy.\"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        return sympy.sympify(s)\n",
        "    except (sympy.SympifyError, TypeError):\n",
        "        return None\n",
        "\n",
        "def compute_em(eval_pred):\n",
        "    \"\"\"Compute exact match metric during evaluation.\"\"\"\n",
        "    predictions, _ = eval_pred\n",
        "    correct = 0\n",
        "    batch_size = 4\n",
        "\n",
        "    for i in range(0, len(predictions), batch_size):\n",
        "        batch_preds = predictions[i:i+batch_size]\n",
        "        batch_texts = [\n",
        "            tokenizer.decode(p.tolist(), skip_special_tokens=True)\n",
        "            for p in batch_preds\n",
        "        ]\n",
        "        # Access messages safely - val_ds has the messages structure from JSONL\n",
        "        batch_indices = list(range(i, min(i+batch_size, len(val_ds))))\n",
        "        batch_golds = []\n",
        "        for idx in batch_indices:\n",
        "            ex = val_ds[idx]\n",
        "            if \"messages\" in ex and len(ex[\"messages\"]) > 2:\n",
        "                batch_golds.append(ex[\"messages\"][2][\"content\"])\n",
        "            else:\n",
        "                batch_golds.append(\"\")  # Fallback if structure is unexpected\n",
        "\n",
        "        for pred_text, gold_text in zip(batch_texts, batch_golds):\n",
        "            pred_ans_str = extract_boxed(pred_text)\n",
        "            gold_ans_str = extract_boxed(gold_text)\n",
        "\n",
        "            pred_ans_sym = normalize_sympy(pred_ans_str)\n",
        "            gold_ans_sym = normalize_sympy(gold_ans_str)\n",
        "\n",
        "            if pred_ans_sym is not None and gold_ans_sym is not None and pred_ans_sym == gold_ans_sym:\n",
        "                correct += 1\n",
        "            elif (pred_ans_str == \"\" or pred_ans_str is None) and (gold_ans_str == \"\" or gold_ans_str is None):\n",
        "                correct += 1\n",
        "\n",
        "        del batch_preds, batch_texts, batch_golds\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return {\"em\": correct / len(predictions)}\n",
        "\n",
        "# Test the evaluation functions\n",
        "print(\"Testing evaluation functions...\")\n",
        "test_cases = [\n",
        "    (\"Some text \\\\boxed{1/2+3} more text\", \"1/2+3\"),\n",
        "    (\"\\\\boxed{42}\", \"42\"),\n",
        "    (\"No boxed answer here\", None)\n",
        "]\n",
        "for text, expected in test_cases:\n",
        "    result = extract_boxed(text)\n",
        "    print(f\"  Input: {text[:50]}...\")\n",
        "    print(f\"  Extracted: {result}, Expected: {expected}\")\n",
        "print(\"Evaluation functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "H8xr_dwV211G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4760d2b7-2b8d-45ff-8f8b-f0959f6a46dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Callbacks configured!\n"
          ]
        }
      ],
      "source": [
        "# Training callbacks\n",
        "# ==================\n",
        "\n",
        "class EMPrintCallback(TrainerCallback):\n",
        "    \"\"\"Print exact match score during evaluation.\"\"\"\n",
        "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "        em_score = metrics.get('eval_em', 0)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Step {state.global_step} | Validation Exact Match: {em_score:.2%}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "class ModelCheckpointCallback(TrainerCallback):\n",
        "    \"\"\"Save model checkpoints to specified directory.\"\"\"\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        checkpoint_dir = f\"checkpoint-{state.global_step}\"\n",
        "        local_path = os.path.join(args.output_dir, checkpoint_dir)\n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"Checkpoint saved: {checkpoint_dir}\")\n",
        "\n",
        "callbacks = [\n",
        "    EMPrintCallback(),\n",
        "    ModelCheckpointCallback(BASE_OUTPUT_DIR),\n",
        "    EarlyStoppingCallback(early_stopping_patience=3)\n",
        "]\n",
        "\n",
        "print(\"Callbacks configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeA51BmIlmDC",
        "outputId": "409d2503-09d2-4dd2-8aba-7f919b6f45d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer configured!\n",
            "Total training steps: 1875\n",
            "Evaluation will run every 100 steps\n"
          ]
        }
      ],
      "source": [
        "# Training arguments and trainer setup\n",
        "# =====================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CHECKPOINT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=300,\n",
        "    save_total_limit=2,\n",
        "    metric_for_best_model=\"eval_em\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=False,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    report_to=\"none\",\n",
        "    dataloader_num_workers=1,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=compute_em,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "print(\"Trainer configured!\")\n",
        "print(f\"Total training steps: {len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}\")\n",
        "print(f\"Evaluation will run every {training_args.eval_steps} steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VBc_3i5t2Qdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "31756e34-5712-487a-f0b8-6b9fce26f5c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Training examples: 7500\n",
            "Validation examples: 50\n",
            "Output directory: /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/math-sft-models/checkpoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='101' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 101/1875 02:49 < 50:30, 0.59 it/s, Epoch 0.05/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/50 00:08 < 00:02, 4.17 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 4.76 GiB. GPU 0 has a total capacity of 22.16 GiB of which 4.60 GiB is free. Process 17559 has 17.56 GiB memory in use. Of the allocated memory 16.59 GiB is allocated by PyTorch, and 741.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2812359207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Output directory: {CHECKPOINT_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2754\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2756\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2757\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3219\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4488\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4489\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4490\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4491\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4710\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4711\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_eval_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4712\u001b[0;31m                     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4714\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         return type(tensors)(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Let's figure out the new shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.76 GiB. GPU 0 has a total capacity of 22.16 GiB of which 4.60 GiB is free. Process 17559 has 17.56 GiB memory in use. Of the allocated memory 16.59 GiB is allocated by PyTorch, and 741.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "# ===============\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Training examples: {len(tokenized_train)}\")\n",
        "print(f\"Validation examples: {len(tokenized_val)}\")\n",
        "print(f\"Output directory: {CHECKPOINT_DIR}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9KNoiX82WsX"
      },
      "outputs": [],
      "source": [
        "# Save final model\n",
        "# ================\n",
        "\n",
        "best_checkpoint = trainer.state.best_model_checkpoint\n",
        "if best_checkpoint:\n",
        "    print(f\"Best checkpoint: {best_checkpoint}\")\n",
        "\n",
        "    # Save the final model\n",
        "    final_model_path = FINAL_MODEL_DIR\n",
        "    os.makedirs(final_model_path, exist_ok=True)\n",
        "\n",
        "    # Save the PEFT model\n",
        "    trainer.model.save_pretrained(final_model_path)\n",
        "    tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "    print(f\"Final model saved to: {final_model_path}\")\n",
        "\n",
        "    # Also save training metrics\n",
        "    metrics_path = os.path.join(BASE_OUTPUT_DIR, \"training_metrics.json\")\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump({\n",
        "            \"best_checkpoint\": best_checkpoint,\n",
        "            \"best_metric\": trainer.state.best_metric,\n",
        "            \"total_steps\": trainer.state.global_step,\n",
        "        }, f, indent=2)\n",
        "    print(f\"Training metrics saved to: {metrics_path}\")\n",
        "else:\n",
        "    print(\"No best checkpoint found. Saving current model...\")\n",
        "    trainer.model.save_pretrained(FINAL_MODEL_DIR)\n",
        "    tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "    print(f\"Model saved to: {FINAL_MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaC_4toXdWg2"
      },
      "outputs": [],
      "source": [
        "# Load model for evaluation\n",
        "# ==========================\n",
        "\n",
        "# Load the best model checkpoint\n",
        "if best_checkpoint and os.path.exists(best_checkpoint):\n",
        "    print(f\"Loading best model from: {best_checkpoint}\")\n",
        "    eval_model = AutoModelForCausalLM.from_pretrained(\n",
        "        best_checkpoint,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config if USE_8BIT else None,\n",
        "        torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    # If it's a PEFT model, we need to load it differently\n",
        "    if os.path.exists(os.path.join(best_checkpoint, \"adapter_config.json\")):\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quantization_config if USE_8BIT else None,\n",
        "            torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        eval_model = PeftModel.from_pretrained(base_model, best_checkpoint)\n",
        "else:\n",
        "    print(\"Using current model for evaluation\")\n",
        "    eval_model = model\n",
        "\n",
        "eval_model.eval()\n",
        "eval_tokenizer = tokenizer\n",
        "\n",
        "print(\"Model loaded for evaluation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Lq001F5Hmt"
      },
      "outputs": [],
      "source": [
        "# Comprehensive evaluation on test set\n",
        "# =====================================\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, num_samples=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate model on test dataset.\n",
        "    Returns: exact_match_score, detailed_results\n",
        "    \"\"\"\n",
        "    if num_samples is None:\n",
        "        num_samples = len(dataset)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    for idx, ex in enumerate(tqdm(dataset.select(range(num_samples)), desc=\"Evaluating\")):\n",
        "        problem = ex[\"messages\"][1][\"content\"]  # user message\n",
        "        gold_solution = ex[\"messages\"][2][\"content\"]  # assistant message\n",
        "        gold_ans_str = extract_boxed(gold_solution)\n",
        "        gold_ans_sym = normalize_sympy(gold_ans_str)\n",
        "\n",
        "        # Generate prediction\n",
        "        prompt = f\"<|system|>\\n{SYSTEM_PROMPT}<|end|>\\n<|user|>\\n{problem}<|end|>\\n<|assistant|>\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=False,\n",
        "                temperature=1.0,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the assistant response\n",
        "        if \"<|assistant|>\" in pred_text:\n",
        "            pred_text = pred_text.split(\"<|assistant|>\")[-1]\n",
        "\n",
        "        pred_ans_str = extract_boxed(pred_text)\n",
        "        pred_ans_sym = normalize_sympy(pred_ans_str)\n",
        "\n",
        "        # Check if correct\n",
        "        is_correct = False\n",
        "        if pred_ans_sym is not None and gold_ans_sym is not None:\n",
        "            is_correct = (pred_ans_sym == gold_ans_sym)\n",
        "        elif (pred_ans_str == \"\" or pred_ans_str is None) and (gold_ans_str == \"\" or gold_ans_str is None):\n",
        "            is_correct = True\n",
        "\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        results.append({\n",
        "            \"problem\": problem[:100] + \"...\" if len(problem) > 100 else problem,\n",
        "            \"predicted\": pred_ans_str,\n",
        "            \"gold\": gold_ans_str,\n",
        "            \"correct\": is_correct\n",
        "        })\n",
        "\n",
        "        if verbose and idx < 3:  # Show first 3 examples\n",
        "            print(f\"\\n--- Example {idx + 1} ---\")\n",
        "            print(f\"Problem: {problem[:150]}...\")\n",
        "            print(f\"Predicted answer: {pred_ans_str}\")\n",
        "            print(f\"Gold answer: {gold_ans_str}\")\n",
        "            print(f\"Correct: {is_correct}\")\n",
        "\n",
        "    exact_match = correct / total if total > 0 else 0.0\n",
        "    return exact_match, results\n",
        "\n",
        "print(\"Evaluating on test set...\")\n",
        "test_em, test_results = evaluate_model(eval_model, eval_tokenizer, test_ds, num_samples=TEST_SIZE, verbose=True)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Test Set Exact Match Score: {test_em:.2%}\")\n",
        "print(f\"Correct: {sum(r['correct'] for r in test_results)}/{len(test_results)}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Save evaluation results\n",
        "eval_results_path = os.path.join(BASE_OUTPUT_DIR, \"evaluation_results.json\")\n",
        "with open(eval_results_path, 'w') as f:\n",
        "    json.dump({\n",
        "        \"exact_match\": test_em,\n",
        "        \"correct\": sum(r['correct'] for r in test_results),\n",
        "        \"total\": len(test_results),\n",
        "        \"results\": test_results[:10]  # Save first 10 for inspection\n",
        "    }, f, indent=2)\n",
        "print(f\"\\nEvaluation results saved to: {eval_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tM98aid58sx"
      },
      "outputs": [],
      "source": [
        "# Performance summary and next steps\n",
        "# ===================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING AND EVALUATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Training examples: {len(train_ds)}\")\n",
        "print(f\"Test examples evaluated: {len(test_ds)}\")\n",
        "print(f\"Test Exact Match: {test_em:.2%}\")\n",
        "print(f\"Best checkpoint: {best_checkpoint if 'best_checkpoint' in locals() else 'N/A'}\")\n",
        "print(f\"Model saved to: {FINAL_MODEL_DIR}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Review evaluation results in:\", eval_results_path)\n",
        "print(\"2. To use the model for inference, load from:\", FINAL_MODEL_DIR)\n",
        "print(\"3. For RAG integration, see the RAG notebook or create a new one\")\n",
        "print(\"\\nTo load the model later:\")\n",
        "print(f\"  from peft import PeftModel\")\n",
        "print(f\"  base_model = AutoModelForCausalLM.from_pretrained('{MODEL_NAME}')\")\n",
        "print(f\"  model = PeftModel.from_pretrained(base_model, '{FINAL_MODEL_DIR}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcqhrWGJVbS5"
      },
      "source": [
        "# RAG Integration (Future Work)\n",
        "\n",
        "This section can be used to integrate RAG components with the fine-tuned model.\n",
        "\n",
        "Key components needed:\n",
        "1. **Knowledge Base**: Mathematical concepts, formulas, theorems\n",
        "2. **Embedding Model**: For encoding queries and documents\n",
        "3. **Vector Store**: FAISS, ChromaDB, or similar\n",
        "4. **Retrieval**: Semantic search to find relevant context\n",
        "5. **Generation**: Use retrieved context with the fine-tuned model\n",
        "\n",
        "See `595 RAG.ipynb` for a basic RAG implementation example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1AuCCPodgzx"
      },
      "outputs": [],
      "source": [
        "# Example: Loading the fine-tuned model for inference\n",
        "# ===================================================\n",
        "\n",
        "# To load the saved model later, use:\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "model = PeftModel.from_pretrained(base_model, FINAL_MODEL_DIR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Example inference\n",
        "def solve_math_problem(problem_text):\n",
        "    prompt = f\"<|system|>\\n{SYSTEM_PROMPT}<|end|>\\n<|user|>\\n{problem_text}<|end|>\\n<|assistant|>\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=False,\n",
        "            temperature=1.0,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"<|assistant|>\" in response:\n",
        "        response = response.split(\"<|assistant|>\")[-1]\n",
        "    return response\n",
        "\n",
        "# Test with a sample problem\n",
        "# sample_problem = \"What is 2 + 2?\"\n",
        "# answer = solve_math_problem(sample_problem)\n",
        "# print(answer)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c5657658e6a485d988bbe896308a42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4eba552c48045279a5660676d434a8a",
              "IPY_MODEL_4946e61020d142b39aa62126b78ec909",
              "IPY_MODEL_693504317cb6468aa8df27fc524f0f08"
            ],
            "layout": "IPY_MODEL_bbd776eb364e4cf58a1c3fc98707f1c2"
          }
        },
        "e4eba552c48045279a5660676d434a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce9cb0207899405984ac9312018e26ea",
            "placeholder": "​",
            "style": "IPY_MODEL_2963794a6a234bcea030daa7b4733bf8",
            "value": "Tokenizing validation set: 100%"
          }
        },
        "4946e61020d142b39aa62126b78ec909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b58cee6bb6c94394887f41ad431e017b",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a7cb3548d4e4cb08ed4c85fe77088b0",
            "value": 50
          }
        },
        "693504317cb6468aa8df27fc524f0f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9df2c77edb1b4086838a8ed2ac4fa5ae",
            "placeholder": "​",
            "style": "IPY_MODEL_57b106f822664dc586a06a23246e5fcc",
            "value": " 50/50 [00:00&lt;00:00, 912.63 examples/s]"
          }
        },
        "bbd776eb364e4cf58a1c3fc98707f1c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce9cb0207899405984ac9312018e26ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2963794a6a234bcea030daa7b4733bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b58cee6bb6c94394887f41ad431e017b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a7cb3548d4e4cb08ed4c85fe77088b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9df2c77edb1b4086838a8ed2ac4fa5ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b106f822664dc586a06a23246e5fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}