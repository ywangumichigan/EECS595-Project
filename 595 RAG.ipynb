{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f10a0b6-cf76-4d8b-94af-2f4ea149a5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is the number of units in the distance between $(2,5)$ and $(-6,-1)$?',\n",
       " 'level': 'Level 2',\n",
       " 'type': 'Algebra',\n",
       " 'solution': 'We use the distance formula: $\\\\sqrt{(-6 - 2)^2 + (-1 - 5)^2},$ so then we find that $\\\\sqrt{64 + 36} = \\\\boxed{10}$.\\n\\n- OR -\\n\\nWe note that the points $(2, 5)$, $(-6, -1)$, and $(2, -1)$ form a right triangle with legs of length 6 and 8. This is a Pythagorean triple, so the length of the hypotenuse must be $\\\\boxed{10}$.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "subjects = [\n",
    "    \"algebra\",\n",
    "    \"counting_and_probability\",\n",
    "    \"geometry\",\n",
    "    \"intermediate_algebra\",\n",
    "    \"number_theory\",\n",
    "    \"prealgebra\",\n",
    "    \"precalculus\"\n",
    "]\n",
    "\n",
    "datasets = [load_dataset(\"EleutherAI/hendrycks_math\", subject) for subject in subjects]\n",
    "\n",
    "train_datasets = [ds[\"train\"] for ds in datasets]\n",
    "test_datasets = [ds[\"test\"] for ds in datasets]\n",
    "\n",
    "combined_train = concatenate_datasets(train_datasets)\n",
    "combined_test = concatenate_datasets(test_datasets)\n",
    "\n",
    "combined_dataset = DatasetDict({\n",
    "    \"train\": combined_train,\n",
    "    \"test\": combined_test\n",
    "})\n",
    "\n",
    "\n",
    "combined_dataset[\"train\"] = combined_dataset[\"train\"].shuffle(seed=42)\n",
    "combined_dataset[\"test\"] = combined_dataset[\"test\"].shuffle(seed=42)\n",
    "combined_dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fe8972-61ab-47ae-81c6-76cfa5d98ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected: Algebra\n",
      "Collected: Calculus\n",
      "Collected: Derivative\n",
      "Collected: Integral\n",
      "Collected: Matrix (mathematics)\n",
      "Collected: Probability\n",
      "Collected: Statistics\n",
      "Collected: Geometry\n",
      "Collected: Trigonometry\n",
      "Collected: Number theory\n",
      "Collected: linear algebra\n",
      "Collected: Linear Regression\n",
      "12 articles collected.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "topics = [\n",
    "    \"Algebra\", \"Calculus\", \"Derivative\", \"Integral\",\n",
    "    \"Matrix (mathematics)\", \"Probability\", \"Statistics\",\n",
    "    \"Geometry\", \"Trigonometry\", \"Number theory\", \"linear algebra\", \"Linear Regression\"\n",
    "]\n",
    "\n",
    "wiki_corpus = []\n",
    "\n",
    "for topic in topics:\n",
    "    try:\n",
    "        summary = wikipedia.page(topic, auto_suggest=False).content\n",
    "        wiki_corpus.append({\"title\": topic, \"content\": summary})\n",
    "        print(f\"Collected: {topic}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {topic}: {e}\")\n",
    "\n",
    "print(len(wiki_corpus), \"articles collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f512c8-f814-431a-9f62-39c91dc56d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Derivative',\n",
       " 'content': 'In mathematics, the derivative is a fundamental tool that quantifies the sensitivity to change of a function\\'s output with respect to its input. The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. The derivative is often described as the instantaneous rate of change, the ratio of the instantaneous change in the dependent variable to that of the independent variable. The process of finding a derivative is called differentiation.\\nThere are multiple different notations for differentiation. Leibniz notation, named after Gottfried Wilhelm Leibniz, is represented as the ratio of two differentials, whereas prime notation is written by adding a prime mark. Higher order notations represent repeated differentiation, and they are usually denoted in Leibniz notation by adding superscripts to the differentials, and in prime notation by adding additional prime marks. Higher order derivatives are used in physics; for example, the first derivative  with respect to time of the position of a moving object is its velocity, and the second derivative is its acceleration.\\nDerivatives can be generalized to functions of several real variables. In this case, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables.  It can be calculated in terms of the partial derivatives with respect to the independent variables.  For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.\\n\\n\\n== Definition ==\\n\\n\\n=== As a limit ===\\nA function of a real variable \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n  \\n is differentiable at a point \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n of its domain, if its domain contains an open interval containing \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060, and the limit\\n\\n  \\n    \\n      \\n        L\\n        =\\n        \\n          lim\\n          \\n            h\\n            →\\n            0\\n          \\n        \\n        \\n          \\n            \\n              f\\n              (\\n              a\\n              +\\n              h\\n              )\\n              −\\n              f\\n              (\\n              a\\n              )\\n            \\n            h\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L=\\\\lim _{h\\\\to 0}{\\\\frac {f(a+h)-f(a)}{h}}}\\n  \\n\\nexists.  This means that, for every positive real number \\u2060\\n  \\n    \\n      \\n        ε\\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon }\\n  \\n\\u2060, there exists a positive real number \\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n  \\n such that, for every \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n such that \\n  \\n    \\n      \\n        \\n          |\\n        \\n        h\\n        \\n          |\\n        \\n        <\\n        δ\\n      \\n    \\n    {\\\\displaystyle |h|<\\\\delta }\\n  \\n and \\n  \\n    \\n      \\n        h\\n        ≠\\n        0\\n      \\n    \\n    {\\\\displaystyle h\\\\neq 0}\\n  \\n then \\n  \\n    \\n      \\n        f\\n        (\\n        a\\n        +\\n        h\\n        )\\n      \\n    \\n    {\\\\displaystyle f(a+h)}\\n  \\n is defined, and \\n\\n  \\n    \\n      \\n        \\n          |\\n          \\n            L\\n            −\\n            \\n              \\n                \\n                  f\\n                  (\\n                  a\\n                  +\\n                  h\\n                  )\\n                  −\\n                  f\\n                  (\\n                  a\\n                  )\\n                \\n                h\\n              \\n            \\n          \\n          |\\n        \\n        <\\n        ε\\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\left|L-{\\\\frac {f(a+h)-f(a)}{h}}\\\\right|<\\\\varepsilon ,}\\n  \\n\\nwhere the vertical bars denote the absolute value. This is an example of the (ε, δ)-definition of limit.\\nIf the function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is differentiable at \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060, that is if the limit \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n  \\n exists, then this limit is called the derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n. Multiple notations for the derivative exist. The derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n can be denoted \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle f\\'(a)}\\n  \\n\\u2060, read as \"\\u2060\\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n\\u2060 prime of \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060\"; or it can be denoted \\u2060\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                d\\n                f\\n              \\n              \\n                d\\n                x\\n              \\n            \\n          \\n          (\\n          a\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle {\\\\frac {df}{dx}}(a)}\\n  \\n\\u2060, read as \"the derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n with respect to \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n at \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060\" or \"\\u2060\\n  \\n    \\n      \\n        d\\n        f\\n      \\n    \\n    {\\\\displaystyle df}\\n  \\n\\u2060 by (or over) \\n  \\n    \\n      \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle dx}\\n  \\n at \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060\". See § Notation below. If \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a function that has a derivative at every point in its domain, then a function can be defined by mapping every point \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n to the value of the derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n. This function is written \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'}\\n  \\n and is called the derivative function or the derivative of \\u2060\\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n\\u2060. The function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n sometimes has a derivative at most, but not all, points of its domain. The function whose value at \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n equals \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle f\\'(a)}\\n  \\n whenever \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle f\\'(a)}\\n  \\n is defined and elsewhere is undefined is also called the derivative of \\u2060\\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n\\u2060. It is still a function, but its domain may be smaller than the domain of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n.\\nFor example, let \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n be the squaring function: \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          x\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=x^{2}}\\n  \\n. Then the quotient in the definition of the derivative is\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              f\\n              (\\n              a\\n              +\\n              h\\n              )\\n              −\\n              f\\n              (\\n              a\\n              )\\n            \\n            h\\n          \\n        \\n        =\\n        \\n          \\n            \\n              (\\n              a\\n              +\\n              h\\n              \\n                )\\n                \\n                  2\\n                \\n              \\n              −\\n              \\n                a\\n                \\n                  2\\n                \\n              \\n            \\n            h\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                a\\n                \\n                  2\\n                \\n              \\n              +\\n              2\\n              a\\n              h\\n              +\\n              \\n                h\\n                \\n                  2\\n                \\n              \\n              −\\n              \\n                a\\n                \\n                  2\\n                \\n              \\n            \\n            h\\n          \\n        \\n        =\\n        2\\n        a\\n        +\\n        h\\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {f(a+h)-f(a)}{h}}={\\\\frac {(a+h)^{2}-a^{2}}{h}}={\\\\frac {a^{2}+2ah+h^{2}-a^{2}}{h}}=2a+h.}\\n  \\n\\nThe division in the last step is valid as long as \\n  \\n    \\n      \\n        h\\n        ≠\\n        0\\n      \\n    \\n    {\\\\displaystyle h\\\\neq 0}\\n  \\n. The closer \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is to \\u2060\\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n  \\n\\u2060, the closer this expression becomes to the value \\n  \\n    \\n      \\n        2\\n        a\\n      \\n    \\n    {\\\\displaystyle 2a}\\n  \\n. The limit exists, and for every input \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n the limit is \\n  \\n    \\n      \\n        2\\n        a\\n      \\n    \\n    {\\\\displaystyle 2a}\\n  \\n. So, the derivative of the squaring function is the doubling function: \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        x\\n        )\\n        =\\n        2\\n        x\\n      \\n    \\n    {\\\\displaystyle f\\'(x)=2x}\\n  \\n\\u2060.\\nThe ratio in the definition of the derivative is the slope of the line through two points on the graph of the function \\u2060\\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n\\u2060, specifically the points \\n  \\n    \\n      \\n        (\\n        a\\n        ,\\n        f\\n        (\\n        a\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle (a,f(a))}\\n  \\n and \\n  \\n    \\n      \\n        (\\n        a\\n        +\\n        h\\n        ,\\n        f\\n        (\\n        a\\n        +\\n        h\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle (a+h,f(a+h))}\\n  \\n. As \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is made smaller, these points grow closer together, and the slope of this line approaches the limiting value, the slope of the tangent to the graph of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n. In other words, the derivative is the slope of the tangent.\\n\\n\\n=== Using infinitesimals ===\\nOne way to think of the derivative \\n  \\n    \\n      \\n        \\n          \\n            \\n              d\\n              f\\n            \\n            \\n              d\\n              x\\n            \\n          \\n        \\n        (\\n        a\\n        )\\n      \\n    \\n    {\\\\textstyle {\\\\frac {df}{dx}}(a)}\\n  \\n is as the ratio of an infinitesimal change in the output of the function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n to an infinitesimal change in its input. In order to make this intuition rigorous, a system of rules for manipulating infinitesimal quantities is required. The system of hyperreal numbers is a way of treating infinite and infinitesimal quantities.  The hyperreals are an extension of the real numbers that contain numbers greater than anything of the form \\n  \\n    \\n      \\n        1\\n        +\\n        1\\n        +\\n        ⋯\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle 1+1+\\\\cdots +1}\\n  \\n for any finite number of terms. Such numbers are infinite, and their reciprocals are infinitesimals. The application of hyperreal numbers to the foundations of calculus is called nonstandard analysis. This provides a way to define the basic concepts of calculus such as the derivative and integral in terms of infinitesimals, thereby giving a precise meaning to the \\n  \\n    \\n      \\n        d\\n      \\n    \\n    {\\\\displaystyle d}\\n  \\n in the Leibniz notation. Thus, the derivative of \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n  \\n becomes \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        x\\n        )\\n        =\\n        st\\n        \\u2061\\n        \\n          (\\n          \\n            \\n              \\n                f\\n                (\\n                x\\n                +\\n                d\\n                x\\n                )\\n                −\\n                f\\n                (\\n                x\\n                )\\n              \\n              \\n                d\\n                x\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'(x)=\\\\operatorname {st} \\\\left({\\\\frac {f(x+dx)-f(x)}{dx}}\\\\right)}\\n  \\n for an arbitrary infinitesimal \\u2060\\n  \\n    \\n      \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle dx}\\n  \\n\\u2060, where \\n  \\n    \\n      \\n        st\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {st} }\\n  \\n denotes the standard part function, which \"rounds off\" each finite hyperreal to the nearest real. Taking the squaring function \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          x\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=x^{2}}\\n  \\n as an example again,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  f\\n                  ′\\n                \\n                (\\n                x\\n                )\\n              \\n              \\n                \\n                =\\n                st\\n                \\u2061\\n                \\n                  (\\n                  \\n                    \\n                      \\n                        \\n                          x\\n                          \\n                            2\\n                          \\n                        \\n                        +\\n                        2\\n                        x\\n                        ⋅\\n                        d\\n                        x\\n                        +\\n                        (\\n                        d\\n                        x\\n                        \\n                          )\\n                          \\n                            2\\n                          \\n                        \\n                        −\\n                        \\n                          x\\n                          \\n                            2\\n                          \\n                        \\n                      \\n                      \\n                        d\\n                        x\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                st\\n                \\u2061\\n                \\n                  (\\n                  \\n                    \\n                      \\n                        2\\n                        x\\n                        ⋅\\n                        d\\n                        x\\n                        +\\n                        (\\n                        d\\n                        x\\n                        \\n                          )\\n                          \\n                            2\\n                          \\n                        \\n                      \\n                      \\n                        d\\n                        x\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                st\\n                \\u2061\\n                \\n                  (\\n                  \\n                    \\n                      \\n                        \\n                          2\\n                          x\\n                          ⋅\\n                          d\\n                          x\\n                        \\n                        \\n                          d\\n                          x\\n                        \\n                      \\n                    \\n                    +\\n                    \\n                      \\n                        \\n                          (\\n                          d\\n                          x\\n                          \\n                            )\\n                            \\n                              2\\n                            \\n                          \\n                        \\n                        \\n                          d\\n                          x\\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                st\\n                \\u2061\\n                \\n                  (\\n                  \\n                    2\\n                    x\\n                    +\\n                    d\\n                    x\\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                2\\n                x\\n                .\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}f\\'(x)&=\\\\operatorname {st} \\\\left({\\\\frac {x^{2}+2x\\\\cdot dx+(dx)^{2}-x^{2}}{dx}}\\\\right)\\\\\\\\&=\\\\operatorname {st} \\\\left({\\\\frac {2x\\\\cdot dx+(dx)^{2}}{dx}}\\\\right)\\\\\\\\&=\\\\operatorname {st} \\\\left({\\\\frac {2x\\\\cdot dx}{dx}}+{\\\\frac {(dx)^{2}}{dx}}\\\\right)\\\\\\\\&=\\\\operatorname {st} \\\\left(2x+dx\\\\right)\\\\\\\\&=2x.\\\\end{aligned}}}\\n  \\n\\n\\n== Continuity and differentiability ==\\n\\nIf \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is differentiable at \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060, then \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n must also be continuous at \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n. As an example, choose a point \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n and let \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n be the step function that returns the value 1 for all \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n less than \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060, and returns a different value 10 for all \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n greater than or equal to \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n.  The function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n cannot have a derivative at \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n. If \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is negative, then \\n  \\n    \\n      \\n        a\\n        +\\n        h\\n      \\n    \\n    {\\\\displaystyle a+h}\\n  \\n is on the low part of the step, so the secant line from \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n to \\n  \\n    \\n      \\n        a\\n        +\\n        h\\n      \\n    \\n    {\\\\displaystyle a+h}\\n  \\n is very steep; as \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n tends to zero, the slope tends to infinity. If \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is positive, then \\n  \\n    \\n      \\n        a\\n        +\\n        h\\n      \\n    \\n    {\\\\displaystyle a+h}\\n  \\n is on the high part of the step, so the secant line from \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n to \\n  \\n    \\n      \\n        a\\n        +\\n        h\\n      \\n    \\n    {\\\\displaystyle a+h}\\n  \\n has slope zero. Consequently, the secant lines do not approach any single slope, so the limit of the difference quotient does not exist. However, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function given by \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          |\\n        \\n        x\\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=|x|}\\n  \\n is continuous at \\u2060\\n  \\n    \\n      \\n        x\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle x=0}\\n  \\n\\u2060, but it is not differentiable there. If \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is positive, then the slope of the secant line from 0 to \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is one; if \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is negative, then the slope of the secant line from \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n  \\n to \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  \\n is \\u2060\\n  \\n    \\n      \\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle -1}\\n  \\n\\u2060.  This can be seen graphically as a \"kink\" or a \"cusp\" in the graph at \\n  \\n    \\n      \\n        x\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle x=0}\\n  \\n. Even a function with a smooth graph is not differentiable at a point where its tangent is vertical: For instance, the function given by \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          x\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=x^{1/3}}\\n  \\n is not differentiable at \\n  \\n    \\n      \\n        x\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle x=0}\\n  \\n. In summary, a function that has a derivative is continuous, but there are continuous functions that do not have a derivative.\\nMost functions that occur in practice have derivatives at all points or almost every point. Early in the history of calculus, many mathematicians assumed that a continuous function was differentiable at most points. Under mild conditions (for example, if the function is a monotone or a Lipschitz function), this is true. However, in 1872, Weierstrass found the first example of a function that is continuous everywhere but differentiable nowhere.  This example is now known as the Weierstrass function. In 1931, Stefan Banach proved that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that hardly any random continuous functions have a derivative at even one point.\\n\\n\\n== Notation ==\\n\\nOne common way of writing the derivative of a function is Leibniz notation, introduced by Gottfried Wilhelm Leibniz in 1675, which denotes a derivative as the quotient of two differentials, such as \\n  \\n    \\n      \\n        d\\n        y\\n      \\n    \\n    {\\\\displaystyle dy}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle dx}\\n  \\n\\u2060. It is still commonly used when the equation \\n  \\n    \\n      \\n        y\\n        =\\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle y=f(x)}\\n  \\n is viewed as a functional relationship between dependent and independent variables. The first derivative is denoted by \\u2060\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                d\\n                y\\n              \\n              \\n                d\\n                x\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle {\\\\frac {dy}{dx}}}\\n  \\n\\u2060, read as \"the derivative of \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n with respect to \\u2060\\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n\\u2060\". This derivative can alternately be treated as the application of a differential operator to a function, \\n  \\n    \\n      \\n        \\n          \\n            \\n              d\\n              y\\n            \\n            \\n              d\\n              x\\n            \\n          \\n        \\n        =\\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        .\\n      \\n    \\n    {\\\\textstyle {\\\\frac {dy}{dx}}={\\\\frac {d}{dx}}f(x).}\\n  \\n Higher derivatives are expressed using the notation \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                d\\n                \\n                  n\\n                \\n              \\n              y\\n            \\n            \\n              d\\n              \\n                x\\n                \\n                  n\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\textstyle {\\\\frac {d^{n}y}{dx^{n}}}}\\n  \\n for the \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n-th derivative of \\n  \\n    \\n      \\n        y\\n        =\\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle y=f(x)}\\n  \\n. These are abbreviations for multiple applications of the derivative operator; for example, \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                d\\n                \\n                  2\\n                \\n              \\n              y\\n            \\n            \\n              d\\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n          \\n            )\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\textstyle {\\\\frac {d^{2}y}{dx^{2}}}={\\\\frac {d}{dx}}{\\\\Bigl (}{\\\\frac {d}{dx}}f(x){\\\\Bigr )}.}\\n  \\n Unlike some alternatives, Leibniz notation involves explicit specification of the variable for differentiation, in the denominator, which removes ambiguity when working with multiple interrelated quantities. The derivative of a composed function can be expressed using the chain rule: if \\n  \\n    \\n      \\n        u\\n        =\\n        g\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle u=g(x)}\\n  \\n and \\n  \\n    \\n      \\n        y\\n        =\\n        f\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle y=f(g(x))}\\n  \\n then \\n  \\n    \\n      \\n        \\n          \\n            \\n              d\\n              y\\n            \\n            \\n              d\\n              x\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              d\\n              y\\n            \\n            \\n              d\\n              u\\n            \\n          \\n        \\n        ⋅\\n        \\n          \\n            \\n              d\\n              u\\n            \\n            \\n              d\\n              x\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\textstyle {\\\\frac {dy}{dx}}={\\\\frac {dy}{du}}\\\\cdot {\\\\frac {du}{dx}}.}\\n  \\n\\nAnother common notation for differentiation is by using the prime mark in the symbol of a function \\u2060\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n  \\n\\u2060. This notation, due to Joseph-Louis Lagrange, is now known as prime notation. The first derivative is written as \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f\\'(x)}\\n  \\n\\u2060, read as \"\\u2060\\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n\\u2060 prime of \\u2060\\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n\\u2060\", or \\u2060\\n  \\n    \\n      \\n        \\n          y\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle y\\'}\\n  \\n\\u2060, read as \"\\u2060\\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n\\u2060 prime\". Similarly, the second and the third derivatives can be written as \\n  \\n    \\n      \\n        \\n          f\\n          ″\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'\\'}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          ‴\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'\\'\\'}\\n  \\n\\u2060, respectively. For denoting the number of higher derivatives beyond this point, some authors use Roman numerals in superscript, whereas others place the number in parentheses, such as \\n  \\n    \\n      \\n        \\n          f\\n          \\n            \\n              i\\n              v\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f^{\\\\mathrm {iv} }}\\n  \\n or \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          \\n            (\\n            4\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f^{(4)}}\\n  \\n\\u2060. The latter notation generalizes to yield the notation \\n  \\n    \\n      \\n        \\n          f\\n          \\n            (\\n            n\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f^{(n)}}\\n  \\n for the \\u2060\\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n\\u2060th derivative of \\u2060\\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n\\u2060.\\nIn Newton\\'s notation or the dot notation, a dot is placed over a symbol to represent a time derivative. If \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n is a function of \\u2060\\n  \\n    \\n      \\n        t\\n      \\n    \\n    {\\\\displaystyle t}\\n  \\n\\u2060, then the first and second derivatives can be written as \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ˙\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\dot {y}}}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ¨\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\ddot {y}}}\\n  \\n\\u2060, respectively. This notation is used exclusively for derivatives with respect to time or arc length. It is typically used in differential equations in physics and differential geometry. However, the dot notation becomes unmanageable for high-order derivatives (of order 4 or more) and cannot deal with multiple independent variables.\\nAnother notation is D-notation, which represents the differential operator by the symbol \\u2060\\n  \\n    \\n      \\n        D\\n      \\n    \\n    {\\\\displaystyle D}\\n  \\n\\u2060. The first derivative is written \\n  \\n    \\n      \\n        D\\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle Df(x)}\\n  \\n and higher derivatives are written with a superscript, so the \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n-th derivative is \\u2060\\n  \\n    \\n      \\n        \\n          D\\n          \\n            n\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle D^{n}f(x)}\\n  \\n\\u2060. This notation is sometimes called Euler notation, although it seems that Leonhard Euler did not use it, and the notation was introduced by Louis François Antoine Arbogast. To indicate a partial derivative, the variable differentiated by is indicated with a subscript, for example given the function \\u2060\\n  \\n    \\n      \\n        u\\n        =\\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle u=f(x,y)}\\n  \\n\\u2060, its partial derivative with respect to \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n can be written \\n  \\n    \\n      \\n        \\n          D\\n          \\n            x\\n          \\n        \\n        u\\n      \\n    \\n    {\\\\displaystyle D_{x}u}\\n  \\n or \\u2060\\n  \\n    \\n      \\n        \\n          D\\n          \\n            x\\n          \\n        \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle D_{x}f(x,y)}\\n  \\n\\u2060. Higher partial derivatives can be indicated by superscripts or multiple subscripts, e.g. \\n  \\n    \\n      \\n        \\n          D\\n          \\n            x\\n            y\\n          \\n        \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        \\n          \\n            ∂\\n            \\n              ∂\\n              y\\n            \\n          \\n        \\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            ∂\\n            \\n              ∂\\n              x\\n            \\n          \\n        \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        \\n          \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\textstyle D_{xy}f(x,y)={\\\\frac {\\\\partial }{\\\\partial y}}{\\\\Bigl (}{\\\\frac {\\\\partial }{\\\\partial x}}f(x,y){\\\\Bigr )}}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        \\n          \\n            D\\n            \\n              x\\n            \\n            \\n              2\\n            \\n          \\n          f\\n          (\\n          x\\n          ,\\n          y\\n          )\\n          =\\n          \\n            \\n              ∂\\n              \\n                ∂\\n                x\\n              \\n            \\n          \\n          \\n            \\n              (\\n            \\n          \\n          \\n            \\n              ∂\\n              \\n                ∂\\n                x\\n              \\n            \\n          \\n          f\\n          (\\n          x\\n          ,\\n          y\\n          )\\n          \\n            \\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle D_{x}^{2}f(x,y)={\\\\frac {\\\\partial }{\\\\partial x}}{\\\\Bigl (}{\\\\frac {\\\\partial }{\\\\partial x}}f(x,y){\\\\Bigr )}}\\n  \\n\\u2060.\\n\\n\\n== Rules of computation ==\\n\\nIn principle, the derivative of a function can be computed from the definition by considering the difference quotient and computing its limit. Once the derivatives of a few simple functions are known, the derivatives of other functions are more easily computed using rules for obtaining derivatives of more complicated functions from simpler ones. This process of finding a derivative is known as differentiation.\\n\\n\\n=== Rules for basic functions ===\\nThe following are the rules for the derivatives of the most common basic functions. Here, \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n is a real number, and \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n  \\n is the base of the natural logarithm, approximately 2.71828. \\n\\nDerivatives of powers:\\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        \\n          x\\n          \\n            a\\n          \\n        \\n        =\\n        a\\n        \\n          x\\n          \\n            a\\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}x^{a}=ax^{a-1}}\\n  \\n\\nFunctions of exponential, natural logarithm, and logarithm with general base:\\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        \\n          e\\n          \\n            x\\n          \\n        \\n        =\\n        \\n          e\\n          \\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}e^{x}=e^{x}}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        \\n          a\\n          \\n            x\\n          \\n        \\n        =\\n        \\n          a\\n          \\n            x\\n          \\n        \\n        ln\\n        \\u2061\\n        (\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}a^{x}=a^{x}\\\\ln(a)}\\n  \\n, for \\n  \\n    \\n      \\n        a\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle a>0}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        ln\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            1\\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\ln(x)={\\\\frac {1}{x}}}\\n  \\n, for \\n  \\n    \\n      \\n        x\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle x>0}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        \\n          log\\n          \\n            a\\n          \\n        \\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            1\\n            \\n              x\\n              ln\\n              \\u2061\\n              (\\n              a\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\log _{a}(x)={\\\\frac {1}{x\\\\ln(a)}}}\\n  \\n, for \\n  \\n    \\n      \\n        x\\n        ,\\n        a\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle x,a>0}\\n  \\n\\nTrigonometric functions:\\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        sin\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        cos\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\sin(x)=\\\\cos(x)}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        cos\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        −\\n        sin\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\cos(x)=-\\\\sin(x)}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        tan\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        \\n          sec\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            1\\n            \\n              \\n                cos\\n                \\n                  2\\n                \\n              \\n              \\u2061\\n              (\\n              x\\n              )\\n            \\n          \\n        \\n        =\\n        1\\n        +\\n        \\n          tan\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\tan(x)=\\\\sec ^{2}(x)={\\\\frac {1}{\\\\cos ^{2}(x)}}=1+\\\\tan ^{2}(x)}\\n  \\n\\nInverse trigonometric functions:\\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        arcsin\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            1\\n            \\n              1\\n              −\\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\arcsin(x)={\\\\frac {1}{\\\\sqrt {1-x^{2}}}}}\\n  \\n, for \\n  \\n    \\n      \\n        −\\n        1\\n        <\\n        x\\n        <\\n        1\\n      \\n    \\n    {\\\\displaystyle -1<x<1}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        arccos\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        −\\n        \\n          \\n            1\\n            \\n              1\\n              −\\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\arccos(x)=-{\\\\frac {1}{\\\\sqrt {1-x^{2}}}}}\\n  \\n, for \\n  \\n    \\n      \\n        −\\n        1\\n        <\\n        x\\n        <\\n        1\\n      \\n    \\n    {\\\\displaystyle -1<x<1}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        arctan\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            1\\n            \\n              1\\n              +\\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\arctan(x)={\\\\frac {1}{1+x^{2}}}}\\n  \\n\\n\\n=== Rules for combined functions ===\\nThe following rules allow deducing derivatives of many functions from the derivatives of the basic functions:\\n\\nConstant rule: if \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a constant function, then for all \\u2060\\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n\\u2060,\\n\\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        x\\n        )\\n        =\\n        0.\\n      \\n    \\n    {\\\\displaystyle f\\'(x)=0.}\\n  \\n\\nSum rule:\\n\\n  \\n    \\n      \\n        (\\n        α\\n        f\\n        +\\n        β\\n        g\\n        \\n          )\\n          ′\\n        \\n        =\\n        α\\n        \\n          f\\n          ′\\n        \\n        +\\n        β\\n        \\n          g\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle (\\\\alpha f+\\\\beta g)\\'=\\\\alpha f\\'+\\\\beta g\\'}\\n  \\n for all functions \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n and \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  \\n and all real numbers \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  \\n and \\u2060\\n  \\n    \\n      \\n        β\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n  \\n\\u2060.\\nProduct rule:\\n\\n  \\n    \\n      \\n        (\\n        f\\n        g\\n        \\n          )\\n          ′\\n        \\n        =\\n        \\n          f\\n          ′\\n        \\n        g\\n        +\\n        f\\n        \\n          g\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle (fg)\\'=f\\'g+fg\\'}\\n  \\n for all functions \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  \\n\\u2060. As a special case, this rule includes the fact \\n  \\n    \\n      \\n        (\\n        α\\n        f\\n        \\n          )\\n          ′\\n        \\n        =\\n        α\\n        \\n          f\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle (\\\\alpha f)\\'=\\\\alpha f\\'}\\n  \\n whenever \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  \\n is a constant because \\n  \\n    \\n      \\n        \\n          α\\n          ′\\n        \\n        f\\n        =\\n        0\\n        ⋅\\n        f\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\alpha \\'f=0\\\\cdot f=0}\\n  \\n by the constant rule.\\nQuotient rule:\\n\\n  \\n    \\n      \\n        \\n          \\n            (\\n            \\n              \\n                f\\n                g\\n              \\n            \\n            )\\n          \\n          ′\\n        \\n        =\\n        \\n          \\n            \\n              \\n                f\\n                ′\\n              \\n              g\\n              −\\n              f\\n              \\n                g\\n                ′\\n              \\n            \\n            \\n              g\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left({\\\\frac {f}{g}}\\\\right)\\'={\\\\frac {f\\'g-fg\\'}{g^{2}}}}\\n  \\n  for all functions \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n and \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  \\n at all inputs where g ≠ 0.\\nChain rule for composite functions: If \\u2060\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        h\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=h(g(x))}\\n  \\n\\u2060, then\\n\\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        x\\n        )\\n        =\\n        \\n          h\\n          ′\\n        \\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n        ⋅\\n        \\n          g\\n          ′\\n        \\n        (\\n        x\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle f\\'(x)=h\\'(g(x))\\\\cdot g\\'(x).}\\n  \\n\\n\\n=== Computation example ===\\nThe derivative of the function given by \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        sin\\n        \\u2061\\n        \\n          (\\n          \\n            x\\n            \\n              2\\n            \\n          \\n          )\\n        \\n        −\\n        ln\\n        \\u2061\\n        (\\n        x\\n        )\\n        \\n          e\\n          \\n            x\\n          \\n        \\n        +\\n        7\\n      \\n    \\n    {\\\\displaystyle f(x)=x^{4}+\\\\sin \\\\left(x^{2}\\\\right)-\\\\ln(x)e^{x}+7}\\n  \\n is\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  f\\n                  ′\\n                \\n                (\\n                x\\n                )\\n              \\n              \\n                \\n                =\\n                4\\n                \\n                  x\\n                  \\n                    (\\n                    4\\n                    −\\n                    1\\n                    )\\n                  \\n                \\n                +\\n                \\n                  \\n                    \\n                      d\\n                      \\n                        (\\n                        \\n                          x\\n                          \\n                            2\\n                          \\n                        \\n                        )\\n                      \\n                    \\n                    \\n                      d\\n                      x\\n                    \\n                  \\n                \\n                cos\\n                \\u2061\\n                \\n                  (\\n                  \\n                    x\\n                    \\n                      2\\n                    \\n                  \\n                  )\\n                \\n                −\\n                \\n                  \\n                    \\n                      d\\n                      \\n                        (\\n                        \\n                          ln\\n                          \\u2061\\n                          \\n                            x\\n                          \\n                        \\n                        )\\n                      \\n                    \\n                    \\n                      d\\n                      x\\n                    \\n                  \\n                \\n                \\n                  e\\n                  \\n                    x\\n                  \\n                \\n                −\\n                ln\\n                \\u2061\\n                (\\n                x\\n                )\\n                \\n                  \\n                    \\n                      d\\n                      \\n                        (\\n                        \\n                          e\\n                          \\n                            x\\n                          \\n                        \\n                        )\\n                      \\n                    \\n                    \\n                      d\\n                      x\\n                    \\n                  \\n                \\n                +\\n                0\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                4\\n                \\n                  x\\n                  \\n                    3\\n                  \\n                \\n                +\\n                2\\n                x\\n                cos\\n                \\u2061\\n                \\n                  (\\n                  \\n                    x\\n                    \\n                      2\\n                    \\n                  \\n                  )\\n                \\n                −\\n                \\n                  \\n                    1\\n                    x\\n                  \\n                \\n                \\n                  e\\n                  \\n                    x\\n                  \\n                \\n                −\\n                ln\\n                \\u2061\\n                (\\n                x\\n                )\\n                \\n                  e\\n                  \\n                    x\\n                  \\n                \\n                .\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}f\\'(x)&=4x^{(4-1)}+{\\\\frac {d\\\\left(x^{2}\\\\right)}{dx}}\\\\cos \\\\left(x^{2}\\\\right)-{\\\\frac {d\\\\left(\\\\ln {x}\\\\right)}{dx}}e^{x}-\\\\ln(x){\\\\frac {d\\\\left(e^{x}\\\\right)}{dx}}+0\\\\\\\\&=4x^{3}+2x\\\\cos \\\\left(x^{2}\\\\right)-{\\\\frac {1}{x}}e^{x}-\\\\ln(x)e^{x}.\\\\end{aligned}}}\\n  \\n\\nHere the second term was computed using the chain rule and the third term using the product rule. The known derivatives of the elementary functions \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{2}}\\n  \\n, \\n  \\n    \\n      \\n        \\n          x\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{4}}\\n  \\n, \\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin(x)}\\n  \\n, \\n  \\n    \\n      \\n        ln\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\ln(x)}\\n  \\n, and \\n  \\n    \\n      \\n        exp\\n        \\u2061\\n        (\\n        x\\n        )\\n        =\\n        \\n          e\\n          \\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\exp(x)=e^{x}}\\n  \\n, as well as the constant \\n  \\n    \\n      \\n        7\\n      \\n    \\n    {\\\\displaystyle 7}\\n  \\n, were also used.\\n\\n\\n== Antidifferentiation ==\\n\\nAn antiderivative of a function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a function whose derivative is \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n. Antiderivatives are not unique: if \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  \\n is an antiderivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n, then so is \\n  \\n    \\n      \\n        A\\n        +\\n        c\\n      \\n    \\n    {\\\\displaystyle A+c}\\n  \\n, where \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n  \\n is any constant, because the derivative of a constant is zero. The fundamental theorem of calculus shows that finding an antiderivative of a function gives a way to compute the areas of shapes bounded by that function. More precisely, the integral of a function over a closed interval is equal to the difference between the values of an antiderivative evaluated at the endpoints of that interval.\\n\\n\\n== Higher-order derivatives ==\\nHigher order derivatives are the result of differentiating a function repeatedly. Given that \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a differentiable function, the derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is the first derivative, denoted as \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'}\\n  \\n\\u2060. The derivative of \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'}\\n  \\n is the second derivative, denoted as \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          ″\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'\\'}\\n  \\n\\u2060, and the derivative of \\n  \\n    \\n      \\n        \\n          f\\n          ″\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'\\'}\\n  \\n is the third derivative, denoted as \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          ‴\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'\\'\\'}\\n  \\n\\u2060. By continuing this process, if it exists, the \\u2060\\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n\\u2060th derivative is the derivative of the \\u2060\\n  \\n    \\n      \\n        (\\n        n\\n        −\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (n-1)}\\n  \\n\\u2060th derivative or the derivative of order \\u2060\\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n\\u2060. As has been discussed above, the generalization of derivative of a function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n may be denoted as \\u2060\\n  \\n    \\n      \\n        \\n          f\\n          \\n            (\\n            n\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f^{(n)}}\\n  \\n\\u2060.  A function that has \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  \\n successive derivatives is called \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  \\n times differentiable. If the \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  \\n-th derivative is continuous, then the function is said to be of differentiability class \\u2060\\n  \\n    \\n      \\n        \\n          C\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C^{k}}\\n  \\n\\u2060. A function that has infinitely many derivatives is called infinitely differentiable or smooth. Any polynomial function is infinitely differentiable; taking derivatives repeatedly will eventually result in a constant function, and all subsequent derivatives of that function are zero.\\nOne application of higher-order derivatives is in physics. Suppose that a function represents the position of an object at the time. The first derivative of that function is the velocity of an object with respect to time, the second derivative of the function is the acceleration of an object with respect to time, and the third derivative is the jerk.\\n\\n\\n== In other dimensions ==\\n\\n\\n=== Vector-valued functions ===\\nA vector-valued function \\n  \\n    \\n      \\n        \\n          y\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {y} }\\n  \\n of a real variable sends real numbers to vectors in some vector space \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{n}}\\n  \\n. A vector-valued function can be split up into its coordinate functions \\n  \\n    \\n      \\n        \\n          y\\n          \\n            1\\n          \\n        \\n        (\\n        t\\n        )\\n        ,\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        (\\n        t\\n        )\\n        ,\\n        …\\n        ,\\n        \\n          y\\n          \\n            n\\n          \\n        \\n        (\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle y_{1}(t),y_{2}(t),\\\\dots ,y_{n}(t)}\\n  \\n, meaning that \\n  \\n    \\n      \\n        \\n          y\\n        \\n        =\\n        (\\n        \\n          y\\n          \\n            1\\n          \\n        \\n        (\\n        t\\n        )\\n        ,\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        (\\n        t\\n        )\\n        ,\\n        …\\n        ,\\n        \\n          y\\n          \\n            n\\n          \\n        \\n        (\\n        t\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {y} =(y_{1}(t),y_{2}(t),\\\\dots ,y_{n}(t))}\\n  \\n. This includes, for example, parametric curves in \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{2}}\\n  \\n or \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{3}}\\n  \\n. The coordinate functions are real-valued functions, so the above definition of derivative applies to them. The derivative of \\n  \\n    \\n      \\n        \\n          y\\n        \\n        (\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {y} (t)}\\n  \\n is defined to be the vector, called the tangent vector, whose coordinates are the derivatives of the coordinate functions. That is,\\n\\n  \\n    \\n      \\n        \\n          \\n            y\\n          \\n          ′\\n        \\n        (\\n        t\\n        )\\n        =\\n        \\n          lim\\n          \\n            h\\n            →\\n            0\\n          \\n        \\n        \\n          \\n            \\n              \\n                y\\n              \\n              (\\n              t\\n              +\\n              h\\n              )\\n              −\\n              \\n                y\\n              \\n              (\\n              t\\n              )\\n            \\n            h\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {y} \\'(t)=\\\\lim _{h\\\\to 0}{\\\\frac {\\\\mathbf {y} (t+h)-\\\\mathbf {y} (t)}{h}},}\\n  \\n\\nif the limit exists. The subtraction in the numerator is the subtraction of vectors, not scalars. If the derivative of \\n  \\n    \\n      \\n        \\n          y\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {y} }\\n  \\n exists for every value of \\u2060\\n  \\n    \\n      \\n        t\\n      \\n    \\n    {\\\\displaystyle t}\\n  \\n\\u2060, then \\n  \\n    \\n      \\n        \\n          \\n            y\\n          \\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {y} \\'}\\n  \\n is another vector-valued function.\\n\\n\\n=== Partial derivatives ===\\n\\nFunctions can depend upon more than one variable. A partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant. Partial derivatives are used in vector calculus and differential geometry. As with ordinary derivatives, multiple notations exist: the partial derivative of a function \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        ,\\n        …\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x,y,\\\\dots )}\\n  \\n with respect to the variable \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is variously denoted by\\n\\namong other possibilities. It can be thought of as the rate of change of the function in the \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n-direction. Here ∂ is a rounded d called the partial derivative symbol.  To distinguish it from the letter d, ∂ is sometimes pronounced \"der\", \"del\", or \"partial\" instead of \"dee\". For example, let \\u2060\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        x\\n        y\\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x,y)=x^{2}+xy+y^{2}}\\n  \\n\\u2060, then the partial derivative of function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n with respect to both variables \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n are, respectively:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              ∂\\n              f\\n            \\n            \\n              ∂\\n              x\\n            \\n          \\n        \\n        =\\n        2\\n        x\\n        +\\n        y\\n        ,\\n        \\n        \\n          \\n            \\n              ∂\\n              f\\n            \\n            \\n              ∂\\n              y\\n            \\n          \\n        \\n        =\\n        x\\n        +\\n        2\\n        y\\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {\\\\partial f}{\\\\partial x}}=2x+y,\\\\qquad {\\\\frac {\\\\partial f}{\\\\partial y}}=x+2y.}\\n  \\n\\nIn general, the partial derivative of a function \\n  \\n    \\n      \\n        f\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f(x_{1},\\\\dots ,x_{n})}\\n  \\n in the direction \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n  \\n at the point \\n  \\n    \\n      \\n        (\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (a_{1},\\\\dots ,a_{n})}\\n  \\n is defined to be:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              ∂\\n              f\\n            \\n            \\n              ∂\\n              \\n                x\\n                \\n                  i\\n                \\n              \\n            \\n          \\n        \\n        (\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        )\\n        =\\n        \\n          lim\\n          \\n            h\\n            →\\n            0\\n          \\n        \\n        \\n          \\n            \\n              f\\n              (\\n              \\n                a\\n                \\n                  1\\n                \\n              \\n              ,\\n              …\\n              ,\\n              \\n                a\\n                \\n                  i\\n                \\n              \\n              +\\n              h\\n              ,\\n              …\\n              ,\\n              \\n                a\\n                \\n                  n\\n                \\n              \\n              )\\n              −\\n              f\\n              (\\n              \\n                a\\n                \\n                  1\\n                \\n              \\n              ,\\n              …\\n              ,\\n              \\n                a\\n                \\n                  i\\n                \\n              \\n              ,\\n              …\\n              ,\\n              \\n                a\\n                \\n                  n\\n                \\n              \\n              )\\n            \\n            h\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {\\\\partial f}{\\\\partial x_{i}}}(a_{1},\\\\ldots ,a_{n})=\\\\lim _{h\\\\to 0}{\\\\frac {f(a_{1},\\\\ldots ,a_{i}+h,\\\\ldots ,a_{n})-f(a_{1},\\\\ldots ,a_{i},\\\\ldots ,a_{n})}{h}}.}\\n  \\n\\nThis is fundamental for the study of the functions of several real variables. Let \\n  \\n    \\n      \\n        f\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f(x_{1},\\\\dots ,x_{n})}\\n  \\n be such a real-valued function. If all partial derivatives \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n with respect to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n  \\n are defined at the point \\u2060\\n  \\n    \\n      \\n        (\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (a_{1},\\\\dots ,a_{n})}\\n  \\n\\u2060, these partial derivatives define the vector\\n\\n  \\n    \\n      \\n        ∇\\n        f\\n        (\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        )\\n        =\\n        \\n          (\\n          \\n            \\n              \\n                \\n                  ∂\\n                  f\\n                \\n                \\n                  ∂\\n                  \\n                    x\\n                    \\n                      1\\n                    \\n                  \\n                \\n              \\n            \\n            (\\n            \\n              a\\n              \\n                1\\n              \\n            \\n            ,\\n            …\\n            ,\\n            \\n              a\\n              \\n                n\\n              \\n            \\n            )\\n            ,\\n            …\\n            ,\\n            \\n              \\n                \\n                  ∂\\n                  f\\n                \\n                \\n                  ∂\\n                  \\n                    x\\n                    \\n                      n\\n                    \\n                  \\n                \\n              \\n            \\n            (\\n            \\n              a\\n              \\n                1\\n              \\n            \\n            ,\\n            …\\n            ,\\n            \\n              a\\n              \\n                n\\n              \\n            \\n            )\\n          \\n          )\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\nabla f(a_{1},\\\\ldots ,a_{n})=\\\\left({\\\\frac {\\\\partial f}{\\\\partial x_{1}}}(a_{1},\\\\ldots ,a_{n}),\\\\ldots ,{\\\\frac {\\\\partial f}{\\\\partial x_{n}}}(a_{1},\\\\ldots ,a_{n})\\\\right),}\\n  \\n\\nwhich is called the gradient of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n. If \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is differentiable at every point in some domain, then the gradient is a vector-valued function \\n  \\n    \\n      \\n        ∇\\n        f\\n      \\n    \\n    {\\\\displaystyle \\\\nabla f}\\n  \\n that maps the point \\n  \\n    \\n      \\n        (\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (a_{1},\\\\dots ,a_{n})}\\n  \\n to the vector \\n  \\n    \\n      \\n        ∇\\n        f\\n        (\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\nabla f(a_{1},\\\\dots ,a_{n})}\\n  \\n. Consequently, the gradient determines a vector field.\\n\\n\\n=== Directional derivatives ===\\n\\nIf \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a real-valued function on \\u2060\\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{n}}\\n  \\n\\u2060, then the partial derivatives of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n measure its variation in the direction of the coordinate axes. For example, if \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a function of \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n\\u2060, then its partial derivatives measure the variation in \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n in the \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n direction. However, they do not directly measure the variation of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n in any other direction, such as along the diagonal line \\u2060\\n  \\n    \\n      \\n        y\\n        =\\n        x\\n      \\n    \\n    {\\\\displaystyle y=x}\\n  \\n\\u2060. These are measured using directional derivatives. Given a vector \\u2060\\n  \\n    \\n      \\n        \\n          v\\n        \\n        =\\n        (\\n        \\n          v\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          v\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {v} =(v_{1},\\\\ldots ,v_{n})}\\n  \\n\\u2060, then the directional derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n in the direction of \\n  \\n    \\n      \\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {v} }\\n  \\n at the point \\n  \\n    \\n      \\n        \\n          x\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {x} }\\n  \\n is:\\n\\n  \\n    \\n      \\n        \\n          D\\n          \\n            \\n              v\\n            \\n          \\n        \\n        \\n          f\\n        \\n        (\\n        \\n          x\\n        \\n        )\\n        =\\n        \\n          lim\\n          \\n            h\\n            →\\n            0\\n          \\n        \\n        \\n          \\n            \\n              f\\n              (\\n              \\n                x\\n              \\n              +\\n              h\\n              \\n                v\\n              \\n              )\\n              −\\n              f\\n              (\\n              \\n                x\\n              \\n              )\\n            \\n            h\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle D_{\\\\mathbf {v} }{f}(\\\\mathbf {x} )=\\\\lim _{h\\\\rightarrow 0}{\\\\frac {f(\\\\mathbf {x} +h\\\\mathbf {v} )-f(\\\\mathbf {x} )}{h}}.}\\n  \\n\\nIf all the partial derivatives of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n exist and are continuous at \\u2060\\n  \\n    \\n      \\n        \\n          x\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {x} }\\n  \\n\\u2060, then they determine the directional derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n in the direction \\n  \\n    \\n      \\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {v} }\\n  \\n by the formula:\\n\\n  \\n    \\n      \\n        \\n          D\\n          \\n            \\n              v\\n            \\n          \\n        \\n        \\n          f\\n        \\n        (\\n        \\n          x\\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            j\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          v\\n          \\n            j\\n          \\n        \\n        \\n          \\n            \\n              ∂\\n              f\\n            \\n            \\n              ∂\\n              \\n                x\\n                \\n                  j\\n                \\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle D_{\\\\mathbf {v} }{f}(\\\\mathbf {x} )=\\\\sum _{j=1}^{n}v_{j}{\\\\frac {\\\\partial f}{\\\\partial x_{j}}}.}\\n  \\n\\n\\n=== Total derivative and Jacobian matrix ===\\n\\nWhen \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a function from an open subset of \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{n}}\\n  \\n to \\u2060\\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{m}}\\n  \\n\\u2060, then the directional derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n in a chosen direction is the best linear approximation to \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at that point and in that direction. However, when \\u2060\\n  \\n    \\n      \\n        n\\n        >\\n        1\\n      \\n    \\n    {\\\\displaystyle n>1}\\n  \\n\\u2060, no single directional derivative can give a complete picture of the behavior of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n. The total derivative gives a complete picture by considering all directions at once. That is, for any vector \\n  \\n    \\n      \\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {v} }\\n  \\n starting at \\u2060\\n  \\n    \\n      \\n        \\n          a\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {a} }\\n  \\n\\u2060, the linear approximation formula holds:\\n\\n  \\n    \\n      \\n        f\\n        (\\n        \\n          a\\n        \\n        +\\n        \\n          v\\n        \\n        )\\n        ≈\\n        f\\n        (\\n        \\n          a\\n        \\n        )\\n        +\\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          a\\n        \\n        )\\n        \\n          v\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle f(\\\\mathbf {a} +\\\\mathbf {v} )\\\\approx f(\\\\mathbf {a} )+f\\'(\\\\mathbf {a} )\\\\mathbf {v} .}\\n  \\n\\nSimilarly with the single-variable derivative, \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          a\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f\\'(\\\\mathbf {a} )}\\n  \\n is chosen so that the error in this approximation is as small as possible. The total derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at \\n  \\n    \\n      \\n        \\n          a\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {a} }\\n  \\n is the unique linear transformation \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          a\\n        \\n        )\\n        :\\n        \\n          \\n            R\\n          \\n          \\n            n\\n          \\n        \\n        →\\n        \\n          \\n            R\\n          \\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f\\'(\\\\mathbf {a} )\\\\colon \\\\mathbb {R} ^{n}\\\\to \\\\mathbb {R} ^{m}}\\n  \\n such that\\n\\n  \\n    \\n      \\n        \\n          lim\\n          \\n            \\n              h\\n            \\n            →\\n            0\\n          \\n        \\n        \\n          \\n            \\n              ‖\\n              f\\n              (\\n              \\n                a\\n              \\n              +\\n              \\n                h\\n              \\n              )\\n              −\\n              (\\n              f\\n              (\\n              \\n                a\\n              \\n              )\\n              +\\n              \\n                f\\n                ′\\n              \\n              (\\n              \\n                a\\n              \\n              )\\n              \\n                h\\n              \\n              )\\n              ‖\\n            \\n            \\n              ‖\\n              \\n                h\\n              \\n              ‖\\n            \\n          \\n        \\n        =\\n        0.\\n      \\n    \\n    {\\\\displaystyle \\\\lim _{\\\\mathbf {h} \\\\to 0}{\\\\frac {\\\\lVert f(\\\\mathbf {a} +\\\\mathbf {h} )-(f(\\\\mathbf {a} )+f\\'(\\\\mathbf {a} )\\\\mathbf {h} )\\\\rVert }{\\\\lVert \\\\mathbf {h} \\\\rVert }}=0.}\\n  \\n\\nHere \\n  \\n    \\n      \\n        \\n          h\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {h} }\\n  \\n is a vector in \\u2060\\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{n}}\\n  \\n\\u2060, so the norm in the denominator is the standard length on \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{n}}\\n  \\n. However, \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          a\\n        \\n        )\\n        \\n          h\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'(\\\\mathbf {a} )\\\\mathbf {h} }\\n  \\n is a vector in \\u2060\\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{m}}\\n  \\n\\u2060, and the norm in the numerator is the standard length on \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{m}}\\n  \\n. If \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n  \\n is a vector starting at \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060, then \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          a\\n        \\n        )\\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'(\\\\mathbf {a} )\\\\mathbf {v} }\\n  \\n is called the pushforward of \\n  \\n    \\n      \\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {v} }\\n  \\n by \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n.\\nIf the total derivative exists at \\u2060\\n  \\n    \\n      \\n        \\n          a\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {a} }\\n  \\n\\u2060, then all the partial derivatives and directional derivatives of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n exist at \\u2060\\n  \\n    \\n      \\n        \\n          a\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {a} }\\n  \\n\\u2060, and for all \\u2060\\n  \\n    \\n      \\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {v} }\\n  \\n\\u2060, \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          a\\n        \\n        )\\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'(\\\\mathbf {a} )\\\\mathbf {v} }\\n  \\n is the directional derivative of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n in the direction \\u2060\\n  \\n    \\n      \\n        \\n          v\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {v} }\\n  \\n\\u2060.  If \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is written using coordinate functions, so that \\u2060\\n  \\n    \\n      \\n        f\\n        =\\n        (\\n        \\n          f\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          f\\n          \\n            2\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          f\\n          \\n            m\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f=(f_{1},f_{2},\\\\dots ,f_{m})}\\n  \\n\\u2060, then the total derivative can be expressed using the partial derivatives as a matrix. This matrix is called the Jacobian matrix of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n at \\n  \\n    \\n      \\n        \\n          a\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {a} }\\n  \\n:\\n\\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          a\\n        \\n        )\\n        =\\n        \\n          Jac\\n          \\n            \\n              a\\n            \\n          \\n        \\n        =\\n        \\n          \\n            (\\n            \\n              \\n                \\n                  ∂\\n                  \\n                    f\\n                    \\n                      i\\n                    \\n                  \\n                \\n                \\n                  ∂\\n                  \\n                    x\\n                    \\n                      j\\n                    \\n                  \\n                \\n              \\n            \\n            )\\n          \\n          \\n            i\\n            j\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle f\\'(\\\\mathbf {a} )=\\\\operatorname {Jac} _{\\\\mathbf {a} }=\\\\left({\\\\frac {\\\\partial f_{i}}{\\\\partial x_{j}}}\\\\right)_{ij}.}\\n  \\n\\n\\n== Generalizations ==\\n\\nThe concept of a derivative can be extended to many other settings. The common thread is that the derivative of a function at a point serves as a linear approximation of the function at that point.\\n\\nAn important generalization of the derivative concerns complex functions of complex variables, such as functions from (a domain in) the complex numbers \\n  \\n    \\n      \\n        \\n          C\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {C} }\\n  \\n to \\u2060\\n  \\n    \\n      \\n        \\n          C\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {C} }\\n  \\n\\u2060. The notion of the derivative of such a function is obtained by replacing real variables with complex variables in the definition. If \\n  \\n    \\n      \\n        \\n          C\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {C} }\\n  \\n is identified with \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{2}}\\n  \\n by writing a complex number \\n  \\n    \\n      \\n        z\\n      \\n    \\n    {\\\\displaystyle z}\\n  \\n as \\u2060\\n  \\n    \\n      \\n        x\\n        +\\n        i\\n        y\\n      \\n    \\n    {\\\\displaystyle x+iy}\\n  \\n\\u2060 then a differentiable function from \\n  \\n    \\n      \\n        \\n          C\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {C} }\\n  \\n to \\n  \\n    \\n      \\n        \\n          C\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {C} }\\n  \\n is certainly differentiable as a function from \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{2}}\\n  \\n to \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{2}}\\n  \\n (in the sense that its partial derivatives all exist), but the converse is not true in general: the complex derivative only exists if the real derivative is complex linear and this imposes relations between the partial derivatives called the Cauchy–Riemann equations – see holomorphic functions.\\nAnother generalization concerns functions between differentiable or smooth manifolds. Intuitively speaking such a manifold \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n  \\n is a space that can be approximated near each point \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n by a vector space called its tangent space: the prototypical example is a smooth surface in \\u2060\\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{3}}\\n  \\n\\u2060. The derivative (or differential) of a (differentiable) map \\n  \\n    \\n      \\n        f\\n        :\\n        M\\n        →\\n        N\\n      \\n    \\n    {\\\\displaystyle f:M\\\\to N}\\n  \\n between manifolds, at a point \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n in \\u2060\\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n  \\n\\u2060, is then a linear map from the tangent space of \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n  \\n at \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n to the tangent space of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n  \\n at \\u2060\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n  \\n\\u2060. The derivative function becomes a map between the tangent bundles of \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n  \\n\\u2060. This definition is used in differential geometry.\\nDifferentiation can also be defined for maps between vector space, such as Banach space, in which those generalizations are the Gateaux derivative and the Fréchet derivative.\\nOne deficiency of the classical derivative is that very many functions are not differentiable. Nevertheless, there is a way of extending the notion of the derivative so that all continuous functions and many other functions can be differentiated using a concept known as the weak derivative. The idea is to embed the continuous functions in a larger space called the space of distributions and only require that a function is differentiable \"on average\".\\nProperties of the derivative have inspired the introduction and study of many similar objects in algebra and topology; an example is differential algebra. Here, it consists of the derivation of some topics in abstract algebra, such as rings, ideals, field, and so on.\\nThe discrete equivalent of differentiation is finite differences. The study of differential calculus is unified with the calculus of finite differences in time scale calculus.\\nThe arithmetic derivative involves the function that is defined for the integers by the prime factorization. This is an analogy with the product rule.\\n\\n\\n== See also ==\\nCovariant derivative\\nDerivation\\nExterior derivative\\nFunctional derivative\\nLie derivative \\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\n\"Derivative\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nKhan Academy: \"Newton, Leibniz, and Usain Bolt\"\\nWeisstein, Eric W. \"Derivative\". MathWorld.\\nOnline Derivative Calculator from Wolfram Alpha.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4858219-01a1-4c12-a50f-2c2bc7d9eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wangyuning/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "sentences = [word_tokenize(doc[\"content\"].lower()) for doc in wiki_corpus]\n",
    "\n",
    "c = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=200,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    workers=4,\n",
    "    sg=1 \n",
    ")\n",
    "\n",
    "def doc_vector(model, text):\n",
    "    words = [w for w in word_tokenize(text.lower()) if w in model.wv]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "doc_vectors = np.array([doc_vector(c, doc[\"content\"]) for doc in wiki_corpus])\n",
    "titles = [doc[\"title\"] for doc in wiki_corpus]\n",
    "contents = [doc[\"content\"] for doc in wiki_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4990ed5a-e030-48c4-b9ba-96de16ad5239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 12 documents.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "d = c.vector_size \n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(doc_vectors.astype('float32'))\n",
    "\n",
    "print(f\"Indexed {index.ntotal} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8d7d7c-7d3e-4464-843b-6877ab71245b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Calculus (distance=0.1930)\n",
      "Rank 2: Statistics (distance=0.2092)\n",
      "Rank 3: Geometry (distance=0.2277)\n",
      "Rank 4: Algebra (distance=0.2853)\n",
      "Rank 5: Number theory (distance=0.3375)\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the difference between derivative and integral.\"\n",
    "query_vec = doc_vector(c, query).astype('float32')\n",
    "\n",
    "k = 5 \n",
    "D, I = index.search(np.array([query_vec]), k)\n",
    "\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"Rank {i+1}: {titles[idx]} (distance={D[0][i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d153b242-afd7-4cb5-8249-783502fa2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = [wiki_corpus[i][\"content\"] for i in I[0]]\n",
    "retrieved_titles = [titles[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47de7297-6d2d-4123-8f2b-3fec0d8d0af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc4f341-2668-4c70-a80f-f885f9924b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Answer:\n",
      " is the main form of algebra taught in schools\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the difference between derivative and integral.\"\n",
    "context = \"\\n\\n\".join(\n",
    "    [f\"[{retrieved_titles[i]}]\\n{retrieved_docs[i]}\" for i in range(len(retrieved_docs))]\n",
    ")\n",
    "prompt = f\"\"\"\n",
    "Answer the question using only the context below.\n",
    "\n",
    "Context:\n",
    "{contents}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nLLM Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2dd118-3d8e-43d8-9498-8b4f469424d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Calculus]\\nCalculus is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations.\\nOriginally called infinitesimal calculus or \"the calculus of infinitesimals\", it has two major branches, differential calculus and integral calculus. The former concerns instantaneous rates of change, and the slopes of curves, while the latter concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus. They make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. It is the \"mathematical backbone\" for dealing with problems where variables change with time or another reference variable.\\nInfinitesimal calculus was formulated separately in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. The concepts and techniques found in calculus have diverse applications in science, engineering, and other branches of mathematics.\\n\\n\\n== Etymology ==\\n\\nIn mathematics education, calculus is an abbreviation of both infinitesimal calculus and integral calculus, which denotes courses of elementary mathematical analysis. \\nIn Latin, the word calculus means “small pebble”, (the diminutive of calx, meaning \"stone\"), a meaning which still persists in medicine. Because such pebbles were used for counting out distances, tallying votes, and doing abacus arithmetic, the word came to be the Latin word for calculation. In this sense, it was used in English at least as early as 1672, several years before the publications of Leibniz and Newton, who wrote their mathematical texts in Latin.\\nIn addition to differential calculus and integral calculus, the term is also used for naming specific methods of computation or theories that imply some sort of computation. Examples of this usage include propositional calculus, Ricci calculus, calculus of variations, lambda calculus, sequent calculus, and process calculus. Furthermore, the term \"calculus\" has variously been applied in ethics and philosophy, for such systems as Bentham\\'s felicific calculus, and the ethical calculus.\\n\\n\\n== History ==\\n\\nModern calculus was developed in 17th-century Europe by Isaac Newton and Gottfried Wilhelm Leibniz (independently of each other, first publishing around the same time). Elements of it first appeared in ancient Egypt and later Greece, then in China and the Middle East, and still later again in medieval Europe and India.\\n\\n\\n=== Ancient precursors ===\\n\\n\\n==== Egypt ====\\nCalculations of volume and area, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (c.\\u20091820 BC), but the formulae are simple instructions, with no indication as to how they were obtained.\\n\\n\\n==== Greece ====\\n\\nLaying the foundations for integral calculus and foreshadowing the concept of the limit, ancient Greek mathematician Eudoxus of Cnidus (c.\\u2009390–337 BC) developed the method of exhaustion to prove the formulas for cone and pyramid volumes.\\nDuring the Hellenistic period, this method was further developed by Archimedes (c.\\u2009287 – c.\\u2009212 BC), who combined it with a concept of the indivisibles—a precursor to infinitesimals—allowing him to solve several problems now treated by integral calculus. In The Method of Mechanical Theorems he describes, for example, calculating the center of gravity of a solid hemisphere, the center of gravity of a frustum of a circular paraboloid, and the area of a region bounded by a parabola and one of its secant lines.\\n\\n\\n==== China ====\\nThe method of exhaustion was later discovered independently in China by Liu Hui in the 3rd century AD to find the area of a circle. In the 5th century AD, Zu Gengzhi, son of Zu Chongzhi, established a method that would later be called Cavalieri\\'s principle to find the volume of a sphere.\\n\\n\\n=== Medieval ===\\n\\n\\n==== Middle East ====\\nIn the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen (c.\\u2009965 – c.\\u20091040 AD) derived a formula for the sum of fourth powers. He determined the equations to calculate the area enclosed by the curve represented by \\n  \\n    \\n      \\n        y\\n        =\\n        \\n          x\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y=x^{k}}\\n  \\n (which translates to the integral \\n  \\n    \\n      \\n        ∫\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\textstyle \\\\int x^{k}dx}\\n  \\n in contemporary notation), for any given non-negative integer value of \\u2060\\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  \\n\\u2060.He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.\\n\\n\\n==== India ====\\nBhāskara II (c.\\u20091114–1185) was acquainted with some ideas of differential calculus and suggested that the \"differential coefficient\" vanishes at an extremum value of the function. In his astronomical work, he gave a procedure that looked like a precursor to infinitesimal methods. Namely, if \\n  \\n    \\n      \\n        x\\n        ≈\\n        y\\n      \\n    \\n    {\\\\displaystyle x\\\\approx y}\\n  \\n then \\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        (\\n        y\\n        )\\n        −\\n        sin\\n        \\u2061\\n        (\\n        x\\n        )\\n        ≈\\n        (\\n        y\\n        −\\n        x\\n        )\\n        cos\\n        \\u2061\\n        (\\n        y\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\sin(y)-\\\\sin(x)\\\\approx (y-x)\\\\cos(y).}\\n  \\n This can be interpreted as the discovery that cosine is the derivative of sine. In the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions. Madhava of Sangamagrama and the Kerala School of Astronomy and Mathematics stated components of calculus. They studied series equivalent to the Maclaurin expansions of \\u2060\\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin(x)}\\n  \\n\\u2060, \\u2060\\n  \\n    \\n      \\n        cos\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\cos(x)}\\n  \\n\\u2060, and \\u2060\\n  \\n    \\n      \\n        arctan\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\arctan(x)}\\n  \\n\\u2060 more than two hundred years before their introduction in Europe. According to Victor J. Katz, they, however, were not able to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today.\"\\n\\n\\n==== Europe ====\\nThe mathematical study of continuity was revived in the 14th century by the Oxford Calculators and French collaborators such as Nicole Oresme. They proved the \"Merton mean speed theorem\": that a uniformly accelerated body travels the same distance as a body with uniform speed whose speed is half the final velocity of the accelerated body.\\n\\n\\n=== Modern ===\\nJohannes Kepler\\'s work Stereometria Doliorum (1615) formed the basis of integral calculus. Kepler developed a method to calculate the area of an ellipse by adding up the lengths of many radii drawn from a focus of the ellipse.\\nSignificant work was performed in a treatise, the origin being Kepler\\'s methods, written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes\\' in The Method, but this treatise is believed to have been lost in the 13th century and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri\\'s work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.\\nThe formal study of calculus brought together Cavalieri\\'s infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term. The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving themselves to be predecessors to the second fundamental theorem of calculus around 1670.\\nThe product rule and chain rule, the notions of higher derivatives and Taylor series, and of analytic functions were used by Isaac Newton in an idiosyncratic notation which he applied to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his Principia Mathematica (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.\\n\\nThese ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton. He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz put painstaking effort into his choices of notation.\\nToday, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics. Leibniz developed much of the notation used in calculus today. The basic insights that both Newton and Leibniz provided led to their development of the laws of differentiation and integration, their emphasis that differentiation and integration are inverse processes, their development of methods for calculating the second and higher derivatives, and their statement of the notion for approximating a polynomial series.\\nWhen Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his Method of Fluxions), but Leibniz published his \"Nova Methodus pro Maximis et Minimis\" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics. A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus \"the science of fluxions\", a term that endured in English schools into the 19th century. The first complete treatise on calculus to be written in English and use the Leibniz notation was not published until 1815.\\n\\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.\\n\\n\\n=== Foundations ===\\nIn calculus, foundations refers to the rigorous development of the subject from axioms and definitions. In early calculus, the use of infinitesimal quantities was thought unrigorous and was fiercely criticized by several authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book The Analyst in 1734. Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz.\\nSeveral mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere \"notions\" of infinitely small quantities. The foundations of differential and integral calculus had been laid. In Cauchy\\'s Cours d\\'Analyse, we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (ε, δ)-definition of limit in the definition of differentiation. In his work, Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called \"infinitesimal calculus\". Bernhard Riemann used these ideas to give a precise definition of the integral. It was also during this period that the ideas of calculus were generalized to the complex plane with the development of complex analysis.\\nIn modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus. The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory, based on earlier developments by Émile Borel, and used it to define integrals of all but the most pathological functions. Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.\\nLimits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson\\'s non-standard analysis. Robinson\\'s approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus. There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher-power infinitesimals during derivations. Based on the ideas of F. W. Lawvere and employing the methods of category theory, smooth infinitesimal analysis views all functions as being continuous and incapable of being expressed in terms of discrete entities. One aspect of this formulation is that the law of excluded middle does not hold. The law of excluded middle is also rejected in constructive mathematics, a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis.\\n\\n\\n=== Significance ===\\nWhile many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Newton and Leibniz built on the work of earlier mathematicians to introduce its basic principles. The Hungarian polymath John von Neumann wrote of this work,\\n\\nThe calculus was the first achievement of modern mathematics and it is difficult to overestimate its importance. I think it defines more unequivocally than anything else the inception of modern mathematics, and the system of mathematical analysis, which is its logical development, still constitutes the greatest technical advance in exact thinking.\\nApplications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization. Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure. More advanced applications include power series and Fourier series.\\nCalculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes.\\n\\n\\n== Principles ==\\n\\n\\n=== Limits and infinitesimals ===\\n\\nCalculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, \"infinitely small\". For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbols \\n  \\n    \\n      \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle dx}\\n  \\n and \\n  \\n    \\n      \\n        d\\n        y\\n      \\n    \\n    {\\\\displaystyle dy}\\n  \\n were taken to be infinitesimal, and the derivative \\n  \\n    \\n      \\n        d\\n        y\\n        \\n          /\\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle dy/dx}\\n  \\n was their ratio.\\nThe infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. In the late 19th century, infinitesimals were replaced within academia by the epsilon, delta approach to limits. Limits describe the behavior of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior using the intrinsic structure of the real number system (as a metric space with the least-upper-bound property). In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by sequences of smaller and smaller numbers, and the infinitely small behavior of a function is found by taking the limiting behavior for these sequences. Limits were thought to provide a more rigorous foundation for calculus, and for this reason, they became the standard approach during the 20th century. However, the infinitesimal concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.\\n\\n\\n=== Differential calculus ===\\n\\nDifferential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called differentiation. Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the derivative function or just the derivative of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to produce another function. The function produced by differentiating the squaring function turns out to be the doubling function.\\nIn more explicit terms the \"doubling function\" may be denoted by g(x) = 2x and the \"squaring function\" by f(x) = x2. The \"derivative\" now takes the function f(x), defined by the expression \"x2\", as an input, that is all the information—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to output another function, the function g(x) = 2x, as will turn out.\\nIn Lagrange\\'s notation, the symbol for a derivative is an apostrophe-like mark called a prime. Thus, the derivative of a function called f is denoted by f′, pronounced \"f prime\" or \"f dash\". For instance, if f(x) = x2 is the squaring function, then f′(x) = 2x is its derivative (the doubling function g from above).\\nIf the input of the function represents time, then the derivative represents change concerning time. For example, if f is a function that takes time as input and gives the position of a ball at that time as output, then the derivative of f is how the position is changing in time, that is, it is the velocity of the ball.\\nIf a function is linear (that is if the graph of the function is a straight line), then the function can be written as y = mx + b, where x is the independent variable, y is the dependent variable, b is the y-intercept, and:\\n\\n  \\n    \\n      \\n        m\\n        =\\n        \\n          \\n            rise\\n            run\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                change in \\n              \\n              y\\n            \\n            \\n              \\n                change in \\n              \\n              x\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              Δ\\n              y\\n            \\n            \\n              Δ\\n              x\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle m={\\\\frac {\\\\text{rise}}{\\\\text{run}}}={\\\\frac {{\\\\text{change in }}y}{{\\\\text{change in }}x}}={\\\\frac {\\\\Delta y}{\\\\Delta x}}.}\\n  \\n\\nThis gives an exact value for the slope of a straight line. If the graph of the function is not a straight line, however, then the change in y divided by the change in x varies. Derivatives give an exact meaning to the notion of change in output concerning change in input. To be concrete, let f be a function, and fix a point a in the domain of f. (a, f(a)) is a point on the graph of the function. If h is a number close to zero, then a + h is a number close to a. Therefore, (a + h, f(a + h)) is close to (a, f(a)). The slope between these two points is\\n\\n  \\n    \\n      \\n        m\\n        =\\n        \\n          \\n            \\n              f\\n              (\\n              a\\n              +\\n              h\\n              )\\n              −\\n              f\\n              (\\n              a\\n              )\\n            \\n            \\n              (\\n              a\\n              +\\n              h\\n              )\\n              −\\n              a\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              f\\n              (\\n              a\\n              +\\n              h\\n              )\\n              −\\n              f\\n              (\\n              a\\n              )\\n            \\n            h\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle m={\\\\frac {f(a+h)-f(a)}{(a+h)-a}}={\\\\frac {f(a+h)-f(a)}{h}}.}\\n  \\n\\nThis expression is called a difference quotient. A line through two points on a curve is called a secant line, so m is the slope of the secant line between (a, f(a)) and (a + h, f(a + h)). The second line is only an approximation to the behavior of the function at the point  a because it does not account for what happens between  a and  a + h. It is not possible to discover the behavior at  a by setting h to zero because this would require dividing by zero, which is undefined. The derivative is defined by taking the limit as h tends to zero, meaning that it considers the behavior of f for all small values of h and extracts a consistent value for the case when h equals zero:\\n\\n  \\n    \\n      \\n        \\n          lim\\n          \\n            h\\n            →\\n            0\\n          \\n        \\n        \\n          \\n            \\n              f\\n              (\\n              a\\n              +\\n              h\\n              )\\n              −\\n              f\\n              (\\n              a\\n              )\\n            \\n            \\n              h\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\lim _{h\\\\to 0}{f(a+h)-f(a) \\\\over {h}}.}\\n  \\n\\nGeometrically, the derivative is the slope of the tangent line to the graph of f at a. The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function f.\\nHere is a particular example, the derivative of the squaring function at the input 3. Let f(x) = x2 be the squaring function.\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  f\\n                  ′\\n                \\n                (\\n                3\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  lim\\n                  \\n                    h\\n                    →\\n                    0\\n                  \\n                \\n                \\n                  \\n                    \\n                      (\\n                      3\\n                      +\\n                      h\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                      −\\n                      \\n                        3\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                    \\n                      h\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  lim\\n                  \\n                    h\\n                    →\\n                    0\\n                  \\n                \\n                \\n                  \\n                    \\n                      9\\n                      +\\n                      6\\n                      h\\n                      +\\n                      \\n                        h\\n                        \\n                          2\\n                        \\n                      \\n                      −\\n                      9\\n                    \\n                    \\n                      h\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  lim\\n                  \\n                    h\\n                    →\\n                    0\\n                  \\n                \\n                \\n                  \\n                    \\n                      6\\n                      h\\n                      +\\n                      \\n                        h\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                    \\n                      h\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  lim\\n                  \\n                    h\\n                    →\\n                    0\\n                  \\n                \\n                (\\n                6\\n                +\\n                h\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                6\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}f\\'(3)&=\\\\lim _{h\\\\to 0}{(3+h)^{2}-3^{2} \\\\over {h}}\\\\\\\\&=\\\\lim _{h\\\\to 0}{9+6h+h^{2}-9 \\\\over {h}}\\\\\\\\&=\\\\lim _{h\\\\to 0}{6h+h^{2} \\\\over {h}}\\\\\\\\&=\\\\lim _{h\\\\to 0}(6+h)\\\\\\\\&=6\\\\end{aligned}}}\\n  \\n\\nThe slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the derivative function of the squaring function or just the derivative of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.\\n\\n\\n=== Leibniz notation ===\\n\\nA common notation, introduced by Leibniz, for the derivative in the example above is\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                y\\n              \\n              \\n                \\n                =\\n                \\n                  x\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n            \\n              \\n                \\n                  \\n                    \\n                      d\\n                      y\\n                    \\n                    \\n                      d\\n                      x\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                2\\n                x\\n                .\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}y&=x^{2}\\\\\\\\{\\\\frac {dy}{dx}}&=2x.\\\\end{aligned}}}\\n  \\n\\nIn an approach based on limits, the symbol \\u2060dy/ dx\\u2060 is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above. Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers, dy being the infinitesimally small change in y caused by an infinitesimally small change  dx applied to x. We can also think of \\u2060d/ dx\\u2060 as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:\\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        (\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n        =\\n        2\\n        x\\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}(x^{2})=2x.}\\n  \\n\\nIn this usage, the dx in the denominator is read as \"with respect to x\". Another example of correct notation could be:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                g\\n                (\\n                t\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  t\\n                  \\n                    2\\n                  \\n                \\n                +\\n                2\\n                t\\n                +\\n                4\\n              \\n            \\n            \\n              \\n                \\n                  \\n                    d\\n                    \\n                      d\\n                      t\\n                    \\n                  \\n                \\n                g\\n                (\\n                t\\n                )\\n              \\n              \\n                \\n                =\\n                2\\n                t\\n                +\\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}g(t)&=t^{2}+2t+4\\\\\\\\{d \\\\over dt}g(t)&=2t+2\\\\end{aligned}}}\\n  \\n\\nEven when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like  dx and dy as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative.\\n\\n\\n=== Integral calculus ===\\n\\nIntegral calculus is the study of the definitions, properties, and applications of two related concepts, the indefinite integral and the definite integral. The process of finding the value of an integral is called integration. The indefinite integral, also known as the antiderivative, is the inverse operation to the derivative. F is an indefinite integral of f when f is a derivative of F. (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.) The definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.\\nA motivating example is the distance traveled in a given time. If the speed is constant, only multiplication is needed:\\n\\n  \\n    \\n      \\n        \\n          D\\n          i\\n          s\\n          t\\n          a\\n          n\\n          c\\n          e\\n        \\n        =\\n        \\n          S\\n          p\\n          e\\n          e\\n          d\\n        \\n        ⋅\\n        \\n          T\\n          i\\n          m\\n          e\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {Distance} =\\\\mathrm {Speed} \\\\cdot \\\\mathrm {Time} }\\n  \\n\\nBut if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.\\nWhen velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time. For example, traveling a steady 50 mph for 3 hours results in a total distance of 150 miles. Plotting the velocity as a function of time yields a rectangle with a height equal to the velocity and a width equal to the time elapsed. Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve. This connection between the area under a curve and the distance traveled can be extended to any irregularly shaped region exhibiting a fluctuating velocity over a given period. If f(x) represents speed as it varies over time, the distance traveled between the times represented by  a and b is the area of the region between f(x) and the x-axis, between x = a and x = b.\\nTo approximate that area, an intuitive method would be to divide up the distance between  a and b into several equal segments, the length of each segment represented by the symbol Δx. For each small segment, we can choose one value of the function f(x). Call that value h. Then the area of the rectangle with base Δx and height h gives the distance (time Δx multiplied by speed h) traveled in that segment. Associated with each segment is the average value of the function above it, f(x) = h. The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for Δx will give more rectangles and in most cases a better approximation, but for an exact answer, we need to take a limit as Δx approaches zero.\\nThe symbol of integration is \\n  \\n    \\n      \\n        ∫\\n      \\n    \\n    {\\\\displaystyle \\\\int }\\n  \\n, an elongated S chosen to suggest summation. The definite integral is written as:\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle \\\\int _{a}^{b}f(x)\\\\,dx}\\n  \\n\\nand is read \"the integral from a to b of f-of-x with respect to x.\" The Leibniz notation dx is intended to suggest dividing the area under the curve into an infinite number of rectangles so that their width Δx becomes the infinitesimally small dx.\\nThe indefinite integral, or antiderivative, is written:\\n\\n  \\n    \\n      \\n        ∫\\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\int f(x)\\\\,dx.}\\n  \\n\\nFunctions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is a family of functions differing only by a constant. Since the derivative of the function y = x2 + C, where C is any constant, is y′ = 2x, the antiderivative of the latter is given by:\\n\\n  \\n    \\n      \\n        ∫\\n        2\\n        x\\n        \\n        d\\n        x\\n        =\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        C\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\int 2x\\\\,dx=x^{2}+C.}\\n  \\n\\nThe unspecified constant C present in the indefinite integral or antiderivative is known as the constant of integration.\\n\\n\\n=== Fundamental theorem ===\\n\\nThe fundamental theorem of calculus states that differentiation and integration are inverse operations. More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.\\nThe fundamental theorem of calculus states: If a function f is continuous on the interval [a, b] and if F is a function whose derivative is f on the interval (a, b), then\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n        =\\n        F\\n        (\\n        b\\n        )\\n        −\\n        F\\n        (\\n        a\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\int _{a}^{b}f(x)\\\\,dx=F(b)-F(a).}\\n  \\n\\nFurthermore, for every x in the interval (a, b),\\n\\n  \\n    \\n      \\n        \\n          \\n            d\\n            \\n              d\\n              x\\n            \\n          \\n        \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            x\\n          \\n        \\n        f\\n        (\\n        t\\n        )\\n        \\n        d\\n        t\\n        =\\n        f\\n        (\\n        x\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {d}{dx}}\\\\int _{a}^{x}f(t)\\\\,dt=f(x).}\\n  \\n\\nThis realization, made by both Newton and Leibniz, was key to the proliferation of analytic results after their work became known. (The extent to which Newton and Leibniz were influenced by immediate predecessors, and particularly what Leibniz may have learned from the work of Isaac Barrow, is difficult to determine because of the priority dispute between them.) The fundamental theorem provides an algebraic method of computing many definite integrals—without performing limit processes—by finding formulae for antiderivatives. It is also a prototype solution of a differential equation. Differential equations relate an unknown function to its derivatives and are ubiquitous in the sciences.\\n\\n\\n== Applications ==\\n\\nCalculus is used in every branch of the physical sciences, actuarial science, computer science, statistics, engineering, economics, business, medicine, demography, and in other fields wherever a problem can be mathematically modeled and an optimal solution is desired. It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other. Calculus can be used in conjunction with other mathematical disciplines. For example, it can be used with linear algebra to find the \"best fit\" linear approximation for a set of points in a domain. Or, it can be used in probability theory to determine the expectation value of a continuous random variable given a probability density function. In analytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope, concavity and inflection points. Calculus is also used to find approximate solutions to equations; in practice, it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such as Newton\\'s method, fixed point iteration, and linear approximation. For instance, spacecraft use a variation of the Euler method to approximate curved courses within zero-gravity environments.\\nPhysics makes particular use of calculus; all concepts in classical mechanics and electromagnetism are related through calculus. The mass of an object of known density, the moment of inertia of objects, and the potential energies due to gravitational and electromagnetic forces can all be found by the use of calculus. An example of the use of calculus in mechanics is Newton\\'s second law of motion, which states that the derivative of an object\\'s momentum concerning time equals the net force upon it. Alternatively, Newton\\'s second law can be expressed by saying that the net force equals the object\\'s mass times its acceleration, which is the time derivative of velocity and thus the second time derivative of spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.\\nMaxwell\\'s theory of electromagnetism and Einstein\\'s theory of general relativity are also expressed in the language of differential calculus. Chemistry also uses calculus in determining reaction rates and in studying radioactive decay. In biology, population dynamics starts with reproduction and death rates to model population changes.\\nGreen\\'s theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as a planimeter, which is used to calculate the area of a flat surface on a drawing. For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.\\nIn the realm of medicine, calculus can be used to find the optimal branching angle of a blood vessel to maximize flow. Calculus can be applied to understand how quickly a drug is eliminated from a body or how quickly a cancerous tumor grows.\\nIn economics, calculus allows for the determination of maximal profit by providing a way to easily calculate both marginal cost and marginal revenue.\\n\\n\\n== See also ==\\n\\nGlossary of calculus\\nList of calculus topics\\nList of derivatives and integrals in alternative calculi\\nList of differentiation identities\\nPublications in calculus\\nTable of integrals\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\\n\"Calculus\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nWeisstein, Eric W. \"Calculus\". MathWorld.\\nTopics on Calculus at PlanetMath.\\nCalculus Made Easy (1914) by Silvanus P. Thompson Full text in PDF\\nCalculus on In Our Time at the BBC\\nCalculus.org: The Calculus page at University of California, Davis – contains resources and links to other sites\\nEarliest Known Uses of Some of the Words of Mathematics: Calculus & Analysis\\nThe Role of Calculus in College Mathematics Archived 26 July 2021 at the Wayback Machine from ERICDigests.org\\nOpenCourseWare Calculus from the Massachusetts Institute of Technology\\nInfinitesimal Calculus – an article on its historical development, in Encyclopedia of Mathematics, ed. Michiel Hazewinkel.\\nDaniel Kleitman, MIT. \"Calculus for Beginners and Artists\".\\nCalculus training materials at imomath.com Archived 9 July 2023 at the Wayback Machine\\n(in English and Arabic) The Excursion of Calculus, 1772\\n\\n[Statistics]\\nStatistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.\\nWhen census data (comprising every member of the target population) cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution\\'s central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena.\\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.\\nStatistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\\n\\n\\n== Introduction ==\\n\\n\"Statistics is both the science of uncertainty and the technology of extracting information from data.\" - featured in the International Encyclopedia of Statistical Science.Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.\\nStatistics is regarded as a body of science or a branch of mathematics. It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics were once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields.\\nSome consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty. Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification. Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.\\nThe word statistics ultimately comes from the Latin word Status, meaning \"situation\" or \"condition\" in society, which in late Latin adopted the meaning \"state\". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair\\'s works. In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name.\\n\\n\\n== Statistical data ==\\n\\n\\n=== Data collection ===\\n\\n\\n==== Sampling ====\\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models.\\nTo use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\\n\\n\\n==== Experimental and observational studies ====\\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies—for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\\n\\n\\n===== Experiments =====\\nThe basic steps of a statistical experiment are:\\n\\nPlanning the research, including finding the number of replicates of the study, using the following information:  preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.\\nDesign of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.\\nPerforming the experiment following the experimental protocol and analyzing the data following the experimental protocol.\\nFurther examining the data set in secondary analyses, to suggest new hypotheses for future study.\\nDocumenting and presenting the results of the study.\\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.\\n\\n\\n===== Observational study =====\\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group. A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.\\n\\n\\n=== Types of data ===\\n\\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating-point arithmetic. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998), van den Berg (1991).)\\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\"\\n\\n\\n== Methods ==\\n\\n\\n=== Descriptive statistics ===\\n\\nA descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information, while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.\\n\\n\\n=== Inferential statistics ===\\n\\nStatistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.\\n\\n\\n==== Terminology and theory of inferential statistics ====\\n\\n\\n===== Statistics, estimators and pivotal quantities =====\\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.\\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student\\'s t-value.\\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\\n\\n\\n===== Null hypothesis and alternative hypothesis =====\\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time. The alternative hypothesis is the name of the hypothesis that contradicts the null hypothesis.\\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (the status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\\n\\n\\n===== Error =====\\nWorking from a null hypothesis, two broad categories of error are recognized:\\n\\nType I errors where the null hypothesis is falsely rejected, giving a \"false positive\".\\nType II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed, giving a \"false negative\".\\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\\nA statistical error is the amount by which an observation differs from its expected value. A residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\\n\\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\\nMeasurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\\n\\n\\n===== Interval estimation =====\\n\\nMost studies only sample part of a population, so results do not fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\\n\\n\\n===== Significance =====\\n\\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\\n\\nThe standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\\nAlthough in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.\\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\\n\\nA difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.\\nFallacy of the transposed conditional, aka prosecutor\\'s fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is the probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.\\nRejecting the null hypothesis does not automatically prove the alternative hypothesis.\\nAs everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.\\n\\n\\n===== Examples =====\\nSome well-known statistical tests and procedures are:\\n\\n\\n=== Bayesian statistics ===\\n\\nAn alternative paradigm to the popular frequentist paradigm is to use Bayes\\' theorem to update the prior probability of the hypotheses in consideration based on the relative likelihood of the evidence gathered to obtain a posterior probability. Bayesian methods have been aided by the increase in available computing power to compute the posterior probability using numerical approximation techniques like Markov Chain Monte Carlo.\\nFor statistically modelling purposes, Bayesian models tend to be hierarchical, for example, one could model each YouTube channel as having video views distributed as a normal distribution with channel dependent mean and variance \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n        (\\n        \\n          μ\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          σ\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {N}}(\\\\mu _{i},\\\\sigma _{i})}\\n  \\n, while modeling the channel means as themselves coming from a normal distribution representing the distribution of average video view counts per channel, and the variances as coming from another distribution.\\nThe concept of using likelihood ratio can also be prominently seen in medical diagnostic testing.\\n\\n\\n=== Exploratory data analysis ===\\n\\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\\n\\n\\n=== Mathematical statistics ===\\n\\nMathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory. All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.\\n\\n\\n== History ==\\n\\nFormal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels. Al-Kindi\\'s Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187–1268) later made an important contribution on the use of sample size in frequency analysis.\\nAlthough the term statistic was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science. The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\\n\\nThe mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli\\'s posthumous work Ars Conjectandi. This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis. The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.\\n\\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton\\'s contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world\\'s first university statistics department at University College London.\\nThe second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher\\'s most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments, where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher\\'s linear discriminator and Fisher information. He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\". In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher\\'s principle (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway, a concept in sexual selection about a positive feedback runaway effect found in evolution.\\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.\\nAmong the early attempts to measure national economic activity were those of William Petty in the 17th century. In the 20th century the uniform System of National Accounts was developed.\\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.\\n\\n\\n== Applications ==\\n\\n\\n=== Applied statistics, theoretical statistics and mathematical statistics ===\\nApplied statistics, sometimes referred to as Statistical science, comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\\nStatistical consultants can help organizations and companies that do not have in-house expertise relevant to their particular questions.\\n\\n\\n=== Machine learning and data mining ===\\nMachine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.\\n\\n\\n=== Statistics in academia ===\\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research. A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student\\'s t-test, linear regression, Pearson\\'s correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon\\'s diversity index, Tukey\\'s range test, cluster analysis, Spearman\\'s rank correlation coefficient and principal component analysis.\\nA typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software.\\n\\n\\n=== Statistical computing ===\\n\\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.\\n\\n\\n=== Business statistics ===\\n\\nIn business, \"statistics\" is a widely used management- and decision support tool. It is particularly applied in financial management, marketing management, and production, services and operations management. Statistics is also heavily used in management accounting and auditing. The discipline of Management Science formalizes the use of statistics, and other mathematics, in business. (Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)\\nA typical \"Business Statistics\" course is intended for business majors, and covers descriptive statistics (collection, description, analysis, and summary of data), probability (typically the binomial and normal distributions), test of hypotheses and confidence intervals, linear regression, and correlation; (follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally. Professional certification programs, such as the CFA, often include topics in statistics.\\n\\n\\n== Specialized disciplines ==\\n\\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\\n\\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\\n\\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions.\\n\\n\\n== Misuse ==\\n\\nMisuse of statistics can produce subtle but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics, by Darrell Huff, outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).\\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"\\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:\\n\\nWho says so? (Do they have an axe to grind?)\\nHow do they know? (Do they have the resources to know the facts?)\\nWhat\\'s missing? (Do they give us a complete picture?)\\nDid someone change the subject? (Do they offer us the right answer to the wrong problem?)\\nDoes it make sense? (Is their conclusion logical and consistent with what we already know?)\\n\\n\\n=== Misinterpretation: correlation ===\\n\\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.\\n\\n\\n== See also ==\\n\\nFoundations and major areas of statistics\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\nLydia Denworth, \"A Significant Problem: Standard scientific methods are under fire. Will anything change?\", Scientific American, vol. 321, no. 4 (October 2019), pp. 62–67. \"The use of p values for nearly a century [since 1925] to determine statistical significance of experimental results has contributed to an illusion of certainty and [to] reproducibility crises in many scientific fields. There is growing determination to reform statistical analysis... Some [researchers] suggest changing statistical methods, whereas others would do away with a threshold for defining \"significant\" results\". (p. 63.)\\nBarbara Illowsky; Susan Dean (2014). Introductory Statistics. OpenStax CNX. ISBN 978-1938168208.\\nStockburger, David W. \"Introductory Statistics: Concepts, Models, and Applications\". Missouri State University (3rd Web ed.). Archived from the original on 28 May 2020.\\nOpenIntro Statistics Archived 2019-06-16 at the Wayback Machine, 3rd edition by Diez, Barr, and Cetinkaya-Rundel\\nStephen Jones, 2010. Statistics in Psychology: Explanations without Equations. Palgrave Macmillan. ISBN 978-1137282392.\\nCohen, J (1990). \"Things I have learned (so far)\" (PDF). American Psychologist. 45 (12): 1304–1312. doi:10.1037/0003-066x.45.12.1304. S2CID 7180431. Archived from the original (PDF) on 2017-10-18.\\nGigerenzer, G (2004). \"Mindless statistics\". Journal of Socio-Economics. 33 (5): 587–606. doi:10.1016/j.socec.2004.09.033.\\nIoannidis, J.P.A. (2005). \"Why most published research findings are false\". PLOS Medicine. 2 (4): 696–701. doi:10.1371/journal.pmed.0040168. PMC 1855693. PMID 17456002.\\n\\n\\n== External links ==\\n\\n(Electronic Version): TIBCO Software Inc. (2020). Data Science Textbook.\\nOnline Statistics Education: An Interactive Multimedia Course of Study. Developed by Rice University (Lead Developer), University of Houston Clear Lake, Tufts University, and National Science Foundation.\\nUCLA Statistical Computing Resources (archived 17 July 2006)\\nPhilosophy of Statistics from the Stanford Encyclopedia of Philosophy\\n\\n[Geometry]\\nGeometry is a branch of mathematics concerned with properties of space such as the distance, shape, size, and relative position of figures. Geometry is, along with arithmetic, one of the oldest branches of mathematics. A mathematician who works in the field of geometry is called a geometer. Until the 19th century, geometry was almost exclusively devoted to Euclidean geometry, which includes the notions of point, line, plane, distance, angle, surface, and curve, as fundamental concepts.\\nOriginally developed to model the physical world, geometry has applications in almost all sciences, and also in art, architecture, and other activities that are related to graphics. Geometry also has applications in areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental in Wiles\\'s proof of Fermat\\'s Last Theorem, a problem that was stated in terms of elementary arithmetic, and remained unsolved for several centuries.\\nDuring the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries is Carl Friedrich Gauss\\'s Theorema Egregium (\"remarkable theorem\") that asserts roughly that the Gaussian curvature of a surface is independent from any specific embedding in a Euclidean space. This implies that surfaces can be studied intrinsically, that is, as stand-alone spaces, and has been expanded into the theory of manifolds and Riemannian geometry. Later in the 19th century, it appeared that geometries without the parallel postulate (non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underlies general relativity is a famous application of non-Euclidean geometry.\\nSince the late 19th century, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry, algebraic geometry, computational geometry, algebraic topology, discrete geometry (also known as combinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometry that consider only alignment of points but not distance and parallelism, affine geometry that omits the concept of angle and distance, finite geometry that omits continuity, and others. This enlargement of the scope of geometry led to a change of meaning of the word \"space\", which originally referred to the three-dimensional space of the physical world and its model provided by Euclidean geometry; presently a geometric space, or simply a space is a mathematical structure on which some geometry is defined.\\n\\n\\n== History ==\\n\\nThe earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC. Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000–1800 BC) and Moscow Papyrus (c.\\u20091890 BC), and the Babylonian clay tablets, such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum. Later clay tablets (350–50 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter\\'s position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries. South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.\\nIn the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales\\'s theorem. Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history. Eudoxus (408–c.\\u2009355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures, as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time, introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework. The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. Archimedes (c.\\u2009287–212 BC) of Syracuse, Italy used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of pi. He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.\\n\\nIndian mathematicians also made many important contributions in geometry. The Shatapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras. According to (Hayashi 2005, p. 363), the Śulba Sūtras contain \"the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples, which are particular cases of Diophantine equations.\\nIn the Bakhshali manuscript, there are a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also \"employs a decimal place value system with a dot for zero.\" Aryabhata\\'s Aryabhatiya (499) includes the computation of areas and volumes.\\nBrahmagupta wrote his astronomical work Brāhmasphuṭasiddhānta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: \"basic operations\" (including cube roots, fractions, ratio and proportion, and barter) and \"practical mathematics\" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain). In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron\\'s formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).\\nIn the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry. Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra. Thābit ibn Qurra (known as Thebit in Latin) (836–901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry. Omar Khayyam (1048–1131) found geometric solutions to cubic equations. The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were part of a line of research on the parallel postulate continued by later European geometers, including Vitello (c.\\u20091230 – c.\\u20091314), Gersonides (1288–1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri, that by the 19th century led to the discovery of hyperbolic geometry.\\nIn the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661). Projective geometry studies properties of shapes which are unchanged under projections and sections, especially as they relate to artistic perspective.\\nTwo developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826–1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincaré, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of \"space\" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.\\n\\n\\n== Main concepts ==\\nThe following are some of the most important concepts in geometry.\\n\\n\\n=== Axioms ===\\n\\nEuclid took an abstract approach to geometry in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid\\'s approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792–1856), János Bolyai (1802–1860), Carl Friedrich Gauss (1777–1855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.\\n\\n\\n=== Spaces and subspaces ===\\n\\n\\n==== Points ====\\n\\nPoints are generally considered fundamental objects for building geometry. They may be defined by the properties that they must have, as in Euclid\\'s definition as \"that which has no part\", or in synthetic geometry. In modern mathematics, they are generally defined as elements of a set called space, which is itself axiomatically defined.\\nWith these modern definitions, every geometric shape is defined as a set of points; this is not the case in synthetic geometry, where a line is another fundamental object that is not viewed as the set of the points through which it passes.\\nHowever, there are modern geometries in which points are not primitive objects, or even without points. One of the oldest such geometries is Whitehead\\'s point-free geometry, formulated by Alfred North Whitehead in 1919–1920.\\n\\n\\n==== Lines ====\\n\\nEuclid described a line as \"breadthless length\" which \"lies equally with respect to the points on itself\". In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it. In differential geometry, a geodesic is a generalization of the notion of a line to curved spaces.\\n\\n\\n==== Planes ====\\n\\nIn Euclidean geometry a plane is a flat, two-dimensional surface that extends infinitely; the definitions for other types of geometries are generalizations of that. Planes are used in many areas of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles; it can be studied as an affine space, where collinearity and ratios can be studied but not distances; it can be studied as the complex plane using techniques of complex analysis; and so on.\\n\\n\\n==== Curves ====\\n\\nA curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.\\nIn topology, a curve is defined by a function from an interval of the real numbers to another space. In differential geometry, the same definition is used, but the defining function is required to be differentiable. Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.\\n\\n\\n==== Surfaces ====\\n\\nA surface is a two-dimensional object, such as a sphere or paraboloid. In differential geometry and topology, surfaces are described by two-dimensional \\'patches\\' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.\\n\\n\\n==== Solids ====\\n\\nA solid is a three-dimensional object bounded by a closed surface; for example, a ball is the volume bounded by a sphere.\\n\\n\\n==== Manifolds ====\\n\\nA manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space. In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.\\nManifolds are used extensively in physics, including in general relativity and string theory.\\n\\n\\n=== Angles ===\\n\\nEuclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other. In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.\\nThe size of an angle is formalized as an angular measure.\\nIn Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right. The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.\\nIn differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.\\n\\n\\n=== Measures: length, area, and volume ===\\n\\nLength, area, and volume describe the size or extent of an object in one dimension, two dimension, and three dimensions respectively.\\nIn Euclidean geometry and analytic geometry, the length of a line segment can often be calculated by the Pythagorean theorem.\\nArea and volume can be defined as fundamental quantities separate from length, or they can be described and calculated in terms of lengths in a plane or 3-dimensional space. Mathematicians have found many explicit formulas for area and formulas for volume of various geometric objects. In calculus, area and volume can be defined in terms of integrals, such as the Riemann integral or the Lebesgue integral.\\nOther geometrical measures include the curvature and compactness.\\n\\n\\n==== Metrics and measures ====\\n\\nThe concept of length or distance can be generalized, leading to the idea of metrics. For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.\\nIn a different direction, the concepts of length, area and volume are extended by measure theory, which studies methods of assigning a size or measure to sets, where the measures follow rules similar to those of classical area and volume.\\n\\n\\n=== Congruence and similarity ===\\n\\nCongruence and similarity are concepts that describe when two shapes have similar characteristics. In Euclidean geometry, similarity is used to describe objects that have the same shape, while congruence is used to describe objects that are the same in both size and shape. Hilbert, in his work on creating a more rigorous foundation for geometry, treated congruence as an undefined term whose properties are defined by axioms.\\nCongruence and similarity are generalized in transformation geometry, which studies the properties of geometric objects that are preserved by different kinds of transformations.\\n\\n\\n=== Compass and straightedge constructions ===\\n\\nClassical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments used in most geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using neusis, parabolas and other curves, or mechanical devices, were found.\\n\\n\\n=== Rotation and orientation ===\\n\\nThe geometrical concepts of rotation and orientation define part of the placement of objects embedded in the plane or in space.\\n\\n\\n=== Dimension ===\\n\\nTraditional geometry allowed dimensions 1 (a line or curve), 2 (a plane or surface), and 3 (our ambient world conceived of as three-dimensional space). Furthermore, mathematicians and physicists have used higher dimensions for nearly two centuries. One example of a mathematical use for higher dimensions is the configuration space of a physical system, which has a dimension equal to the system\\'s degrees of freedom. For instance, the configuration of a screw can be described by five coordinates.\\nIn general topology, the concept of dimension has been extended from natural numbers, to infinite dimension (Hilbert spaces, for example) and positive real numbers (in fractal geometry). In algebraic geometry, the dimension of an algebraic variety has received a number of apparently different definitions, which are all equivalent in the most common cases.\\n\\n\\n=== Symmetry ===\\n\\nThe theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of Leonardo da Vinci, M. C. Escher, and others. In the second half of the 19th century, the relationship between symmetry and geometry came under intense scrutiny. Felix Klein\\'s Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is. Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein\\'s idea to \\'define a geometry via its symmetry group\\' found its inspiration. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry.\\nA different type of symmetry is the principle of duality in projective geometry, among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and the result is an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.\\n\\n\\n== Contemporary geometry ==\\n\\n\\n=== Euclidean geometry ===\\n\\nEuclidean geometry is geometry in its classical sense. As it models the space of the physical world, it is used in many scientific areas, such as mechanics, astronomy, crystallography, and many technical fields, such as engineering, architecture, geodesy, aerodynamics, and navigation. The mandatory educational curriculum of the majority of nations includes the study of Euclidean concepts such as points, lines, planes, angles, triangles, congruence, similarity, solid figures, circles, and analytic geometry.\\n\\n\\n==== Euclidean vectors ====\\n\\nEuclidean vectors are used for a myriad of applications in physics and engineering, such as position, displacement, deformation, velocity, acceleration, force, etc.\\n\\n\\n=== Differential geometry ===\\n\\nDifferential geometry uses techniques of calculus and linear algebra to study problems in geometry. It has applications in physics, econometrics, and bioinformatics, among others.\\nIn particular, differential geometry is of importance to mathematical physics due to Albert Einstein\\'s general relativity postulation that the universe is curved. Differential geometry can either be intrinsic (meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point) or extrinsic (where the object under study is a part of some ambient flat Euclidean space).\\n\\n\\n==== Non-Euclidean geometry ====\\n\\n\\n=== Topology ===\\n\\nTopology is the field concerned with the properties of continuous mappings, and can be considered a generalization of Euclidean geometry. In practice, topology often means dealing with large-scale properties of spaces, such as connectedness and compactness.\\nThe field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the saying \\'topology is rubber-sheet geometry\\'. Subfields of topology include geometric topology, differential topology, algebraic topology and general topology.\\n\\n\\n=== Algebraic geometry ===\\n\\nAlgebraic geometry is fundamentally the study by means of algebraic methods of some geometrical shapes, called algebraic sets, and defined as common zeros of multivariate polynomials. Algebraic geometry became an autonomous subfield of geometry c.\\u20091900, with a theorem called Hilbert\\'s Nullstellensatz that establishes a strong correspondence between algebraic sets and ideals of polynomial rings. This led to a parallel development of algebraic geometry, and its algebraic counterpart, called commutative algebra. From the late 1950s through the mid-1970s algebraic geometry had undergone major foundational development, with the introduction by Alexander Grothendieck of scheme theory, which allows using topological methods, including cohomology theories in a purely algebraic context. Scheme theory allowed to solve many difficult problems not only in geometry, but also in number theory. Wiles\\' proof of Fermat\\'s Last Theorem is a famous example of a long-standing problem of number theory whose solution uses scheme theory and its extensions such as stack theory. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.\\nAlgebraic geometry has applications in many areas, including cryptography and string theory.\\n\\n\\n=== Complex geometry ===\\n\\nComplex geometry studies the nature of geometric structures modelled on, or arising out of, the complex plane. Complex geometry lies at the intersection of differential geometry, algebraic geometry, and analysis of several complex variables, and has found applications to string theory and mirror symmetry.\\nComplex geometry first appeared as a distinct area of study in the work of Bernhard Riemann in his study of Riemann surfaces. Work in the spirit of Riemann was carried out by the Italian school of algebraic geometry in the early 1900s. Contemporary treatment of complex geometry began with the work of Jean-Pierre Serre, who introduced the concept of sheaves to the subject, and illuminated the relations between complex geometry and algebraic geometry.\\nThe primary objects of study in complex geometry are complex manifolds, complex algebraic varieties, and complex analytic varieties, and holomorphic vector bundles and coherent sheaves over these spaces. Special examples of spaces studied in complex geometry include Riemann surfaces, and Calabi–Yau manifolds, and these spaces find uses in string theory. In particular, worldsheets of strings are modelled by Riemann surfaces, and superstring theory predicts that the extra 6 dimensions of 10 dimensional spacetime may be modelled by Calabi–Yau manifolds.\\n\\n\\n=== Discrete geometry ===\\n\\nDiscrete geometry is a subject that has close connections with convex geometry. It is concerned mainly with questions of relative position of simple geometric objects, such as points, lines and circles. Examples include the study of sphere packings, triangulations, the Kneser-Poulsen conjecture, etc. It shares many methods and principles with combinatorics.\\n\\n\\n=== Computational geometry ===\\n\\nComputational geometry deals with algorithms and their implementations for manipulating geometrical objects. Important problems historically have included the travelling salesman problem, minimum spanning trees, hidden-line removal, and linear programming.\\nAlthough being a young area of geometry, it has many applications in computer vision, image processing, computer-aided design, medical imaging, etc.\\n\\n\\n=== Geometric group theory ===\\n\\nGroups have been understood as geometric objects since Klein\\'s Erlangen programme. Geometric group theory studies group actions on objects that are regarded as geometric (significantly, isometric actions on metric spaces) to study finitely generated groups, often involving large-scale geometric techniques and borrowing from topology, geometry, dynamics and analysis. It had a significant impact on low-dimensional topology, a celebrated result being Agol\\'s proof of the virtually Haken conjecture that combines Perelman geometrization with cubulation techniques.\\nGroup actions on their Cayley graphs are foundational examples of isometric group actions. Other major topics include quasi-isometries, Gromov-hyperbolic groups and their generalizations (relatively and acylindrically hyperbolic groups), free groups and their automorphisms, groups acting on trees, various notions of nonpositive curvature for groups (CAT(0) groups, Dehn functions, automaticity...), right angled Artin groups, and topics close to combinatorial group theory such as small cancellation theory and algorithmic problems (e.g. the word, conjugacy, and isomorphism problems). Other group-theoretic topics like mapping class groups, property (T), solvability, amenability and lattices in Lie groups are sometimes regarded as strongly geometric as well.\\n\\n\\n=== Convex geometry ===\\n\\nConvex geometry investigates convex shapes in the Euclidean space and its more abstract analogues, often using techniques of real analysis and discrete mathematics. It has close connections to convex analysis, optimization and functional analysis and important applications in number theory.\\nConvex geometry dates back to antiquity. Archimedes gave the first known precise definition of convexity. The isoperimetric problem, a recurring concept in convex geometry, was studied by the Greeks as well, including Zenodorus. Archimedes, Plato, Euclid, and later Kepler and Coxeter all studied convex polytopes and their properties. From the 19th century on, mathematicians have studied other areas of convex mathematics, including higher-dimensional polytopes, volume and surface area of convex bodies, Gaussian curvature, algorithms, tilings and lattices.\\n\\n\\n== Applications ==\\nGeometry has found applications in many fields, some of which are described below.\\n\\n\\n=== Art ===\\n\\nMathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.\\nArtists have long used concepts of proportion in design. Vitruvius developed a complicated theory of ideal proportions for the human figure. These concepts have been used and adapted by artists from Michelangelo to modern comic book artists.\\nThe golden ratio is a particular proportion that has had a controversial role in art. Often claimed to be the most aesthetically pleasing ratio of lengths, it is frequently stated to be incorporated into famous works of art, though the most reliable and unambiguous examples were made deliberately by artists aware of this legend.\\nTilings, or tessellations, have been used in art throughout history. Islamic art makes frequent use of tessellations, as did the art of M. C. Escher. Escher\\'s work also made use of hyperbolic geometry.\\nCézanne advanced the theory that all images can be built up from the sphere, the cone, and the cylinder. This is still used in art theory today, although the exact list of shapes varies from author to author.\\n\\n\\n=== Architecture ===\\n\\nGeometry has many applications in architecture. In fact, it has been said that geometry lies at the core of architectural design. Applications of geometry to architecture include the use of projective geometry to create forced perspective, the use of conic sections in constructing domes and similar objects, the use of tessellations, and the use of symmetry.\\n\\n\\n=== Physics ===\\n\\nThe field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.\\nRiemannian geometry and pseudo-Riemannian geometry are used in general relativity. String theory makes use of several variants of geometry, as does quantum information theory.\\n\\n\\n=== Other fields of mathematics ===\\n\\nCalculus was strongly influenced by geometry. For instance, the introduction of coordinates by René Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century. Analytic geometry continues to be a mainstay of pre-calculus and calculus curriculum.\\nAnother important area of application is number theory. In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths contradicted their philosophical views. Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles\\'s proof of Fermat\\'s Last Theorem.\\n\\n\\n== See also ==\\n\\nLists\\nList of geometers\\nCategory:Algebraic geometers\\nCategory:Differential geometers\\nCategory:Geometers\\nCategory:Topologists\\nList of formulas in elementary geometry\\nList of geometry topics\\nList of important publications in geometry\\nLists of mathematics topics\\nRelated topics\\nDescriptive geometry\\nFlatland, a book written by Edwin Abbott Abbott about two- and three-dimensional space, to understand the concept of four dimensions\\nList of interactive geometry software\\nOther applications\\nMolecular geometry\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n=== Sources ===\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\\n\"Geometry\" . Encyclopædia Britannica. Vol. 11 (11th ed.). 1911. pp. 675–736.\\nA geometry course from Wikiversity\\nUnusual Geometry Problems\\nThe Math Forum – Geometry Archived 28 January 2022 at the Wayback Machine\\nThe Math Forum – K–12 Geometry Archived 15 April 2008 at the Wayback Machine\\nThe Math Forum – College Geometry Archived 15 April 2008 at the Wayback Machine\\nThe Math Forum – Advanced Geometry Archived 16 April 2008 at the Wayback Machine\\nNature Precedings – Pegs and Ropes Geometry at Stonehenge\\nThe Mathematical Atlas – Geometric Areas of Mathematics\\n\"4000 Years of Geometry\", lecture by Robin Wilson given at Gresham College, 3 October 2007 (available for MP3 and MP4 download as well as a text file)\\nFinitism in Geometry at the Stanford Encyclopedia of Philosophy\\nThe Geometry Junkyard\\nInteractive geometry reference with hundreds of applets\\nDynamic Geometry Sketches (with some Student Explorations)\\nGeometry classes at Khan Academy\\n\\n[Algebra]\\nAlgebra is a branch of mathematics that deals with abstract systems, known as algebraic structures, and the manipulation of expressions within those systems. It is a generalization of arithmetic that introduces variables and algebraic operations other than the standard arithmetic operations, such as addition and multiplication.\\nElementary algebra is the main form of algebra taught in schools. It examines mathematical statements using variables for unspecified values and seeks to determine for which values the statements are true. To do so, it uses different methods of transforming equations to isolate variables. Linear algebra is a closely related field that investigates linear equations and combinations of them called systems of linear equations. It provides methods to find the values that solve all equations in the system at the same time, and to study the set of these solutions.\\nAbstract algebra studies algebraic structures, which consist of a set of mathematical objects together with one or several operations defined on that set. It is a generalization of elementary and linear algebra since it allows mathematical objects other than numbers and non-arithmetic operations. It distinguishes between different types of algebraic structures, such as groups, rings, and fields, based on the number of operations they use and the laws they follow, called axioms. Universal algebra and category theory provide general frameworks to investigate abstract patterns that characterize different classes of algebraic structures.\\nAlgebraic methods were first studied in the ancient period to solve specific problems in fields like geometry. Subsequent mathematicians examined general techniques to solve equations independent of their specific applications. They described equations and their solutions using words and abbreviations until the 16th and 17th centuries when a rigorous symbolic formalism was developed. In the mid-19th century, the scope of algebra broadened beyond a theory of equations to cover diverse types of algebraic operations and structures. Algebra is relevant to many branches of mathematics, such as geometry, topology, number theory, and calculus, and other fields of inquiry, like logic and the empirical sciences.\\n\\n\\n== Definition and etymology ==\\nAlgebra is the branch of mathematics that studies algebraic structures and the operations they use. An algebraic structure is a non-empty set of mathematical objects, such as the integers, together with algebraic operations defined on that set, like addition and multiplication. Algebra explores the laws, general characteristics, and types of algebraic structures. Within certain algebraic structures, it examines the use of variables in equations and how to manipulate these equations.\\nAlgebra is often understood as a generalization of arithmetic. Arithmetic studies operations like addition, subtraction, multiplication, and division, in a particular domain of numbers, such as the real numbers. Elementary algebra constitutes the first level of abstraction. Like arithmetic, it restricts itself to specific types of numbers and operations. It generalizes these operations by allowing indefinite quantities in the form of variables in addition to numbers. A higher level of abstraction is found in abstract algebra, which is not limited to a particular domain and examines algebraic structures such as groups and rings. It extends beyond typical arithmetic operations by also covering other types of operations. Universal algebra is still more abstract in that it is not interested in specific algebraic structures but investigates the characteristics of algebraic structures in general.\\n\\nThe term \"algebra\" is sometimes used in a more narrow sense to refer only to elementary algebra or only to abstract algebra. When used as a countable noun, an algebra is a specific type of algebraic structure that involves a vector space equipped with a certain type of binary operation, a bilinear map. Depending on the context, \"algebra\" can also refer to other algebraic structures, like a Lie algebra or an associative algebra.\\nThe word algebra comes from the Arabic term الجبر (al-jabr), which originally referred to the surgical treatment of bonesetting. In the 9th century, the term received a mathematical meaning when Muhammad ibn Musa al-Khwarizmi employed it to name a method for transforming equations and used it in the title of his treatise al-Kitāb al-Mukhtaṣar fī Ḥisāb al-Jabr wal-Muqābalah [The Compendious Book on Calculation by Completion and Balancing] which was translated into Latin as Liber Algebrae et Almucabola. The word entered the English language in the 16th century from Italian, Spanish, and medieval Latin. Initially, its meaning was restricted to the theory of equations, that is, to the art of manipulating polynomial equations in view of solving them. This changed in the 19th century when the scope of algebra broadened to cover the study of diverse types of algebraic operations and structures together with their underlying axioms, the laws they follow.\\n\\n\\n== Major branches ==\\n\\n\\n=== Elementary algebra ===\\n\\nElementary algebra, also called school algebra, college algebra, and classical algebra, is the oldest and most basic form of algebra. It is a generalization of arithmetic that relies on variables and examines how mathematical statements may be transformed. \\nArithmetic is the study of numerical operations and investigates how numbers are combined and transformed using the arithmetic operations of addition, subtraction, multiplication, division, exponentiation, extraction of roots, and logarithm. For example, the operation of addition combines two numbers, called the addends, into a third number, called the sum, as in \\n  \\n    \\n      \\n        2\\n        +\\n        5\\n        =\\n        7\\n      \\n    \\n    {\\\\displaystyle 2+5=7}\\n  \\n.\\nElementary algebra relies on the same operations while allowing variables in addition to regular numbers. Variables are symbols for unspecified or unknown quantities. They make it possible to state relationships for which one does not know the exact values and to express general laws that are true, independent of which numbers are used. For example, the equation \\n  \\n    \\n      \\n        2\\n        ×\\n        3\\n        =\\n        3\\n        ×\\n        2\\n      \\n    \\n    {\\\\displaystyle 2\\\\times 3=3\\\\times 2}\\n  \\n belongs to arithmetic and expresses an equality only for these specific numbers. By replacing the numbers with variables, it is possible to express a general law that applies to any possible combination of numbers, like the commutative property of multiplication, which is expressed in the equation \\n  \\n    \\n      \\n        a\\n        ×\\n        b\\n        =\\n        b\\n        ×\\n        a\\n      \\n    \\n    {\\\\displaystyle a\\\\times b=b\\\\times a}\\n  \\n.\\nAlgebraic expressions are formed by using arithmetic operations to combine variables and numbers. By convention, the lowercase letters \\u2060\\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n\\u2060, \\u2060\\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n\\u2060, and \\n  \\n    \\n      \\n        z\\n      \\n    \\n    {\\\\displaystyle z}\\n  \\n represent variables. In some cases, subscripts are added to distinguish variables, as in \\u2060\\n  \\n    \\n      \\n        \\n          x\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{1}}\\n  \\n\\u2060, \\u2060\\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{2}}\\n  \\n\\u2060, and \\u2060\\n  \\n    \\n      \\n        \\n          x\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{3}}\\n  \\n\\u2060. The lowercase letters \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060, \\u2060\\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n\\u2060, and \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n  \\n are usually used for constants and coefficients. The expression \\n  \\n    \\n      \\n        5\\n        x\\n        +\\n        3\\n      \\n    \\n    {\\\\displaystyle 5x+3}\\n  \\n is an algebraic expression created by multiplying the number 5 with the variable \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n and adding the number 3 to the result. Other examples of algebraic expressions are \\n  \\n    \\n      \\n        32\\n        x\\n        y\\n        z\\n      \\n    \\n    {\\\\displaystyle 32xyz}\\n  \\n and \\n  \\n    \\n      \\n        64\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        \\n          \\n\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        7\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        −\\n        c\\n      \\n    \\n    {\\\\displaystyle 64x_{1}{}^{2}+7x_{2}-c}\\n  \\n.\\nSome algebraic expressions take the form of statements that relate two expressions to one another. An equation is a statement formed by comparing two expressions, saying that they are equal. This can be expressed using the equals sign (\\u2060\\n  \\n    \\n      \\n        =\\n      \\n    \\n    {\\\\displaystyle =}\\n  \\n\\u2060), as in \\u2060\\n  \\n    \\n      \\n        5\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        6\\n        x\\n        =\\n        3\\n        y\\n        +\\n        4\\n      \\n    \\n    {\\\\displaystyle 5x^{2}+6x=3y+4}\\n  \\n\\u2060. Inequations involve a different type of comparison, saying that the two sides are different. This can be expressed using symbols such as the less-than sign (\\u2060\\n  \\n    \\n      \\n        <\\n      \\n    \\n    {\\\\displaystyle <}\\n  \\n\\u2060), the greater-than sign (\\u2060\\n  \\n    \\n      \\n        >\\n      \\n    \\n    {\\\\displaystyle >}\\n  \\n\\u2060), and the inequality sign (\\u2060\\n  \\n    \\n      \\n        ≠\\n      \\n    \\n    {\\\\displaystyle \\\\neq }\\n  \\n\\u2060). Unlike other expressions, statements can be true or false, and their truth value usually depends on the values of the variables. For example, the statement \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        =\\n        4\\n      \\n    \\n    {\\\\displaystyle x^{2}=4}\\n  \\n is true if \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is either 2 or −2 and false otherwise. Equations with variables can be divided into identity equations and conditional equations. Identity equations are true for all values that can be assigned to the variables, such as the equation \\u2060\\n  \\n    \\n      \\n        2\\n        x\\n        +\\n        5\\n        x\\n        =\\n        7\\n        x\\n      \\n    \\n    {\\\\displaystyle 2x+5x=7x}\\n  \\n\\u2060. Conditional equations are only true for some values. For example, the equation \\n  \\n    \\n      \\n        x\\n        +\\n        4\\n        =\\n        9\\n      \\n    \\n    {\\\\displaystyle x+4=9}\\n  \\n is only true if \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is 5.\\nThe main goal of elementary algebra is to determine the values for which a statement is true. This can be achieved by transforming and manipulating statements according to certain rules. A key principle guiding this process is that whatever operation is applied to one side of an equation also needs to be done to the other side. For example, if one subtracts 5 from the left side of an equation one also needs to subtract 5 from the right side to balance both sides. The goal of these steps is usually to isolate the variable one is interested in on one side, a process known as solving the equation for that variable. For example, the equation \\n  \\n    \\n      \\n        x\\n        −\\n        7\\n        =\\n        4\\n      \\n    \\n    {\\\\displaystyle x-7=4}\\n  \\n can be solved for \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n by adding 7 to both sides, which isolates \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n on the left side and results in the equation \\u2060\\n  \\n    \\n      \\n        x\\n        =\\n        11\\n      \\n    \\n    {\\\\displaystyle x=11}\\n  \\n\\u2060.\\nThere are many other techniques used to solve equations. Simplification is employed to replace a complicated expression with an equivalent simpler one. For example, the expression \\n  \\n    \\n      \\n        7\\n        x\\n        −\\n        3\\n        x\\n      \\n    \\n    {\\\\displaystyle 7x-3x}\\n  \\n can be replaced with the expression \\n  \\n    \\n      \\n        4\\n        x\\n      \\n    \\n    {\\\\displaystyle 4x}\\n  \\n since \\n  \\n    \\n      \\n        7\\n        x\\n        −\\n        3\\n        x\\n        =\\n        (\\n        7\\n        −\\n        3\\n        )\\n        x\\n        =\\n        4\\n        x\\n      \\n    \\n    {\\\\displaystyle 7x-3x=(7-3)x=4x}\\n  \\n by the distributive property. For statements with several variables, substitution is a common technique to replace one variable with an equivalent expression that does not use this variable. For example, if one knows that \\n  \\n    \\n      \\n        y\\n        =\\n        3\\n        x\\n      \\n    \\n    {\\\\displaystyle y=3x}\\n  \\n then one can simplify the expression \\n  \\n    \\n      \\n        7\\n        x\\n        y\\n      \\n    \\n    {\\\\displaystyle 7xy}\\n  \\n to arrive at \\u2060\\n  \\n    \\n      \\n        21\\n        \\n          x\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 21x^{2}}\\n  \\n\\u2060. In a similar way, if one knows the value of one variable one may be able to use it to determine the value of other variables.\\n\\nAlgebraic equations can be interpreted geometrically to describe spatial figures in the form of a graph. To do so, the different variables in the equation are understood as coordinates and the values that solve the equation are interpreted as points of a graph. For example, if \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is set to zero in the equation \\u2060\\n  \\n    \\n      \\n        y\\n        =\\n        0.5\\n        x\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle y=0.5x-1}\\n  \\n\\u2060, then \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n must be −1 for the equation to be true. This means that the \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n  \\n-pair \\n  \\n    \\n      \\n        (\\n        0\\n        ,\\n        −\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (0,-1)}\\n  \\n is part of the graph of the equation. The \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n  \\n-pair \\u2060\\n  \\n    \\n      \\n        (\\n        0\\n        ,\\n        7\\n        )\\n      \\n    \\n    {\\\\displaystyle (0,7)}\\n  \\n\\u2060, by contrast, does not solve the equation and is therefore not part of the graph. The graph encompasses the totality of \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n  \\n-pairs that solve the equation.\\n\\n\\n==== Polynomials ====\\n\\nA polynomial is an expression consisting of one or more terms that are added or subtracted from each other, like \\u2060\\n  \\n    \\n      \\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        3\\n        x\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        +\\n        5\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle x^{4}+3xy^{2}+5x^{3}-1}\\n  \\n\\u2060. Each term is either a constant, a variable, or a product of a constant and variables. Each variable can be raised to a positive integer power. A monomial is a polynomial with one term while two- and three-term polynomials are called binomials and trinomials. The degree of a polynomial is the maximal value (among its terms) of the sum of the exponents of the variables (4 in the above example). Polynomials of degree one are called linear polynomials. Linear algebra studies systems of linear polynomials. A polynomial is said to be univariate or multivariate, depending on whether it uses one or more variables.\\nFactorization is a method used to simplify polynomials, making it easier to analyze them and determine the values for which they evaluate to zero. Factorization consists of rewriting a polynomial as a product of several factors. For example, the polynomial \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        −\\n        3\\n        x\\n        −\\n        10\\n      \\n    \\n    {\\\\displaystyle x^{2}-3x-10}\\n  \\n can be factorized as \\u2060\\n  \\n    \\n      \\n        (\\n        x\\n        +\\n        2\\n        )\\n        (\\n        x\\n        −\\n        5\\n        )\\n      \\n    \\n    {\\\\displaystyle (x+2)(x-5)}\\n  \\n\\u2060. The polynomial as a whole is zero if and only if one of its factors is zero, i.e., if \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is either −2 or 5. Before the 19th century, much of algebra was devoted to polynomial equations, that is equations obtained by equating a polynomial to zero. The first attempts for solving polynomial equations were to express the solutions in terms of nth roots. The solution of a second-degree polynomial equation of the form \\n  \\n    \\n      \\n        a\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        b\\n        x\\n        +\\n        c\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle ax^{2}+bx+c=0}\\n  \\n is given by the quadratic formula\\n\\n  \\n    \\n      \\n        x\\n        =\\n        \\n          \\n            \\n              −\\n              b\\n              ±\\n              \\n                \\n                  \\n                    b\\n                    \\n                      2\\n                    \\n                  \\n                  −\\n                  4\\n                  a\\n                  c\\n                   \\n                \\n              \\n            \\n            \\n              2\\n              a\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle x={\\\\frac {-b\\\\pm {\\\\sqrt {b^{2}-4ac\\\\ }}}{2a}}.}\\n  \\n\\nSolutions for the degrees 3 and 4 are given by the cubic and quartic formulas. There are no general solutions for higher degrees, as proven in the 19th century by the Abel–Ruffini theorem. Even when general solutions do not exist, approximate solutions can be found by numerical tools like the Newton–Raphson method.\\nThe fundamental theorem of algebra asserts that every univariate polynomial equation of positive degree with real or complex coefficients has at least one complex solution. Consequently, every polynomial of a positive degree can be factorized into linear polynomials. This theorem was proved at the beginning of the 19th century, but this does not close the problem since the theorem does not provide any way for computing the solutions.\\n\\n\\n=== Linear algebra ===\\n\\nLinear algebra starts with the study of systems of linear equations. An equation is linear if it can be expressed in the form \\u2060\\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n        \\n          x\\n          \\n            1\\n          \\n        \\n        +\\n        \\n          a\\n          \\n            2\\n          \\n        \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        .\\n        .\\n        .\\n        +\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        \\n          x\\n          \\n            n\\n          \\n        \\n        =\\n        b\\n      \\n    \\n    {\\\\displaystyle a_{1}x_{1}+a_{2}x_{2}+...+a_{n}x_{n}=b}\\n  \\n\\u2060, where \\u2060\\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1}}\\n  \\n\\u2060, \\u2060\\n  \\n    \\n      \\n        \\n          a\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{2}}\\n  \\n\\u2060, ..., \\n  \\n    \\n      \\n        \\n          a\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{n}}\\n  \\n and \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n are constants. Examples are \\n  \\n    \\n      \\n        \\n          x\\n          \\n            1\\n          \\n        \\n        −\\n        7\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        3\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle x_{1}-7x_{2}+3x_{3}=0}\\n  \\n and \\u2060\\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              4\\n            \\n          \\n          x\\n          −\\n          y\\n          =\\n          4\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle {\\\\frac {1}{4}}x-y=4}\\n  \\n\\u2060. A system of linear equations is a set of linear equations for which one is interested in common solutions.\\nMatrices are rectangular arrays of values that have been originally introduced for having a compact and synthetic notation for systems of linear equations. For example, the system of equations\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                9\\n                \\n                  x\\n                  \\n                    1\\n                  \\n                \\n                +\\n                3\\n                \\n                  x\\n                  \\n                    2\\n                  \\n                \\n                −\\n                13\\n                \\n                  x\\n                  \\n                    3\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                0\\n              \\n            \\n            \\n              \\n                2.3\\n                \\n                  x\\n                  \\n                    1\\n                  \\n                \\n                +\\n                7\\n                \\n                  x\\n                  \\n                    3\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                9\\n              \\n            \\n            \\n              \\n                −\\n                5\\n                \\n                  x\\n                  \\n                    1\\n                  \\n                \\n                −\\n                17\\n                \\n                  x\\n                  \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                −\\n                3\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}9x_{1}+3x_{2}-13x_{3}&=0\\\\\\\\2.3x_{1}+7x_{3}&=9\\\\\\\\-5x_{1}-17x_{2}&=-3\\\\end{aligned}}}\\n  \\n\\ncan be written as \\n\\n  \\n    \\n      \\n        A\\n        X\\n        =\\n        B\\n        ,\\n      \\n    \\n    {\\\\displaystyle AX=B,}\\n  \\n\\nwhere \\u2060\\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  \\n\\u2060, \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n  \\n and \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n  \\n are the matrices\\n\\n  \\n    \\n      \\n        A\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  9\\n                \\n                \\n                  3\\n                \\n                \\n                  −\\n                  13\\n                \\n              \\n              \\n                \\n                  2.3\\n                \\n                \\n                  0\\n                \\n                \\n                  7\\n                \\n              \\n              \\n                \\n                  −\\n                  5\\n                \\n                \\n                  −\\n                  17\\n                \\n                \\n                  0\\n                \\n              \\n            \\n            ]\\n          \\n        \\n        ,\\n        \\n        X\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  \\n                    x\\n                    \\n                      1\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    x\\n                    \\n                      2\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    x\\n                    \\n                      3\\n                    \\n                  \\n                \\n              \\n            \\n            ]\\n          \\n        \\n        ,\\n        \\n        B\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  0\\n                \\n              \\n              \\n                \\n                  9\\n                \\n              \\n              \\n                \\n                  −\\n                  3\\n                \\n              \\n            \\n            ]\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle A={\\\\begin{bmatrix}9&3&-13\\\\\\\\2.3&0&7\\\\\\\\-5&-17&0\\\\end{bmatrix}},\\\\quad X={\\\\begin{bmatrix}x_{1}\\\\\\\\x_{2}\\\\\\\\x_{3}\\\\end{bmatrix}},\\\\quad B={\\\\begin{bmatrix}0\\\\\\\\9\\\\\\\\-3\\\\end{bmatrix}}.}\\n  \\n\\nUnder some conditions on the number of rows and columns, matrices can be added, multiplied, and sometimes inverted. All methods for solving linear systems may be expressed as matrix manipulations using these operations. For example, solving the above system consists of computing an inverted matrix \\n  \\n    \\n      \\n        \\n          A\\n          \\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A^{-1}}\\n  \\n such that \\n  \\n    \\n      \\n        \\n          A\\n          \\n            −\\n            1\\n          \\n        \\n        A\\n        =\\n        I\\n        ,\\n      \\n    \\n    {\\\\displaystyle A^{-1}A=I,}\\n  \\n where \\n  \\n    \\n      \\n        I\\n      \\n    \\n    {\\\\displaystyle I}\\n  \\n is the identity matrix. Then, multiplying on the left both members of the above matrix equation by \\n  \\n    \\n      \\n        \\n          A\\n          \\n            −\\n            1\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle A^{-1},}\\n  \\n one gets the solution of the system of linear equations as\\n\\n  \\n    \\n      \\n        X\\n        =\\n        \\n          A\\n          \\n            −\\n            1\\n          \\n        \\n        B\\n        .\\n      \\n    \\n    {\\\\displaystyle X=A^{-1}B.}\\n  \\n\\nMethods of solving systems of linear equations range from the introductory, like substitution and elimination, to more advanced techniques using matrices, such as Cramer\\'s rule, the Gaussian elimination, and LU decomposition. Some systems of equations are inconsistent, meaning that no solutions exist because the equations contradict each other. Consistent systems have either one unique solution or an infinite number of solutions.\\nThe study of vector spaces and linear maps form a large part of linear algebra. A vector space is an algebraic structure formed by a set with an addition that makes it an abelian group and a scalar multiplication that is compatible with addition (see vector space for details). A linear map is a function between vector spaces that is compatible with addition and scalar multiplication. In the case of finite-dimensional vector spaces, vectors and linear maps can be represented by matrices. It follows that the theories of matrices and finite-dimensional vector spaces are essentially the same. In particular, vector spaces provide a third way for expressing and manipulating systems of linear equations. From this perspective, a matrix is a representation of a linear map: if one chooses a particular basis to describe the vectors being transformed, then the entries in the matrix give the results of applying the linear map to the basis vectors.\\n\\nSystems of equations can be interpreted as geometric figures. For systems with two variables, each equation represents a line in two-dimensional space. The point where the two lines intersect is the solution of the full system because this is the only point that solves both the first and the second equation. For inconsistent systems, the two lines run parallel, meaning that there is no solution since they never intersect. If two equations are not independent then they describe the same line, meaning that every solution of one equation is also a solution of the other equation. These relations make it possible to seek solutions graphically by plotting the equations and determining where they intersect. The same principles also apply to systems of equations with more variables, with the difference being that the equations do not describe lines but higher dimensional figures. For instance, equations with three variables correspond to planes in three-dimensional space, and the points where all planes intersect solve the system of equations.\\n\\n\\n=== Abstract algebra ===\\n\\nAbstract algebra, also called modern algebra, is the study of algebraic structures. An algebraic structure is a framework for understanding operations on mathematical objects, like the addition of numbers. While elementary algebra and linear algebra work within the confines of particular algebraic structures, abstract algebra takes a more general approach that compares how algebraic structures differ from each other and what types of algebraic structures there are, such as groups, rings, and fields. The key difference between these types of algebraic structures lies in the number of operations they use and the laws they obey. In mathematics education, abstract algebra refers to an advanced undergraduate course that mathematics majors take after completing courses in linear algebra.\\n\\nOn a formal level, an algebraic structure is a set of mathematical objects, called the underlying set, together with one or several operations. Abstract algebra is primarily interested in binary operations, which take any two objects from the underlying set as inputs and map them to another object from this set as output. For example, the algebraic structure \\n  \\n    \\n      \\n        ⟨\\n        \\n          N\\n        \\n        ,\\n        +\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle \\\\mathbb {N} ,+\\\\rangle }\\n  \\n has the natural numbers (\\u2060\\n  \\n    \\n      \\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} }\\n  \\n\\u2060) as the underlying set and addition (\\u2060\\n  \\n    \\n      \\n        +\\n      \\n    \\n    {\\\\displaystyle +}\\n  \\n\\u2060) as its binary operation. The underlying set can contain mathematical objects other than numbers, and the operations are not restricted to regular arithmetic operations. For instance, the underlying set of the symmetry group of a geometric object is made up of geometric transformations, such as rotations, under which the object remains unchanged. Its binary operation is function composition, which takes two transformations as input and has the transformation resulting from applying the first transformation followed by the second as its output.\\n\\n\\n==== Group theory ====\\n\\nAbstract algebra classifies algebraic structures based on the laws or axioms that its operations obey and the number of operations it uses. One of the most basic types is a group, which has one operation and requires that this operation is associative and has an identity element and inverse elements. An operation is associative if the order of several applications does not matter, i.e., if \\n  \\n    \\n      \\n        (\\n        a\\n        ∘\\n        b\\n        )\\n        ∘\\n        c\\n      \\n    \\n    {\\\\displaystyle (a\\\\circ b)\\\\circ c}\\n  \\n is the same as \\n  \\n    \\n      \\n        a\\n        ∘\\n        (\\n        b\\n        ∘\\n        c\\n        )\\n      \\n    \\n    {\\\\displaystyle a\\\\circ (b\\\\circ c)}\\n  \\n for all elements. An operation has an identity element or a neutral element if one element e exists that does not change the value of any other element, i.e., if \\u2060\\n  \\n    \\n      \\n        a\\n        ∘\\n        e\\n        =\\n        e\\n        ∘\\n        a\\n        =\\n        a\\n      \\n    \\n    {\\\\displaystyle a\\\\circ e=e\\\\circ a=a}\\n  \\n\\u2060. An operation has inverse elements if for any element \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n there exists a reciprocal element \\n  \\n    \\n      \\n        \\n          a\\n          \\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{-1}}\\n  \\n that undoes \\u2060\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n\\u2060. If an element operates on its inverse then the result is the neutral element e, expressed formally as \\u2060\\n  \\n    \\n      \\n        a\\n        ∘\\n        \\n          a\\n          \\n            −\\n            1\\n          \\n        \\n        =\\n        \\n          a\\n          \\n            −\\n            1\\n          \\n        \\n        ∘\\n        a\\n        =\\n        e\\n      \\n    \\n    {\\\\displaystyle a\\\\circ a^{-1}=a^{-1}\\\\circ a=e}\\n  \\n\\u2060. Every algebraic structure that fulfills these requirements is a group. For example, \\n  \\n    \\n      \\n        ⟨\\n        \\n          Z\\n        \\n        ,\\n        +\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle \\\\mathbb {Z} ,+\\\\rangle }\\n  \\n is a group formed by the set of integers together with the operation of addition. The neutral element is 0 and the inverse element of any number \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n is \\n  \\n    \\n      \\n        −\\n        a\\n      \\n    \\n    {\\\\displaystyle -a}\\n  \\n. The natural numbers with addition, by contrast, do not form a group since they contain only positive integers and therefore lack inverse elements.\\nGroup theory examines the nature of groups, with basic theorems such as the fundamental theorem of finite abelian groups and the Feit–Thompson theorem. The latter was a key early step in one of the most important mathematical achievements of the 20th century: the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 2004, that culminated in a complete classification of finite simple groups.\\n\\n\\n==== Ring theory and field theory ====\\n\\nA ring is an algebraic structure with two operations that work similarly to the addition and multiplication of numbers and are named and generally denoted similarly. A ring is a commutative group under addition: the addition of the ring is associative, commutative, and has an identity element and inverse elements. The multiplication is associative and distributive with respect to addition; that is, \\n  \\n    \\n      \\n        a\\n        (\\n        b\\n        +\\n        c\\n        )\\n        =\\n        a\\n        b\\n        +\\n        a\\n        c\\n      \\n    \\n    {\\\\displaystyle a(b+c)=ab+ac}\\n  \\n and \\n  \\n    \\n      \\n        (\\n        b\\n        +\\n        c\\n        )\\n        a\\n        =\\n        b\\n        a\\n        +\\n        c\\n        a\\n        .\\n      \\n    \\n    {\\\\displaystyle (b+c)a=ba+ca.}\\n  \\n Moreover, multiplication is associative and has an identity element generally denoted as 1. Multiplication needs not to be commutative; if it is commutative, one has a commutative ring. The ring of integers (\\u2060\\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n  \\n\\u2060) is one of the simplest commutative rings.\\nA field is a commutative ring such that \\u2060\\n  \\n    \\n      \\n        1\\n        ≠\\n        0\\n      \\n    \\n    {\\\\displaystyle 1\\\\neq 0}\\n  \\n\\u2060 and each nonzero element has a multiplicative inverse. The ring of integers does not form a field because it lacks multiplicative inverses. For example, the multiplicative inverse of \\n  \\n    \\n      \\n        7\\n      \\n    \\n    {\\\\displaystyle 7}\\n  \\n is \\u2060\\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              7\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {1}{7}}}\\n  \\n\\u2060, which is not an integer. The rational numbers, the real numbers, and the complex numbers each form a field with the operations of addition and multiplication.\\nRing theory is the study of rings, exploring concepts such as subrings, quotient rings, polynomial rings, and ideals as well as theorems such as Hilbert\\'s basis theorem. Field theory is concerned with fields, examining field extensions, algebraic closures, and finite fields. Galois theory explores the relation between field theory and group theory, relying on the fundamental theorem of Galois theory.\\n\\n\\n==== Theories of interrelations among structures ====\\n\\nBesides groups, rings, and fields, there are many other algebraic structures studied by algebra. They include magmas, semigroups, monoids, abelian groups, commutative rings, modules, lattices, vector spaces, algebras over a field, and associative and non-associative algebras. They differ from each other regarding the types of objects they describe and the requirements that their operations fulfill. Many are related to each other in that a basic structure can be turned into a more specialized structure by adding constraints. For example, a magma becomes a semigroup if its operation is associative.\\nHomomorphisms are tools to examine structural features by comparing two algebraic structures. A homomorphism is a function from the underlying set of one algebraic structure to the underlying set of another algebraic structure that preserves certain structural characteristics. If the two algebraic structures use binary operations and have the form \\n  \\n    \\n      \\n        ⟨\\n        A\\n        ,\\n        ∘\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle A,\\\\circ \\\\rangle }\\n  \\n and \\n  \\n    \\n      \\n        ⟨\\n        B\\n        ,\\n        ⋆\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle B,\\\\star \\\\rangle }\\n  \\n then the function \\n  \\n    \\n      \\n        h\\n        :\\n        A\\n        →\\n        B\\n      \\n    \\n    {\\\\displaystyle h:A\\\\to B}\\n  \\n is a homomorphism if it fulfills the following requirement: \\u2060\\n  \\n    \\n      \\n        h\\n        (\\n        x\\n        ∘\\n        y\\n        )\\n        =\\n        h\\n        (\\n        x\\n        )\\n        ⋆\\n        h\\n        (\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle h(x\\\\circ y)=h(x)\\\\star h(y)}\\n  \\n\\u2060. The existence of a homomorphism reveals that the operation \\n  \\n    \\n      \\n        ⋆\\n      \\n    \\n    {\\\\displaystyle \\\\star }\\n  \\n in the second algebraic structure plays the same role as the operation \\n  \\n    \\n      \\n        ∘\\n      \\n    \\n    {\\\\displaystyle \\\\circ }\\n  \\n does in the first algebraic structure. Isomorphisms are a special type of homomorphism that indicates a high degree of similarity between two algebraic structures. An isomorphism is a bijective homomorphism, meaning that it establishes a one-to-one relationship between the elements of the two algebraic structures. This implies that every element of the first algebraic structure is mapped to one unique element in the second structure without any unmapped elements in the second structure.\\n\\nAnother tool of comparison is the relation between an algebraic structure and its subalgebra. The algebraic structure and its subalgebra use the same operations, which follow the same axioms. The only difference is that the underlying set of the subalgebra is a subset of the underlying set of the algebraic structure. All operations in the subalgebra are required to be closed in its underlying set, meaning that they only produce elements that belong to this set. For example, the set of even integers together with addition is a subalgebra of the full set of integers together with addition. This is the case because the sum of two even numbers is again an even number. But the set of odd integers together with addition is not a subalgebra because it is not closed: adding two odd numbers produces an even number, which is not part of the chosen subset.\\nUniversal algebra is the study of algebraic structures in general. As part of its general perspective, it is not concerned with the specific elements that make up the underlying sets and considers operations with more than two inputs, such as ternary operations. It provides a framework for investigating what structural features different algebraic structures have in common. One of those structural features concerns the identities that are true in different algebraic structures. In this context, an identity is a universal equation or an equation that is true for all elements of the underlying set. For example, commutativity is a universal equation that states that \\n  \\n    \\n      \\n        a\\n        ∘\\n        b\\n      \\n    \\n    {\\\\displaystyle a\\\\circ b}\\n  \\n is identical to \\n  \\n    \\n      \\n        b\\n        ∘\\n        a\\n      \\n    \\n    {\\\\displaystyle b\\\\circ a}\\n  \\n for all elements. A variety is a class of all algebraic structures that satisfy certain identities. For example, if two algebraic structures satisfy commutativity then they are both part of the corresponding variety.\\nCategory theory examines how mathematical objects are related to each other using the concept of categories. A category is a collection of objects together with a collection of morphisms or \"arrows\" between those objects. These two collections must satisfy certain conditions. For example, morphisms can be joined, or composed: if there exists a morphism from object \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n to object \\u2060\\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n\\u2060, and another morphism from object \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n to object \\u2060\\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n  \\n\\u2060, then there must also exist one from object \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n to object \\u2060\\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n  \\n\\u2060. Composition of morphisms is required to be associative, and there must be an \"identity morphism\" for every object. Categories are widely used in contemporary mathematics since they provide a unifying framework to describe and analyze many fundamental mathematical concepts. For example, sets can be described with the category of sets, and any group can be regarded as the morphisms of a category with just one object.\\n\\n\\n== History ==\\n\\nThe origin of algebra lies in attempts to solve mathematical problems involving arithmetic calculations and unknown quantities. These developments happened in the ancient period in Babylonia, Egypt, Greece, China, and India. One of the earliest documents on algebraic problems is the Rhind Mathematical Papyrus from ancient Egypt, which was written around 1650 BCE. It discusses solutions to linear equations, as expressed in problems like \"A quantity; its fourth is added to it. It becomes fifteen. What is the quantity?\" Babylonian clay tablets from around the same time explain methods to solve linear and quadratic polynomial equations, such as the method of completing the square.\\nMany of these insights found their way to the ancient Greeks. Starting in the 6th century BCE, their main interest was geometry rather than algebra, but they employed algebraic methods to solve geometric problems. For example, they studied geometric figures while taking their lengths and areas as unknown quantities to be determined, as exemplified in Pythagoras\\' formulation of the difference of two squares method and later in Euclid\\'s Elements. In the 3rd century CE, Diophantus provided a detailed treatment of how to solve algebraic equations in a series of books called Arithmetica. He was the first to experiment with symbolic notation to express polynomials. Diophantus\\'s work influenced Arab development of algebra with many of his methods reflected in the concepts and techniques used in medieval Arabic algebra. In ancient China, The Nine Chapters on the Mathematical Art, a book composed over the period spanning from the 10th century BCE to the 2nd century CE, explored various techniques for solving algebraic equations, including the use of matrix-like constructs.\\nThere is no unanimity of opinion as to whether these early developments are part of algebra or only precursors. They offered solutions to algebraic problems but did not conceive them in an abstract and general manner, focusing instead on specific cases and applications. This changed with the Persian mathematician al-Khwarizmi, who published his The Compendious Book on Calculation by Completion and Balancing in 825 CE. It presents the first detailed treatment of general methods that can be used to manipulate linear and quadratic equations by \"reducing\" and \"balancing\" both sides. Other influential contributions to algebra came from the Arab mathematician Thābit ibn Qurra also in the 9th century and the Persian mathematician Omar Khayyam in the 11th and 12th centuries.\\nIn India, Brahmagupta investigated how to solve quadratic equations and systems of equations with several variables in the 7th century CE. Among his innovations were the use of zero and negative numbers in algebraic equations. The Indian mathematicians Mahāvīra in the 9th century and Bhāskara II in the 12th century further refined Brahmagupta\\'s methods and concepts. In 1247, the Chinese mathematician Qin Jiushao wrote the Mathematical Treatise in Nine Sections, which includes an algorithm for the numerical evaluation of polynomials, including polynomials of higher degrees.\\n\\nThe Italian mathematician Fibonacci brought al-Khwarizmi\\'s ideas and techniques to Europe in books including his Liber Abaci. In 1545, the Italian polymath Gerolamo Cardano published his book Ars Magna, which covered many topics in algebra, discussed imaginary numbers, and was the first to present general methods for solving cubic and quartic equations. In the 16th and 17th centuries, the French mathematicians François Viète and René Descartes introduced letters and symbols to denote variables and operations, making it possible to express equations in an concise and abstract manner. Their predecessors had relied on verbal descriptions of problems and solutions. Some historians see this development as a key turning point in the history of algebra and consider what came before it as the prehistory of algebra because it lacked the abstract nature based on symbolic manipulation.\\nIn the 17th and 18th centuries, many attempts were made to find general solutions to polynomials of degree five and higher. All of them failed. At the end of the 18th century, the German mathematician Carl Friedrich Gauss proved the fundamental theorem of algebra, which describes the existence of zeros of polynomials of any degree without providing a general solution. At the beginning of the 19th century, the Italian mathematician Paolo Ruffini and the Norwegian mathematician Niels Henrik Abel were able to show that no general solution exists for polynomials of degree five and higher. In response to and shortly after their findings, the French mathematician Évariste Galois developed what came later to be known as Galois theory, which offered a more in-depth analysis of the solutions of polynomials while also laying the foundation of group theory. Mathematicians soon realized the relevance of group theory to other fields and applied it to disciplines like geometry and number theory.\\n\\nStarting in the mid-19th century, interest in algebra shifted from the study of polynomials associated with elementary algebra towards a more general inquiry into algebraic structures, marking the emergence of abstract algebra. This approach explored the axiomatic basis of arbitrary algebraic operations. The invention of new algebraic systems based on different operations and elements accompanied this development, such as Boolean algebra, vector algebra, and matrix algebra. Influential early developments in abstract algebra were made by the German mathematicians David Hilbert, Ernst Steinitz, and Emmy Noether as well as the Austrian mathematician Emil Artin. They researched different forms of algebraic structures and categorized them based on their underlying axioms into types, like groups, rings, and fields.\\nThe idea of the even more general approach associated with universal algebra was conceived by the English mathematician Alfred North Whitehead in his 1898 book A Treatise on Universal Algebra. Starting in the 1930s, the American mathematician Garrett Birkhoff expanded these ideas and developed many of the foundational concepts of this field. The invention of universal algebra led to the emergence of various new areas focused on the algebraization of mathematics—that is, the application of algebraic methods to other branches of mathematics. Topological algebra arose in the early 20th century, studying algebraic structures such as topological groups and Lie groups. In the 1940s and 50s, homological algebra emerged, employing algebraic techniques to study homology. Around the same time, category theory was developed and has since played a key role in the foundations of mathematics. Other developments were the formulation of model theory and the study of free algebras.\\n\\n\\n== Applications ==\\n\\nThe influence of algebra is wide-reaching, both within mathematics and in its applications to other fields. The algebraization of mathematics is the process of applying algebraic methods and principles to other branches of mathematics, such as geometry, topology, number theory, and calculus. It happens by employing symbols in the form of variables to express mathematical insights on a more general level, allowing mathematicians to develop formal models describing how objects interact and relate to each other.\\n\\nOne application, found in geometry, is the use of algebraic statements to describe geometric figures. For example, the equation \\n  \\n    \\n      \\n        y\\n        =\\n        3\\n        x\\n        −\\n        7\\n      \\n    \\n    {\\\\displaystyle y=3x-7}\\n  \\n describes a line in two-dimensional space while the equation \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          z\\n          \\n            2\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle x^{2}+y^{2}+z^{2}=1}\\n  \\n corresponds to a sphere in three-dimensional space. Of special interest to algebraic geometry are algebraic varieties, which are solutions to systems of polynomial equations that can be used to describe more complex geometric figures. Algebraic reasoning can also solve geometric problems. For example, one can determine whether and where the line described by \\n  \\n    \\n      \\n        y\\n        =\\n        x\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle y=x+1}\\n  \\n intersects with the circle described by \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        =\\n        25\\n      \\n    \\n    {\\\\displaystyle x^{2}+y^{2}=25}\\n  \\n by solving the system of equations made up of these two equations. Topology studies the properties of geometric figures or topological spaces that are preserved under operations of continuous deformation. Algebraic topology relies on algebraic theories such as group theory to classify topological spaces. For example, homotopy groups classify topological spaces based on the existence of loops or holes in them.\\nNumber theory is concerned with the properties of and relations between integers. Algebraic number theory applies algebraic methods and principles to this field of inquiry. Examples are the use of algebraic expressions to describe general laws, like Fermat\\'s Last Theorem, and of algebraic structures to analyze the behavior of numbers, such as the ring of integers. The related field of combinatorics uses algebraic techniques to solve problems related to counting, arrangement, and combination of discrete objects. An example in algebraic combinatorics is the application of group theory to analyze graphs and symmetries. The insights of algebra are also relevant to calculus, which uses mathematical expressions to examine rates of change and accumulation. It relies on algebra, for instance, to understand how these expressions can be transformed and what role variables play in them. Algebraic logic employs the methods of algebra to describe and analyze the structures and patterns that underlie logical reasoning, exploring both the relevant mathematical structures themselves and their application to concrete problems of logic. It includes the study of Boolean algebra to describe propositional logic as well as the formulation and analysis of algebraic structures corresponding to more complex systems of logic.\\n\\nAlgebraic methods are also commonly employed in other areas, like the natural sciences. For example, they are used to express scientific laws and solve equations in physics, chemistry, and biology. Similar applications are found in fields like economics, geography, engineering (including electronics and robotics), and computer science to express relationships, solve problems, and model systems. Linear algebra plays a central role in artificial intelligence and machine learning, for instance, by enabling the efficient processing and analysis of large datasets. Various fields rely on algebraic structures investigated by abstract algebra. For example, physical sciences like crystallography and quantum mechanics make extensive use of group theory, which is also employed to study puzzles such as Sudoku and Rubik\\'s Cubes, and origami. Both coding theory and cryptology rely on abstract algebra to solve problems associated with data transmission, like avoiding the effects of noise and ensuring data security.\\n\\n\\n== Education ==\\n\\nAlgebra education mostly focuses on elementary algebra, which is one of the reasons why elementary algebra is also called school algebra. It is usually not introduced until secondary education since it requires mastery of the fundamentals of arithmetic while posing new cognitive challenges associated with abstract reasoning and generalization. It aims to familiarize students with the formal side of mathematics by helping them understand mathematical symbolism, for example, how variables can be used to represent unknown quantities. An additional difficulty for students lies in the fact that, unlike arithmetic calculations, algebraic expressions are often difficult to solve directly. Instead, students need to learn how to transform them according to certain laws, often to determine an unknown quantity.\\nSome tools to introduce students to the abstract side of algebra rely on concrete models and visualizations of equations, including geometric analogies, manipulatives including sticks or cups, and \"function machines\" representing equations as flow diagrams. One method uses balance scales as a pictorial approach to help students grasp basic problems of algebra. The mass of some objects on the scale is unknown and represents variables. Solving an equation corresponds to adding and removing objects on both sides in such a way that the sides stay in balance until the only object remaining on one side is the object of unknown mass. Word problems are another tool to show how algebra is applied to real-life situations. For example, students may be presented with a situation in which Naomi\\'s brother has twice as many apples as Naomi. Given that both together have twelve apples, students are then asked to find an algebraic equation that describes this situation (\\u2060\\n  \\n    \\n      \\n        2\\n        x\\n        +\\n        x\\n        =\\n        12\\n      \\n    \\n    {\\\\displaystyle 2x+x=12}\\n  \\n\\u2060) and to determine how many apples Naomi has (\\u2060\\n  \\n    \\n      \\n        x\\n        =\\n        4\\n      \\n    \\n    {\\\\displaystyle x=4}\\n  \\n\\u2060).\\nAt the university level, mathematics students encounter advanced algebra topics from linear and abstract algebra. Initial undergraduate courses in linear algebra focus on matrices, vector spaces, and linear maps. Upon completing them, students are usually introduced to abstract algebra, where they learn about algebraic structures like groups, rings, and fields, as well as the relations between them. The curriculum typically also covers specific instances of algebraic structures, such as the systems of rational numbers, the real numbers, and the polynomials.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n=== Notes ===\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\\n\\n== External links ==\\n\\n[Number theory]\\nNumber theory is a branch of pure mathematics devoted primarily to the study of the integers and arithmetic functions. Number theorists study prime numbers as well as the properties of mathematical objects constructed from integers (for example, rational numbers), or defined as generalizations of the integers (for example, algebraic integers). \\nIntegers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory can often be understood through the study of analytical objects, such as the Riemann zeta function, that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, as for instance how irrational numbers can be approximated by fractions (Diophantine approximation).\\nNumber theory is one of the oldest branches of mathematics alongside geometry. One quirk of number theory is that it deals with statements that are simple to understand but are very difficult to solve. Examples of this are Fermat\\'s Last Theorem, which was proved 358 years after the original formulation, and Goldbach\\'s conjecture, which remains unsolved since the 18th century. German mathematician Carl Friedrich Gauss (1777–1855) once remarked, \"Mathematics is the queen of the sciences—and number theory is the queen of mathematics.\" It was regarded as the example of pure mathematics with no applications outside mathematics until the 1970s, when it became known that prime numbers would be used as the basis for the creation of public-key cryptography algorithms.\\n\\n\\n== Definition ==\\nNumber theory is the branch of mathematics that studies integers and their properties and relations. The integers comprise a set that extends the set of natural numbers \\n  \\n    \\n      \\n        {\\n        1\\n        ,\\n        2\\n        ,\\n        3\\n        ,\\n        …\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{1,2,3,\\\\dots \\\\}}\\n  \\n to include number \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n  \\n and the negation of natural numbers \\n  \\n    \\n      \\n        {\\n        −\\n        1\\n        ,\\n        −\\n        2\\n        ,\\n        −\\n        3\\n        ,\\n        …\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{-1,-2,-3,\\\\dots \\\\}}\\n  \\n. Number theorists study prime numbers as well as the properties of mathematical objects constructed from integers (for example, rational numbers), or defined as generalizations of the integers (for example, algebraic integers).\\nNumber theory is closely related to arithmetic and some authors use the terms as synonyms. However, the word \"arithmetic\" is used today to mean the study of numerical operations and extends to the real numbers. In a more specific sense, number theory is restricted to the study of integers and focuses on their properties and relationships. Traditionally, it is known as higher arithmetic. By the early twentieth century, the term number theory had been widely adopted. The term number means whole numbers, which refers to either the natural numbers or the integers.\\nElementary number theory studies aspects of integers that can be investigated using elementary methods such as elementary proofs. Analytic number theory, by contrast, relies on complex numbers and techniques from analysis and calculus. Algebraic number theory employs algebraic structures such as fields and rings to analyze the properties of and relations between numbers. Geometric number theory uses concepts from geometry to study numbers. Further branches of number theory are probabilistic number theory, combinatorial number theory, computational number theory, and applied number theory, which examines the application of number theory to science and technology.\\n\\n\\n== History ==\\n\\nIn recorded history, knowledge of numbers existed in the ancient civilisations of Mesopotamia, Egypt, China, and India. The earliest historical find of an arithmetical nature is the Plimpton 322, dated c. 1800 BC. It is a broken clay tablet that contains a list of Pythagorean triples, that is, integers \\n  \\n    \\n      \\n        (\\n        a\\n        ,\\n        b\\n        ,\\n        c\\n        )\\n      \\n    \\n    {\\\\displaystyle (a,b,c)}\\n  \\n such that \\n  \\n    \\n      \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          b\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{2}+b^{2}=c^{2}}\\n  \\n. The triples are too numerous and too large to have been obtained by brute force. The table\\'s layout suggests that it was constructed by means of what amounts, in modern language, to the identity\\n  \\n    \\n      \\n        \\n          \\n            (\\n            \\n              \\n                \\n                  1\\n                  2\\n                \\n              \\n              \\n                (\\n                \\n                  x\\n                  −\\n                  \\n                    \\n                      1\\n                      x\\n                    \\n                  \\n                \\n                )\\n              \\n            \\n            )\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        1\\n        =\\n        \\n          \\n            (\\n            \\n              \\n                \\n                  1\\n                  2\\n                \\n              \\n              \\n                (\\n                \\n                  x\\n                  +\\n                  \\n                    \\n                      1\\n                      x\\n                    \\n                  \\n                \\n                )\\n              \\n            \\n            )\\n          \\n          \\n            2\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\left({\\\\frac {1}{2}}\\\\left(x-{\\\\frac {1}{x}}\\\\right)\\\\right)^{2}+1=\\\\left({\\\\frac {1}{2}}\\\\left(x+{\\\\frac {1}{x}}\\\\right)\\\\right)^{2},}\\n  \\nwhich is implicit in routine Old Babylonian exercises. It has been suggested instead that the table was a source of numerical examples for school problems. Plimpton 322 tablet is the only surviving evidence of what today would be called number theory within Babylonian mathematics, though a kind of Babylonian algebra was much more developed.\\nAlthough other civilizations probably influenced Greek mathematics at the beginning, all evidence of such borrowings appear relatively late, and it is likely that Greek arithmētikḗ (the theoretical or philosophical study of numbers) is an indigenous tradition. The ancient Greeks developed a keen interest in divisibility. The Pythagoreans attributed mystical quality to perfect and amicable numbers. The Pythagorean tradition also spoke of so-called polygonal or figurate numbers. Euclid devoted part of his Elements to topics that belong to elementary number theory, including prime numbers and divisibility. He gave the Euclidean algorithm for computing the greatest common divisor of two numbers and a proof implying the infinitude of primes. The foremost authority in arithmētikḗ in Late Antiquity was Diophantus of Alexandria, who probably lived in the 3rd century AD. He wrote Arithmetica, a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        \\n          z\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x,y)=z^{2}}\\n  \\n or \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        ,\\n        z\\n        )\\n        =\\n        \\n          w\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x,y,z)=w^{2}}\\n  \\n. In modern parlance, Diophantine equations are polynomial equations to which rational or integer solutions are sought.\\nAfter the fall of Rome, development shifted to Asia, albeit intermittently. The Chinese remainder theorem appears as an exercise in Sunzi Suanjing (between the third and fifth centuries). The result was later generalized with a complete solution called Da-yan-shu (大衍術) in Qin Jiushao\\'s 1247 Mathematical Treatise in Nine Sections. There is also some numerical mysticism in Chinese mathematics, but, unlike that of the Pythagoreans, it seems to have led nowhere. While Greek astronomy probably influenced Indian learning it seems to be the case that Indian mathematics is otherwise an autochthonous tradition. Āryabhaṭa (476–550 AD) showed that pairs of simultaneous congruences \\n  \\n    \\n      \\n        n\\n        ≡\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        \\n          \\n            mod\\n            \\n              m\\n            \\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n\\\\equiv a_{1}{\\\\bmod {m}}_{1}}\\n  \\n, \\n  \\n    \\n      \\n        n\\n        ≡\\n        \\n          a\\n          \\n            2\\n          \\n        \\n        \\n          \\n            mod\\n            \\n              m\\n            \\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n\\\\equiv a_{2}{\\\\bmod {m}}_{2}}\\n  \\n could be solved by a method he called kuṭṭaka, or pulveriser; this is a procedure close to the Euclidean algorithm. Āryabhaṭa seems to have had in mind applications to astronomical calculations. Brahmagupta (628 AD) started the systematic study of indefinite quadratic equations—in particular, the Pell equation. A general procedure for solving Pell\\'s equation was probably found by Jayadeva; the earliest surviving exposition appears in Bhāskara II\\'s Bīja-gaṇita (twelfth century).\\nIn the early ninth century, the caliph al-Ma\\'mun ordered translations of many Greek mathematical works and at least one Sanskrit work.\\nDiophantus\\'s main work, the Arithmetica, was translated into Arabic by Qusta ibn Luqa (820–912).\\nPart of the treatise al-Fakhri (by al-Karajī, 953 – c. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī\\'s contemporary Ibn al-Haytham knew what would later be called Wilson\\'s theorem. Other than a treatise on squares in arithmetic progression by Fibonacci no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus\\' Arithmetica.\\n\\nFrench mathematician Pierre de Fermat (1607–1665) never published his writings but communicated through correspondence and wrote in marginal notes instead. His contributions to number theory brought renewed interest in the field in Europe. He conjectured Fermat\\'s little theorem, a basic result in modular arithmetic, and Fermat\\'s Last Theorem, as well as proved Fermat\\'s right triangle theorem. He also studied prime numbers, the four-square theorem, and Pell\\'s equations.\\nThe interest of Leonhard Euler (1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur Christian Goldbach, pointed him towards some of Fermat\\'s work on the subject. This has been called the \"rebirth\" of modern number theory, after Fermat\\'s relative lack of success in getting his contemporaries\\' attention for the subject. He proved Fermat\\'s assertions, including Fermat\\'s little theorem; made initial work towards a proof that every integer is the sum of four squares; and specific cases of Fermat\\'s Last Theorem. He wrote on the link between continued fractions and Pell\\'s equation. He made the first steps towards analytic number theory.\\nThree European contemporaries continued the work in elementary number theory. Joseph-Louis Lagrange (1736–1813) gave full proofs of the four-square theorem, Wilson\\'s theorem, and developed the basic theory of Pell\\'s equations. Adrien-Marie Legendre (1752–1833) stated the law of quadratic reciprocity. He also conjectured what amounts to the prime number theorem and Dirichlet\\'s theorem on arithmetic progressions. He gave a full treatment of the equation \\n  \\n    \\n      \\n        a\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        b\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        +\\n        c\\n        \\n          z\\n          \\n            2\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle ax^{2}+by^{2}+cz^{2}=0}\\n  \\n. In his old age, he was the first to prove Fermat\\'s Last Theorem for \\n  \\n    \\n      \\n        n\\n        =\\n        5\\n      \\n    \\n    {\\\\displaystyle n=5}\\n  \\n. Carl Friedrich Gauss (1777–1855) wrote Disquisitiones Arithmeticae (1801), which had an immense influence in the area of number theory and set its agenda for much of the 19th century. Gauss proved in this work the law of quadratic reciprocity and developed the theory of quadratic forms. He also introduced some basic notation to congruences and devoted a section to computational matters, including primality tests. He established a link between roots of unity and number theory. In this way, Gauss arguably made forays towards Évariste Galois\\'s work and the area algebraic number theory.\\n\\nStarting early in the nineteenth century, the following developments gradually took place:\\n\\nThe rise to self-consciousness of number theory (or higher arithmetic) as a field of study.\\nThe development of much of modern mathematics necessary for basic modern number theory: complex analysis, group theory, Galois theory—accompanied by greater rigor in analysis and abstraction in algebra.\\nThe rough subdivision of number theory into its modern subfields—in particular, analytic and algebraic number theory.\\nAlgebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet\\'s theorem on arithmetic progressions (1837), whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable. The first use of analytic ideas in number theory actually goes back to Euler (1730s), who used formal power series and non-rigorous (or implicit) limiting arguments. The use of complex analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point; Jacobi\\'s four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).\\nThe American Mathematical Society awards the Cole Prize in Number Theory. Moreover, number theory is one of the three mathematical subdisciplines rewarded by the Fermat Prize.\\n\\n\\n== Main subdivisions ==\\n\\n\\n=== Elementary number theory ===\\n\\nElementary number theory deals with the topics in number theory by means of basic methods in arithmetic. Its primary subjects of study are divisibility, factorization, and primality, as well as congruences in modular arithmetic. Other topics in elementary number theory include Diophantine equations, continued fractions, integer partitions, and Diophantine approximations.\\nArithmetic is the study of numerical operations and investigates how numbers are combined and transformed using the arithmetic operations of addition, subtraction, multiplication, division, exponentiation, extraction of roots, and logarithms. Multiplication, for instance, is an operation that combines two numbers, referred to as factors, to form a single number, termed the product, such as \\n  \\n    \\n      \\n        2\\n        ×\\n        3\\n        =\\n        6\\n      \\n    \\n    {\\\\displaystyle 2\\\\times 3=6}\\n  \\n.\\nDivisibility is a property between two nonzero integers related to division. An integer \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n is said to be divisible by a nonzero integer \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n if \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n is a multiple of \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n; that is, if there exists an integer \\n  \\n    \\n      \\n        q\\n      \\n    \\n    {\\\\displaystyle q}\\n  \\n such that \\n  \\n    \\n      \\n        a\\n        =\\n        b\\n        q\\n      \\n    \\n    {\\\\displaystyle a=bq}\\n  \\n. An equivalent formulation is that \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n divides \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n and is denoted by a vertical bar, which in this case is \\n  \\n    \\n      \\n        b\\n        \\n          |\\n        \\n        a\\n      \\n    \\n    {\\\\displaystyle b|a}\\n  \\n. Conversely, if this were not the case, then \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n would not be divided evenly by \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n, resulting in a remainder. Euclid\\'s division lemma asserts that \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n and \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n can generally be written as \\n  \\n    \\n      \\n        a\\n        =\\n        b\\n        q\\n        +\\n        r\\n      \\n    \\n    {\\\\displaystyle a=bq+r}\\n  \\n, where the remainder \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n  \\n accounts for the smallest positive leftover quantity. Elementary number theory studies divisibility rules in order to quickly identify if a given integer is divisible by a fixed divisor. For instance, it is known that any integer is divisible by 3 if its decimal digit sum is divisible by 3.\\n\\nA common divisor of several nonzero integers is an integer that divides all of them. The greatest common divisor (gcd) is the largest of such divisors. Two integers are said to be coprime or relatively prime to one another if their greatest common divisor, and simultaneously their only divisor, is 1. The Euclidean algorithm computes the greatest common divisor of two integers \\n  \\n    \\n      \\n        a\\n        ,\\n        b\\n      \\n    \\n    {\\\\displaystyle a,b}\\n  \\n by means of repeatedly applying the division lemma and shifting the divisor and remainder after every step. The algorithm can be extended to solve a special case of linear Diophantine equations \\n  \\n    \\n      \\n        a\\n        x\\n        +\\n        b\\n        y\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle ax+by=1}\\n  \\n. A Diophantine equation has several unknowns and integer coefficients. Another kind of Diophantine equation is described in the Pythagorean theorem, \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          z\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{2}+y^{2}=z^{2}}\\n  \\n, whose solutions are called Pythagorean triples if they are all integers. Another kind of expression is the continued fraction, which writes a sum of an integer and a fraction whose denominator is another such sum.\\nElementary number theory studies the divisibility properties of integers such as parity (even and odd numbers), prime numbers, and perfect numbers. Important number-theoric functions include the divisor-counting function, the divisor summatory function and its modifications, and Euler\\'s totient function. A prime number is an integer greater than 1 whose only positive divisors are 1 and the prime itself. A positive integer greater than 1 that is not prime is called a composite number. Euclid\\'s theorem demonstrates that there are infinitely many prime numbers that comprise the set {2, 3, 5, 7, 11, ...}. The sieve of Eratosthenes was devised as an efficient algorithm for identifying all primes up to a given natural number by eliminating all composite numbers.\\nFactorization is a method of expressing a number as a product. Specifically in number theory, integer factorization is the decomposition of an integer into a product of integers. The process of repeatedly applying this procedure until all factors are prime is known as prime factorization. A fundamental property of primes is shown in Euclid\\'s lemma. It is a consequence of the lemma that if a prime divides a product of integers, then that prime divides at least one of the factors in the product. The unique factorization theorem is the fundamental theorem of arithmetic that relates to prime factorization. The theorem states that every integer greater than 1 can be factorised into a product of prime numbers and that this factorisation is unique up to the order of the factors. For example, \\n  \\n    \\n      \\n        120\\n      \\n    \\n    {\\\\displaystyle 120}\\n  \\n is expressed uniquely as \\n  \\n    \\n      \\n        2\\n        ×\\n        2\\n        ×\\n        2\\n        ×\\n        3\\n        ×\\n        5\\n      \\n    \\n    {\\\\displaystyle 2\\\\times 2\\\\times 2\\\\times 3\\\\times 5}\\n  \\n or simply \\n  \\n    \\n      \\n        \\n          2\\n          \\n            3\\n          \\n        \\n        ×\\n        3\\n        ×\\n        5\\n      \\n    \\n    {\\\\displaystyle 2^{3}\\\\times 3\\\\times 5}\\n  \\n.\\nModular arithmetic works with finite sets of integers and introduces the concepts of congruence and residue classes. A congruence of two integers \\n  \\n    \\n      \\n        a\\n        ,\\n        b\\n      \\n    \\n    {\\\\displaystyle a,b}\\n  \\n modulo \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n (a positive integer called the modulus) is an equivalence relation whereby \\n  \\n    \\n      \\n        n\\n        \\n          |\\n        \\n        (\\n        a\\n        −\\n        b\\n        )\\n      \\n    \\n    {\\\\displaystyle n|(a-b)}\\n  \\n is true. Performing Euclidean division on both \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n and \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n, and on \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n and \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n, yields the same remainder. This written as \\n  \\n    \\n      \\n        a\\n        ≡\\n        b\\n        \\n          \\n          (\\n          mod\\n          \\n          n\\n          )\\n        \\n      \\n    \\n    {\\\\textstyle a\\\\equiv b{\\\\pmod {n}}}\\n  \\n. In a manner analogous to the 12-hour clock, the sum of 4 and 9 is equal to 13, yet congruent to 1. A residue class modulo \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n is a set that contains all integers congruent to a specified \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n  \\n modulo \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n. For example, \\n  \\n    \\n      \\n        6\\n        \\n          Z\\n        \\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle 6\\\\mathbb {Z} +1}\\n  \\n contains all multiples of 6 incremented by 1. Modular arithmetic provides a range of formulas for rapidly solving congruences of very large powers. An influential theorem is Fermat\\'s little theorem, which states that if a prime \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  \\n is coprime to some integer \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n, then \\n  \\n    \\n      \\n        \\n          a\\n          \\n            p\\n            −\\n            1\\n          \\n        \\n        ≡\\n        1\\n        \\n          \\n          (\\n          mod\\n          \\n          p\\n          )\\n        \\n      \\n    \\n    {\\\\textstyle a^{p-1}\\\\equiv 1{\\\\pmod {p}}}\\n  \\n is true. Euler\\'s theorem extends this to assert that every integer \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n satisfies the congruence\\n  \\n    \\n      \\n        \\n          a\\n          \\n            φ\\n            (\\n            n\\n            )\\n          \\n        \\n        ≡\\n        1\\n        \\n          \\n          (\\n          mod\\n          \\n          n\\n          )\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle a^{\\\\varphi (n)}\\\\equiv 1{\\\\pmod {n}},}\\n  \\nwhere Euler\\'s totient function \\n  \\n    \\n      \\n        φ\\n      \\n    \\n    {\\\\displaystyle \\\\varphi }\\n  \\n counts all positive integers up to \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n that are coprime to \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  \\n. Modular arithmetic also provides formulas that are used to solve congruences with unknowns in a similar vein to equation solving in algebra, such as the Chinese remainder theorem.\\n\\n\\n=== Analytic number theory ===\\n\\nAnalytic number theory, in contrast to elementary number theory, relies on complex numbers and techniques from analysis and calculus. Analytic number theory may be defined\\n\\nin terms of its tools, as the study of the integers by means of tools from real and complex analysis; or\\nin terms of its concerns, as the study within number theory of estimates on the size and density of certain numbers (e.g., primes), as opposed to identities.\\nIt studies the distribution of primes, behavior of number-theoric functions, and irrational numbers.\\nNumber theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, many of the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics. The following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture, the twin prime conjecture, the Hardy–Littlewood conjectures, the Waring problem and the Riemann hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.\\nAnalysis is the branch of mathematics that studies the limit, defined as the value to which a sequence or function tends as the argument (or index) approaches a specific value. For example, the limit of the sequence 0.9, 0.99, 0.999, ... is 1. In the context of functions, the limit of \\n  \\n    \\n      \\n        \\n          \\n            1\\n            x\\n          \\n        \\n      \\n    \\n    {\\\\textstyle {\\\\frac {1}{x}}}\\n  \\n as \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n approaches infinity is 0. The complex numbers extend the real numbers with the imaginary unit \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  \\n defined as the solution to \\n  \\n    \\n      \\n        \\n          i\\n          \\n            2\\n          \\n        \\n        =\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle i^{2}=-1}\\n  \\n. Every complex number can be expressed as \\n  \\n    \\n      \\n        x\\n        +\\n        i\\n        y\\n      \\n    \\n    {\\\\displaystyle x+iy}\\n  \\n, where \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is called the real part and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n is called the imaginary part.\\nThe distribution of primes, described by the function \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n  \\n that counts all primes up to a given real number, is unpredictable and is a major subject of study in number theory. Elementary formulas for a partial sequence of primes, including Euler\\'s prime-generating polynomials have been developed. However, these cease to function as the primes become too large. The prime number theorem in analytic number theory provides a formalisation of the notion that prime numbers appear less commonly as their numerical value increases. One distribution states, informally, that the function \\n  \\n    \\n      \\n        \\n          \\n            x\\n            \\n              log\\n              \\u2061\\n              (\\n              x\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {x}{\\\\log(x)}}}\\n  \\n approximates \\n  \\n    \\n      \\n        π\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (x)}\\n  \\n. Another distribution involves an offset logarithmic integral which converges to \\n  \\n    \\n      \\n        π\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (x)}\\n  \\n more quickly.\\n\\nThe zeta function has been demonstrated to be connected to the distribution of primes. It is defined as the series\\n  \\n    \\n      \\n        ζ\\n        (\\n        s\\n        )\\n        =\\n        \\n          ∑\\n          \\n            n\\n            =\\n            1\\n          \\n          \\n            ∞\\n          \\n        \\n        \\n          \\n            1\\n            \\n              n\\n              \\n                s\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              1\\n              \\n                s\\n              \\n            \\n          \\n        \\n        +\\n        \\n          \\n            1\\n            \\n              2\\n              \\n                s\\n              \\n            \\n          \\n        \\n        +\\n        \\n          \\n            1\\n            \\n              3\\n              \\n                s\\n              \\n            \\n          \\n        \\n        +\\n        ⋯\\n      \\n    \\n    {\\\\displaystyle \\\\zeta (s)=\\\\sum _{n=1}^{\\\\infty }{\\\\frac {1}{n^{s}}}={\\\\frac {1}{1^{s}}}+{\\\\frac {1}{2^{s}}}+{\\\\frac {1}{3^{s}}}+\\\\cdots }\\n  \\nthat converges if \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  \\n is greater than 1. Euler demonstrated a link involving the infinite product over all prime numbers, expressed as the identity \\n  \\n    \\n      \\n        ζ\\n        (\\n        s\\n        )\\n        =\\n        \\n          ∏\\n          \\n            p\\n            \\n               prime\\n            \\n          \\n        \\n        \\n          \\n            (\\n            \\n              1\\n              −\\n              \\n                \\n                  1\\n                  \\n                    p\\n                    \\n                      s\\n                    \\n                  \\n                \\n              \\n            \\n            )\\n          \\n          \\n            −\\n            1\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\zeta (s)=\\\\prod _{p{\\\\text{ prime}}}\\\\left(1-{\\\\frac {1}{p^{s}}}\\\\right)^{-1}.}\\n  \\nRiemann extended the definition to a complex variable and conjectured that all nontrivial cases (\\n  \\n    \\n      \\n        0\\n        <\\n        ℜ\\n        (\\n        s\\n        )\\n        <\\n        1\\n      \\n    \\n    {\\\\displaystyle 0<\\\\Re (s)<1}\\n  \\n) where the function returns a zero are those in which the real part of \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  \\n is equal to \\n  \\n    \\n      \\n        \\n          \\n            1\\n            2\\n          \\n        \\n      \\n    \\n    {\\\\textstyle {\\\\frac {1}{2}}}\\n  \\n. He established a connection between the nontrivial zeroes and the prime-counting function. In what is now recognised as the unsolved Riemann hypothesis, a solution to it would imply direct consequences for understanding the distribution of primes.\\nOne may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject. This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.\\nElementary number theory works with elementary proofs, a term that excludes the use of complex numbers but may include basic analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erdős and Selberg. The term is somewhat ambiguous. For example, proofs based on complex Tauberian theorems, such as Wiener–Ikehara, are often seen as quite enlightening but not elementary despite using Fourier analysis, not complex analysis. Here as elsewhere, an elementary proof may be longer and more difficult for most readers than a more advanced proof.\\nSome subjects generally considered to be part of analytic number theory (e.g., sieve theory) are better covered by the second rather than the first definition. Small sieves, for instance, use little analysis and yet still belong to analytic number theory.\\n\\n\\n=== Algebraic number theory ===\\n\\nAn algebraic number is any complex number that is a solution to some polynomial equation \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x)=0}\\n  \\n with rational coefficients; for example, every solution \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n of \\n  \\n    \\n      \\n        \\n          x\\n          \\n            5\\n          \\n        \\n        +\\n        (\\n        11\\n        \\n          /\\n        \\n        2\\n        )\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        −\\n        7\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        9\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle x^{5}+(11/2)x^{3}-7x^{2}+9=0}\\n  \\n is an algebraic number. Fields of algebraic numbers are also called algebraic number fields, or shortly number fields. Algebraic number theory studies algebraic number fields.\\nIt could be argued that the simplest kind of number fields, namely quadratic fields, were already studied by Gauss, as the discussion of quadratic forms in Disquisitiones Arithmeticae can be restated in terms of ideals and\\nnorms in quadratic fields. (A quadratic field consists of all\\nnumbers of the form \\n  \\n    \\n      \\n        a\\n        +\\n        b\\n        \\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a+b{\\\\sqrt {d}}}\\n  \\n, where\\n\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n and \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  \\n are rational numbers and \\n  \\n    \\n      \\n        d\\n      \\n    \\n    {\\\\displaystyle d}\\n  \\n\\nis a fixed rational number whose square root is not rational.)\\nFor that matter, the eleventh-century chakravala method amounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.\\nThe grounds of the subject were set in the late nineteenth century, when ideal numbers, the theory of ideals and valuation theory were introduced; these are three complementary ways of dealing with the lack of unique factorization in algebraic number fields. (For example, in the field generated by the rationals\\nand \\n  \\n    \\n      \\n        \\n          \\n            −\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {-5}}}\\n  \\n, the number \\n  \\n    \\n      \\n        6\\n      \\n    \\n    {\\\\displaystyle 6}\\n  \\n can be factorised both as \\n  \\n    \\n      \\n        6\\n        =\\n        2\\n        ⋅\\n        3\\n      \\n    \\n    {\\\\displaystyle 6=2\\\\cdot 3}\\n  \\n and\\n\\n  \\n    \\n      \\n        6\\n        =\\n        (\\n        1\\n        +\\n        \\n          \\n            −\\n            5\\n          \\n        \\n        )\\n        (\\n        1\\n        −\\n        \\n          \\n            −\\n            5\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle 6=(1+{\\\\sqrt {-5}})(1-{\\\\sqrt {-5}})}\\n  \\n; all of \\n  \\n    \\n      \\n        2\\n      \\n    \\n    {\\\\displaystyle 2}\\n  \\n, \\n  \\n    \\n      \\n        3\\n      \\n    \\n    {\\\\displaystyle 3}\\n  \\n, \\n  \\n    \\n      \\n        1\\n        +\\n        \\n          \\n            −\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 1+{\\\\sqrt {-5}}}\\n  \\n and\\n\\n  \\n    \\n      \\n        1\\n        −\\n        \\n          \\n            −\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 1-{\\\\sqrt {-5}}}\\n  \\n\\nare irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws, that is, generalizations of quadratic reciprocity.\\nNumber fields are often studied as extensions of smaller number fields: a field L is said to be an extension of a field K if L contains K.\\n(For example, the complex numbers C are an extension of the reals R, and the reals R are an extension of the rationals Q.)\\nClassifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensions L of K such that the Galois group Gal(L/K) of L over K is an abelian group—are relatively well understood.\\nTheir classification was the object of the programme of class field theory, which was initiated in the late nineteenth century (partly by Kronecker and Eisenstein) and carried out largely in 1900–1950.\\nAn example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.\\n\\n\\n=== Diophantine geometry ===\\n\\nThe central problem of Diophantine geometry is to determine when a Diophantine equation has integer or rational solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.\\nFor example, an equation in two variables defines a curve in the plane. More generally, an equation or system of equations in two or more variables defines a curve, a surface, or some other such object in n-dimensional space. In Diophantine geometry, one asks whether there are any rational points (points all of whose coordinates are rationals) or\\nintegral points (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is whether there are finitely\\nor infinitely many rational points on a given curve or surface.\\nConsider, for instance, the Pythagorean equation \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle x^{2}+y^{2}=1}\\n  \\n. One would like to know its rational solutions, namely \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n  \\n such that x and y are both rational. This is the same as asking for all integer solutions\\nto \\n  \\n    \\n      \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          b\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{2}+b^{2}=c^{2}}\\n  \\n; any solution to the latter equation gives us a solution \\n  \\n    \\n      \\n        x\\n        =\\n        a\\n        \\n          /\\n        \\n        c\\n      \\n    \\n    {\\\\displaystyle x=a/c}\\n  \\n, \\n  \\n    \\n      \\n        y\\n        =\\n        b\\n        \\n          /\\n        \\n        c\\n      \\n    \\n    {\\\\displaystyle y=b/c}\\n  \\n to the former. It is also the\\nsame as asking for all points with rational coordinates on the curve described by \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle x^{2}+y^{2}=1}\\n  \\n (a circle of radius 1 centered on the origin).\\n\\nThe rephrasing of questions on equations in terms of points on curves is felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve (that is, rational or integer solutions to an equation \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x,y)=0}\\n  \\n, where \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\n is a polynomial in two variables) depends crucially on the genus of the curve. A major achievement of this approach is Wiles\\'s proof of Fermat\\'s Last Theorem, for which other geometrical notions are just as crucial.\\nThere is also the closely linked area of Diophantine approximations: given a number \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n, determine how well it can be approximated by rational numbers. One seeks approximations that are good relative to the amount of space required to write the rational number: call \\n  \\n    \\n      \\n        a\\n        \\n          /\\n        \\n        q\\n      \\n    \\n    {\\\\displaystyle a/q}\\n  \\n (with \\n  \\n    \\n      \\n        gcd\\n        (\\n        a\\n        ,\\n        q\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\gcd(a,q)=1}\\n  \\n) a good approximation to \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n if \\n  \\n    \\n      \\n        \\n          |\\n        \\n        x\\n        −\\n        a\\n        \\n          /\\n        \\n        q\\n        \\n          |\\n        \\n        <\\n        \\n          \\n            1\\n            \\n              q\\n              \\n                c\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |x-a/q|<{\\\\frac {1}{q^{c}}}}\\n  \\n, where \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n  \\n is large. This question is of special interest if \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is an algebraic number. If \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n cannot be approximated well, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) are critical both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be approximated better than any algebraic number, then it is a transcendental number. It is by this argument that π and e have been shown to be transcendental.\\nDiophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. Arithmetic geometry is a contemporary term for the same domain covered by Diophantine geometry, particularly when one wishes to emphasize the connections to modern algebraic geometry (for example, in Faltings\\'s theorem) rather than to techniques in Diophantine approximations.\\n\\n\\n=== Other subfields ===\\n\\nProbabilistic number theory starts with questions such as the following: Take an integer n at random between one and a million. How likely is it to be prime? (this is just another way of asking how many primes there are between one and a million). How many prime divisors will n have on average? What is the probability that it will have many more or many fewer divisors or prime divisors than the average?\\nCombinatorics in number theory starts with questions like the following: Does a fairly \"thick\" infinite set \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  \\n contain many elements in arithmetic progression: \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n,\\n\\n  \\n    \\n      \\n        a\\n        +\\n        b\\n        ,\\n        a\\n        +\\n        2\\n        b\\n        ,\\n        a\\n        +\\n        3\\n        b\\n        ,\\n        …\\n        ,\\n        a\\n        +\\n        10\\n        b\\n      \\n    \\n    {\\\\displaystyle a+b,a+2b,a+3b,\\\\ldots ,a+10b}\\n  \\n? Should it be possible to write large integers as sums of elements of \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  \\n?There are two main questions: \"Can this be computed?\" and \"Can it be computed rapidly?\" Anyone can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. Fast algorithms for testing primality are now known, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.\\n\\n\\n== Applications ==\\nFor a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of mathematics other than the use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance. The number-theorist Leonard Dickson (1874–1954) said \"Thank God that number theory is unsullied by any application\". Such a view is no longer applicable to number theory. \\nThis vision of the purity of number theory was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public-key cryptography algorithms. Schemes such as RSA are based on the difficulty of factoring large composite numbers into their prime factors. These applications have led to significant study of algorithms for computing with prime numbers, and in particular of primality testing, methods for determining whether a given number is prime. Prime numbers are also used in computing for checksums, hash tables, and pseudorandom number generators.\\nIn 1974, Donald Knuth said \"virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations\".\\nElementary number theory is taught in discrete mathematics courses for computer scientists. It also has applications to the continuous in numerical analysis.\\nNumber theory has now several modern applications spanning diverse areas such as:\\n\\nComputer science: The fast Fourier transform (FFT) algorithm, which is used to efficiently compute the discrete Fourier transform, has important applications in signal processing and data analysis.\\nPhysics: The Riemann hypothesis has connections to the distribution of prime numbers and has been studied for its potential implications in physics.\\nError correction codes: The theory of finite fields and algebraic geometry have been used to construct efficient error-correcting codes.\\nStudy of musical scales: the concept of \"equal temperament\", which is the basis for most modern Western music, involves dividing the octave into 12 equal parts. This has been studied using number theory and in particular the properties of the 12th root of 2.\\n\\n\\n== See also ==\\n\\nArithmetic dynamics\\nAlgebraic function field\\nArithmetic topology\\nFinite field\\np-adic number\\nList of number theoretic algorithms\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n=== Sources ===\\n\\nThis article incorporates material from the Citizendium article \"Number theory\", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.\\n\\n\\n== Further reading ==\\nTwo of the most popular introductions to the subject are:\\n\\nHardy, G. H.; Wright, E. M. (2008) [1938]. An introduction to the theory of numbers (rev. by D. R. Heath-Brown and J. H. Silverman, 6th ed.). Oxford University Press. ISBN 978-0-19-921986-5.\\nVinogradov, I. M. (2003) [1954]. Elements of Number Theory (reprint of the 1954 ed.). Mineola, NY: Dover Publications.\\nHardy and Wright\\'s book is a comprehensive classic, though its clarity sometimes suffers due to the authors\\' insistence on elementary methods (Apostol 1981).\\nVinogradov\\'s main attraction consists in its set of problems, which quickly lead to Vinogradov\\'s own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:\\n\\nIvan M. Niven; Herbert S. Zuckerman; Hugh L. Montgomery (2008) [1960]. An introduction to the theory of numbers (reprint of the 5th 1991 ed.). John Wiley & Sons. ISBN 978-81-265-1811-1. Retrieved 2016-02-28.\\nRosen, Kenneth H. (2010). Elementary Number Theory (6th ed.). Pearson Education. ISBN 978-0-321-71775-7. Retrieved 2016-02-28.\\nPopular choices for a second textbook include:\\n\\nBorevich, A. I.; Shafarevich, Igor R. (1966). Number theory. Pure and Applied Mathematics. Vol. 20. Boston, MA: Academic Press. ISBN 978-0-12-117850-5. MR 0195803.\\nSerre, Jean-Pierre (1996) [1973]. A course in arithmetic. Graduate Texts in Mathematics. Vol. 7. Springer. ISBN 978-0-387-90040-7.\\n\\n\\n== External links ==\\n\\nNumber Theory entry in the Encyclopedia of Mathematics\\nNumber Theory Web'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
